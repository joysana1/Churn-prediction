{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "#BOX-COX\n",
    "#lemda=0.5\n",
    "#num_df=(num_df**lemda)\n",
    "#num_df=num_df-1\n",
    "#num_df[num_df < 0]=0\n",
    "#num_df=num_df.div(lemda)\n",
    "#End BOX-COX\n",
    "#Z-score\n",
    "#num_df=(num_df-num_df.min())/(num_df.std(ddof=0)) ##Z-score\n",
    "\n",
    "#Log\n",
    "num_df=round(np.log(num_df.add(1)),2)\n",
    "num_df=num_df.replace([np.inf, -np.inf], np.nan)\n",
    "#End Log\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df[result_df < 0]=0\n",
    "\n",
    "result_df_op=result_df\n",
    "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
    "#result_df_op=np.nan_to_num(result_df_op)\n",
    "\n",
    "X=result_df_op\n",
    "#y=result_df['churn'] \n",
    "y=churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crclscod</td>\n",
       "      <td>1803.718570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mouiwylisv_Mean</td>\n",
       "      <td>543.543293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mou_opkv_Mean</td>\n",
       "      <td>490.794990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ccrndmou_Mean</td>\n",
       "      <td>474.621785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>iwylis_vce_Mean</td>\n",
       "      <td>451.771701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cc_mou_Mean</td>\n",
       "      <td>422.894192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>asl_flag</td>\n",
       "      <td>415.391673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mou_cvce_Mean</td>\n",
       "      <td>387.201183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custcare_Mean</td>\n",
       "      <td>376.980953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mou_rvce_Mean</td>\n",
       "      <td>341.931592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>opk_vce_Mean</td>\n",
       "      <td>340.848723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mou_peav_Mean</td>\n",
       "      <td>335.151915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mou_Mean</td>\n",
       "      <td>306.393340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mouowylisv_Mean</td>\n",
       "      <td>300.777692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>complete_Mean</td>\n",
       "      <td>293.585312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comp_vce_Mean</td>\n",
       "      <td>292.289639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>peak_vce_Mean</td>\n",
       "      <td>278.953029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>attempt_Mean</td>\n",
       "      <td>271.942043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plcd_vce_Mean</td>\n",
       "      <td>270.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>owylis_vce_Mean</td>\n",
       "      <td>268.834785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>recv_vce_Mean</td>\n",
       "      <td>240.697329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>hnd_webcap</td>\n",
       "      <td>226.846793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>unan_vce_Mean</td>\n",
       "      <td>205.231082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inonemin_Mean</td>\n",
       "      <td>201.831172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>avg3mou</td>\n",
       "      <td>201.305786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>eqpdays</td>\n",
       "      <td>196.419048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>callwait_Mean</td>\n",
       "      <td>187.236927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>lor</td>\n",
       "      <td>151.749136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>avg3qty</td>\n",
       "      <td>149.348801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vceovr_Mean</td>\n",
       "      <td>130.510981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>avg6qty</td>\n",
       "      <td>11.391180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unan_dat_Mean</td>\n",
       "      <td>11.011101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>refurb_new</td>\n",
       "      <td>10.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>adjrev</td>\n",
       "      <td>7.998187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>prizm_social_one</td>\n",
       "      <td>7.534467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>numbcars</td>\n",
       "      <td>6.715042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>totcalls</td>\n",
       "      <td>5.489226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>totrev</td>\n",
       "      <td>5.416160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>avg3rev</td>\n",
       "      <td>5.318513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>adjqty</td>\n",
       "      <td>4.862023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drop_dat_Mean</td>\n",
       "      <td>4.595589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>actvsubs</td>\n",
       "      <td>3.281815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>avgmou</td>\n",
       "      <td>3.108271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>creditcd</td>\n",
       "      <td>2.698027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>callfwdv_Mean</td>\n",
       "      <td>2.650712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>totmou</td>\n",
       "      <td>2.111632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>avg6rev</td>\n",
       "      <td>1.894306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>adjmou</td>\n",
       "      <td>1.704058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recv_sms_Mean</td>\n",
       "      <td>1.550934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>forgntvl</td>\n",
       "      <td>1.494926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>avgqty</td>\n",
       "      <td>0.915564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.797717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kid0_2</td>\n",
       "      <td>0.610982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>new_cell</td>\n",
       "      <td>0.517556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>kid16_17</td>\n",
       "      <td>0.333963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kid3_5</td>\n",
       "      <td>0.248540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>avgrev</td>\n",
       "      <td>0.185670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rv</td>\n",
       "      <td>0.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kid11_15</td>\n",
       "      <td>0.010772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kid6_10</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature       Scores\n",
       "76          crclscod  1803.718570\n",
       "32   mouiwylisv_Mean   543.543293\n",
       "39     mou_opkv_Mean   490.794990\n",
       "22     ccrndmou_Mean   474.621785\n",
       "31   iwylis_vce_Mean   451.771701\n",
       "23       cc_mou_Mean   422.894192\n",
       "77          asl_flag   415.391673\n",
       "26     mou_cvce_Mean   387.201183\n",
       "21     custcare_Mean   376.980953\n",
       "28     mou_rvce_Mean   341.931592\n",
       "37      opk_vce_Mean   340.848723\n",
       "35     mou_peav_Mean   335.151915\n",
       "1           mou_Mean   306.393340\n",
       "30   mouowylisv_Mean   300.777692\n",
       "43     complete_Mean   293.585312\n",
       "19     comp_vce_Mean   292.289639\n",
       "33     peak_vce_Mean   278.953029\n",
       "42      attempt_Mean   271.942043\n",
       "15     plcd_vce_Mean   270.508733\n",
       "29   owylis_vce_Mean   268.834785\n",
       "17     recv_vce_Mean   240.697329\n",
       "82        hnd_webcap   226.846793\n",
       "13     unan_vce_Mean   205.231082\n",
       "24     inonemin_Mean   201.831172\n",
       "58           avg3mou   201.305786\n",
       "74           eqpdays   196.419048\n",
       "45     callwait_Mean   187.236927\n",
       "69               lor   151.749136\n",
       "59           avg3qty   149.348801\n",
       "6        vceovr_Mean   130.510981\n",
       "..               ...          ...\n",
       "62           avg6qty    11.391180\n",
       "14     unan_dat_Mean    11.011101\n",
       "81        refurb_new    10.910400\n",
       "52            adjrev     7.998187\n",
       "78  prizm_social_one     7.534467\n",
       "72          numbcars     6.715042\n",
       "49          totcalls     5.489226\n",
       "51            totrev     5.416160\n",
       "60           avg3rev     5.318513\n",
       "54            adjqty     4.862023\n",
       "10     drop_dat_Mean     4.595589\n",
       "48          actvsubs     3.281815\n",
       "56            avgmou     3.108271\n",
       "95          creditcd     2.698027\n",
       "44     callfwdv_Mean     2.650712\n",
       "50            totmou     2.111632\n",
       "63           avg6rev     1.894306\n",
       "53            adjmou     1.704058\n",
       "18     recv_sms_Mean     1.550934\n",
       "73          forgntvl     1.494926\n",
       "57            avgqty     0.915564\n",
       "67             truck     0.797717\n",
       "90            kid0_2     0.610982\n",
       "75          new_cell     0.517556\n",
       "94          kid16_17     0.333963\n",
       "91            kid3_5     0.248540\n",
       "55            avgrev     0.185670\n",
       "68                rv     0.027819\n",
       "93          kid11_15     0.010772\n",
       "92           kid6_10     0.000403\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X\n",
    "y_train=y\n",
    "#Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_feature = SelectKBest(chi2, k=80).fit(X_train, y_train)\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                     'Scores':select_feature.scores_})\n",
    "selected_features_df.sort_values(by='Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y\n",
    "x_train_chi = select_feature.transform(X)\n",
    "x_train_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6ca28190d33f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var_smoothing': 1.873817422860383e-07}\n",
      "GaussianNB(priors=None, var_smoothing=1.873817422860383e-07)\n",
      "0.5508966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "classifier=GaussianNB();\n",
    "#params = {\n",
    "#          \"priors\" : \"None\",\n",
    "#          \"var_smoothing\" : 1e-9\n",
    "#}\n",
    "#create an list of var_smoothing to cross validate\n",
    "#steps = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "grid = GridSearchCV(estimator=classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27976 22462 22726 26836\n",
      "pod:  0.5414632177878214\n",
      "pof:  0.44533883183314166\n",
      "AUC:  0.5480621929773398\n",
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB(priors=None, var_smoothing=1.873817422860383e-07)\n",
    "\n",
    "classifier.fit(x_train_chi,y_train)\n",
    "\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=10, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 5)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    " #accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "accuracy=(26836+27976)/(27976+22462+22726+26836)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 555.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 3133.4min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 3152.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.57708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},    \n",
    "    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}\n",
    "]\n",
    " \n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30042 20396 21896 27666\n",
      "pod:  0.5582099188894718\n",
      "pof:  0.4043776517704905\n",
      "AUC:  0.5769161335594907\n",
      "accuracy:  0.57708\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 88.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "param_grid ={\n",
    "   'n_neighbors': [3,5,11,19],\n",
    "   'weights': ['uniform', 'distance'],\n",
    "   'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 42 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 410.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 1993.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], \n",
    "                     'gamma': [0.001, 0.01, 0.1, 1, 1e-3, 1e-4], \n",
    "                     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28689 21749 24095 25467\n",
      "pod:  0.5138412493442557\n",
      "pof:  0.43120266465759943\n",
      "AUC:  0.5413192923433281\n",
      "accuracy:  0.54156\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(metric='manhattan', weights='uniform', n_neighbors=19 )  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a44d8b843c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 48.3min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 115.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 212.7min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 931.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "#parameters = {'n_estimators': [4, 6, 9], \n",
    "#              'max_features': ['log2', 'sqrt','auto'], \n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [2, 3, 5, 10], \n",
    "#              'min_samples_split': [2, 3, 5],\n",
    "#              'min_samples_leaf': [1,5,8]\n",
    "#             }\n",
    "\n",
    "#model_params = {\n",
    "#    'n_estimators': [50, 150, 250],\n",
    "#    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "#    'min_samples_split': [2, 4, 6]\n",
    "#}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=110, max_features=3, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=3, min_samples_split=8,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.58342\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29094 21344 20001 29561\n",
      "pod:  0.5964448569468545\n",
      "pof:  0.4231730044807486\n",
      "AUC:  0.5866359262330528\n",
      "accuracy:  0.58655\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(bootstrap= True, max_depth= 110, max_features= 3, min_samples_leaf= 3, min_samples_split= 8, n_estimators= 1000)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-67af43a91f99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26342 24096 23719 25843\n",
      "pod:  0.5214277067107865\n",
      "pof:  0.47773504104048536\n",
      "AUC:  0.5218463328351506\n",
      "accuracy:  0.52185\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27829 22609 18287 31275\n",
      "pod:  0.6310278035591784\n",
      "pof:  0.44825330108251715\n",
      "AUC:  0.5913872512383306\n",
      "accuracy:  0.59104\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 112us/step - loss: 0.6855 - accuracy: 0.5479 - val_loss: 0.6759 - val_accuracy: 0.5821\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.6787 - accuracy: 0.5687 - val_loss: 0.6605 - val_accuracy: 0.6134\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.6754 - accuracy: 0.5742 - val_loss: 0.6671 - val_accuracy: 0.6042\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 99us/step - loss: 0.6732 - accuracy: 0.5784 - val_loss: 0.6582 - val_accuracy: 0.6110\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.6722 - accuracy: 0.5799 - val_loss: 0.6564 - val_accuracy: 0.6220\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 7s 99us/step - loss: 0.6709 - accuracy: 0.5826 - val_loss: 0.6783 - val_accuracy: 0.5684\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.6706 - accuracy: 0.5833 - val_loss: 0.6687 - val_accuracy: 0.5911\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 8s 108us/step - loss: 0.6697 - accuracy: 0.5849 - val_loss: 0.6841 - val_accuracy: 0.5617\n",
      "y2_pred:  [[0.36557782]\n",
      " [0.52909315]\n",
      " [0.83746386]\n",
      " ...\n",
      " [0.609007  ]\n",
      " [0.6374638 ]\n",
      " [0.6035595 ]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8390155335888643\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.6848 - accuracy: 0.5523 - val_loss: 0.6587 - val_accuracy: 0.6129\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 7s 100us/step - loss: 0.6742 - accuracy: 0.5781 - val_loss: 0.6650 - val_accuracy: 0.6112\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.6722 - accuracy: 0.5818 - val_loss: 0.6668 - val_accuracy: 0.5858\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 97us/step - loss: 0.6699 - accuracy: 0.5844 - val_loss: 0.6611 - val_accuracy: 0.6179\n",
      "y2_pred:  [[0.4033202 ]\n",
      " [0.4710892 ]\n",
      " [0.5991324 ]\n",
      " ...\n",
      " [0.47694975]\n",
      " [0.6519075 ]\n",
      " [0.5509294 ]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6106516037926165\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6868 - accuracy: 0.5427 - val_loss: 0.6914 - val_accuracy: 0.5557\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.6771 - accuracy: 0.5723 - val_loss: 0.6655 - val_accuracy: 0.6062\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6731 - accuracy: 0.5790 - val_loss: 0.6728 - val_accuracy: 0.5989\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.6716 - accuracy: 0.5819 - val_loss: 0.6787 - val_accuracy: 0.5748\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.6705 - accuracy: 0.5849 - val_loss: 0.6719 - val_accuracy: 0.5941\n",
      "y2_pred:  [[0.5020635 ]\n",
      " [0.5370193 ]\n",
      " [0.5596598 ]\n",
      " ...\n",
      " [0.48909727]\n",
      " [0.40674615]\n",
      " [0.54603934]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.6680790960451978\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.6868 - accuracy: 0.5451 - val_loss: 0.6862 - val_accuracy: 0.5642\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6762 - accuracy: 0.5717 - val_loss: 0.6609 - val_accuracy: 0.6074\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6723 - accuracy: 0.5820 - val_loss: 0.6728 - val_accuracy: 0.5784\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.6714 - accuracy: 0.5827 - val_loss: 0.6626 - val_accuracy: 0.6102\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6695 - accuracy: 0.5875 - val_loss: 0.6623 - val_accuracy: 0.6113\n",
      "y2_pred:  [[0.3595677 ]\n",
      " [0.4644264 ]\n",
      " [0.6834676 ]\n",
      " ...\n",
      " [0.32906914]\n",
      " [0.5579842 ]\n",
      " [0.54172915]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.5205811138014528\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 97us/step - loss: 0.6860 - accuracy: 0.5464 - val_loss: 0.6734 - val_accuracy: 0.5909\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6763 - accuracy: 0.5736 - val_loss: 0.6723 - val_accuracy: 0.6087\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 94us/step - loss: 0.6726 - accuracy: 0.5811 - val_loss: 0.6820 - val_accuracy: 0.5586\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.6707 - accuracy: 0.5823 - val_loss: 0.6878 - val_accuracy: 0.5362\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 96us/step - loss: 0.6695 - accuracy: 0.5851 - val_loss: 0.6811 - val_accuracy: 0.5562\n",
      "y2_pred:  [[0.5878371 ]\n",
      " [0.6088454 ]\n",
      " [0.4893211 ]\n",
      " ...\n",
      " [0.38231093]\n",
      " [0.54791605]\n",
      " [0.3968251 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7544390637610977\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.6852 - accuracy: 0.5512 - val_loss: 0.6763 - val_accuracy: 0.6004\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.6749 - accuracy: 0.5765 - val_loss: 0.6650 - val_accuracy: 0.6037\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6725 - accuracy: 0.5813 - val_loss: 0.6914 - val_accuracy: 0.5357\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6705 - accuracy: 0.5837 - val_loss: 0.7196 - val_accuracy: 0.5152\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6694 - accuracy: 0.5861 - val_loss: 0.6961 - val_accuracy: 0.5348\n",
      "y2_pred:  [[0.537477  ]\n",
      " [0.56196934]\n",
      " [0.61565566]\n",
      " ...\n",
      " [0.61964697]\n",
      " [0.45541754]\n",
      " [0.6099481 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7764326069410815\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6807 - accuracy: 0.5591 - val_loss: 0.6912 - val_accuracy: 0.5627\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.6724 - accuracy: 0.5809 - val_loss: 0.6774 - val_accuracy: 0.5769\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.6698 - accuracy: 0.5875 - val_loss: 0.6865 - val_accuracy: 0.5661\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 97us/step - loss: 0.6680 - accuracy: 0.5873 - val_loss: 0.6800 - val_accuracy: 0.5798\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6666 - accuracy: 0.5900 - val_loss: 0.6982 - val_accuracy: 0.5366\n",
      "y2_pred:  [[0.41037425]\n",
      " [0.45881274]\n",
      " [0.62179387]\n",
      " ...\n",
      " [0.77904683]\n",
      " [0.59465075]\n",
      " [0.42833123]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.8119451170298628\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6860 - accuracy: 0.5481 - val_loss: 0.7003 - val_accuracy: 0.4878\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.6733 - accuracy: 0.5811 - val_loss: 0.6880 - val_accuracy: 0.5524\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6699 - accuracy: 0.5874 - val_loss: 0.6735 - val_accuracy: 0.5831\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.6672 - accuracy: 0.5912 - val_loss: 0.6733 - val_accuracy: 0.5863\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6648 - accuracy: 0.5950 - val_loss: 0.6806 - val_accuracy: 0.5621\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6640 - accuracy: 0.5968 - val_loss: 0.6981 - val_accuracy: 0.5206\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.6625 - accuracy: 0.5987 - val_loss: 0.6762 - val_accuracy: 0.5781\n",
      "y2_pred:  [[0.5640708 ]\n",
      " [0.37050593]\n",
      " [0.6219019 ]\n",
      " ...\n",
      " [0.5323001 ]\n",
      " [0.5491469 ]\n",
      " [0.41621462]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.4548022598870056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6873 - accuracy: 0.5400 - val_loss: 0.6819 - val_accuracy: 0.5755\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6786 - accuracy: 0.5670 - val_loss: 0.6725 - val_accuracy: 0.5984\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6734 - accuracy: 0.5775 - val_loss: 0.6722 - val_accuracy: 0.5997\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.6713 - accuracy: 0.5827 - val_loss: 0.6921 - val_accuracy: 0.5423\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.6704 - accuracy: 0.5829 - val_loss: 0.7006 - val_accuracy: 0.5299\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6682 - accuracy: 0.5889 - val_loss: 0.6757 - val_accuracy: 0.5870\n",
      "y2_pred:  [[0.6189847 ]\n",
      " [0.39016813]\n",
      " [0.41438925]\n",
      " ...\n",
      " [0.46686333]\n",
      " [0.4666784 ]\n",
      " [0.2825427 ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.5012106537530266\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6838 - accuracy: 0.5545 - val_loss: 0.6698 - val_accuracy: 0.5945\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.6755 - accuracy: 0.5752 - val_loss: 0.6722 - val_accuracy: 0.5826\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.6720 - accuracy: 0.5820 - val_loss: 0.6638 - val_accuracy: 0.5978\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.6703 - accuracy: 0.5855 - val_loss: 0.6610 - val_accuracy: 0.6076\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 95us/step - loss: 0.6696 - accuracy: 0.5854 - val_loss: 0.6899 - val_accuracy: 0.5417\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6683 - accuracy: 0.5903 - val_loss: 0.6674 - val_accuracy: 0.5927\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 93us/step - loss: 0.6674 - accuracy: 0.5899 - val_loss: 0.6911 - val_accuracy: 0.5440\n",
      "y2_pred:  [[0.66975975]\n",
      " [0.4114774 ]\n",
      " [0.74399316]\n",
      " ...\n",
      " [0.8952876 ]\n",
      " [0.7708105 ]\n",
      " [0.55986875]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6731234866828087\n",
      "pod:  [0.8390155335888643, 0.6106516037926165, 0.6680790960451978, 0.5205811138014528, 0.7544390637610977, 0.7764326069410815, 0.8119451170298628, 0.4548022598870056, 0.5012106537530266, 0.6731234866828087]\n",
      "pof:  [0.6699048374306106, 0.4129659000793021, 0.49246629659000796, 0.33287073750991275, 0.5870340999206979, 0.6560269627279937, 0.7688342585249802, 0.41415543219666934, 0.3361094586555622, 0.5268689272258577]\n",
      "auc:  [0.5845553480791268, 0.5988428518566573, 0.5878063997275949, 0.59385518814577, 0.5837024819201999, 0.560202822106544, 0.5215554292524414, 0.5203234138451681, 0.5825505975487323, 0.5731272797284754]\n",
      "tn_list:  [1665, 2961, 2560, 3365, 2083, 1735, 1166, 2955, 3348, 2386]\n",
      "fp_list:  [3379, 2083, 2484, 1679, 2961, 3309, 3878, 2089, 1695, 2657]\n",
      "fn_list:  [798, 1930, 1645, 2376, 1217, 1108, 932, 2702, 2472, 1620]\n",
      "tp_list:  [4159, 3027, 3311, 2580, 3739, 3848, 4024, 2254, 2484, 3336]\n",
      "24224 26214 16800 32762\n"
     ]
    }
   ],
   "source": [
    "df_x_train_chi = pd.DataFrame(x_train_chi)\n",
    "df_x_train_chi.head()\n",
    "\n",
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print('tn_list: ',tn_list)\n",
    "print ('fp_list: ',fp_list)\n",
    "print ('fn_list: ',fn_list)\n",
    "print ('tp_list: ',tp_list)\n",
    "\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 40s 558us/step - loss: 0.6890 - acc: 0.5341 - val_loss: 0.7065 - val_acc: 0.4566\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 40s 553us/step - loss: 0.6850 - acc: 0.5518 - val_loss: 0.6863 - val_acc: 0.5427\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 38s 532us/step - loss: 0.6827 - acc: 0.5582 - val_loss: 0.6957 - val_acc: 0.5155\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 39s 545us/step - loss: 0.6812 - acc: 0.5625 - val_loss: 0.6885 - val_acc: 0.5360\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 39s 544us/step - loss: 0.6791 - acc: 0.5666 - val_loss: 0.6815 - val_acc: 0.5604\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8125882590276376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 39s 541us/step - loss: 0.6884 - acc: 0.5396 - val_loss: 0.6918 - val_acc: 0.5220\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 38s 528us/step - loss: 0.6850 - acc: 0.5513 - val_loss: 0.6850 - val_acc: 0.5483\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 39s 535us/step - loss: 0.6838 - acc: 0.5543 - val_loss: 0.6967 - val_acc: 0.5291\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 38s 532us/step - loss: 0.6826 - acc: 0.5587 - val_loss: 0.6926 - val_acc: 0.5220\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 38s 529us/step - loss: 0.6813 - acc: 0.5614 - val_loss: 0.6924 - val_acc: 0.5341\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.851523098648376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 40s 558us/step - loss: 0.6885 - acc: 0.5400 - val_loss: 0.6860 - val_acc: 0.5541\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 40s 558us/step - loss: 0.6855 - acc: 0.5524 - val_loss: 0.6940 - val_acc: 0.5083\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 38s 527us/step - loss: 0.6843 - acc: 0.5540 - val_loss: 0.6886 - val_acc: 0.5401\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 40s 559us/step - loss: 0.6826 - acc: 0.5596 - val_loss: 0.6751 - val_acc: 0.5817\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 39s 536us/step - loss: 0.6823 - acc: 0.5613 - val_loss: 0.6835 - val_acc: 0.5593\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.54317998385795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 40s 561us/step - loss: 0.6883 - acc: 0.5386 - val_loss: 0.6951 - val_acc: 0.5105\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 39s 542us/step - loss: 0.6857 - acc: 0.5504 - val_loss: 0.6883 - val_acc: 0.5426\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 39s 543us/step - loss: 0.6839 - acc: 0.5548 - val_loss: 0.6833 - val_acc: 0.5607\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 563us/step - loss: 0.6822 - acc: 0.5586 - val_loss: 0.6936 - val_acc: 0.5283\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 39s 546us/step - loss: 0.6809 - acc: 0.5643 - val_loss: 0.6846 - val_acc: 0.5466\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.6836158192090396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 40s 557us/step - loss: 0.6885 - acc: 0.5418 - val_loss: 0.6881 - val_acc: 0.5458\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 39s 546us/step - loss: 0.6857 - acc: 0.5509 - val_loss: 0.6969 - val_acc: 0.5052\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 40s 551us/step - loss: 0.6845 - acc: 0.5546 - val_loss: 0.7003 - val_acc: 0.5002\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 39s 543us/step - loss: 0.6833 - acc: 0.5583 - val_loss: 0.6951 - val_acc: 0.5310\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 40s 549us/step - loss: 0.6819 - acc: 0.5646 - val_loss: 0.6874 - val_acc: 0.5458\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7308313155770783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.6880 - acc: 0.5424 - val_loss: 0.6804 - val_acc: 0.5692\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 564us/step - loss: 0.6852 - acc: 0.5524 - val_loss: 0.6902 - val_acc: 0.5283\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 39s 547us/step - loss: 0.6843 - acc: 0.5549 - val_loss: 0.6950 - val_acc: 0.5302\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 563us/step - loss: 0.6830 - acc: 0.5579 - val_loss: 0.7167 - val_acc: 0.4652\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 41s 564us/step - loss: 0.6817 - acc: 0.5631 - val_loss: 0.6946 - val_acc: 0.5158\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7393058918482648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 41s 563us/step - loss: 0.6871 - acc: 0.5394 - val_loss: 0.6896 - val_acc: 0.5429\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 575us/step - loss: 0.6808 - acc: 0.5603 - val_loss: 0.6920 - val_acc: 0.5357\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 48s 671us/step - loss: 0.6793 - acc: 0.5640 - val_loss: 0.6946 - val_acc: 0.5346\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 567us/step - loss: 0.6767 - acc: 0.5708 - val_loss: 0.7015 - val_acc: 0.5145\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6750 - acc: 0.5747 - val_loss: 0.6804 - val_acc: 0.5688\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.5964487489911219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 621us/step - loss: 0.6878 - acc: 0.5410 - val_loss: 0.6904 - val_acc: 0.5277\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 44s 616us/step - loss: 0.6841 - acc: 0.5559 - val_loss: 0.6885 - val_acc: 0.5381\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 605us/step - loss: 0.6810 - acc: 0.5638 - val_loss: 0.7068 - val_acc: 0.5044\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 600us/step - loss: 0.6789 - acc: 0.5688 - val_loss: 0.7015 - val_acc: 0.5059\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 609us/step - loss: 0.6776 - acc: 0.5714 - val_loss: 0.6909 - val_acc: 0.5450\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.3506860371267151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 613us/step - loss: 0.6887 - acc: 0.5366 - val_loss: 0.6889 - val_acc: 0.5464\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6851 - acc: 0.5529 - val_loss: 0.7128 - val_acc: 0.4889\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 46s 638us/step - loss: 0.6843 - acc: 0.5537 - val_loss: 0.6953 - val_acc: 0.5357\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 613us/step - loss: 0.6830 - acc: 0.5585 - val_loss: 0.6979 - val_acc: 0.5182\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 615us/step - loss: 0.6816 - acc: 0.5617 - val_loss: 0.7007 - val_acc: 0.4933\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7530266343825666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 623us/step - loss: 0.6884 - acc: 0.5407 - val_loss: 0.6796 - val_acc: 0.5805\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 594us/step - loss: 0.6851 - acc: 0.5509 - val_loss: 0.6859 - val_acc: 0.5479\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 601us/step - loss: 0.6841 - acc: 0.5543 - val_loss: 0.6804 - val_acc: 0.5696\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 602us/step - loss: 0.6828 - acc: 0.5585 - val_loss: 0.6890 - val_acc: 0.5286\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 594us/step - loss: 0.6817 - acc: 0.5598 - val_loss: 0.6887 - val_acc: 0.5379\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6501210653753027\n",
      "pod:  [0.8125882590276376, 0.851523098648376, 0.54317998385795, 0.6836158192090396, 0.7308313155770783, 0.7393058918482648, 0.5964487489911219, 0.3506860371267151, 0.7530266343825666, 0.6501210653753027]\n",
      "pof:  [0.6871530531324346, 0.7634813639968279, 0.4290245836637589, 0.5311260904044409, 0.6171689135606662, 0.6681205392545598, 0.6300555114988104, 0.4105868358445678, 0.670037675986516, 0.5649415030735673]\n",
      "auc:  [0.5627176029476015, 0.544020867325774, 0.5570777000970956, 0.5762448644022993, 0.556831201008206, 0.5355926762968526, 0.4831966187461557, 0.47004960064107365, 0.5414944791980254, 0.5425897811508678]\n",
      "20318 30120 16299 33263\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
