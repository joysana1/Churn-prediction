{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "#BOX-COX\n",
    "lemda=0.5\n",
    "num_df=(num_df**lemda)\n",
    "num_df=num_df-1\n",
    "num_df[num_df < 0]=0\n",
    "num_df=num_df.div(lemda)\n",
    "#End BOX-COX\n",
    "#Z-score\n",
    "#num_df=(num_df-num_df.min())/(num_df.std(ddof=0)) ##Z-score\n",
    "\n",
    "#Log\n",
    "#num_df=round(np.log(num_df.add(1)),2)\n",
    "#num_df=num_df.replace([np.inf, -np.inf], np.nan)\n",
    "#End Log\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df[result_df < 0]=0\n",
    "\n",
    "result_df_op=result_df\n",
    "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
    "#result_df_op=np.nan_to_num(result_df_op)\n",
    "\n",
    "X=result_df_op\n",
    "#y=result_df['churn'] \n",
    "y=churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>eqpdays</td>\n",
       "      <td>7327.590098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mou_Mean</td>\n",
       "      <td>6249.960050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mou_opkv_Mean</td>\n",
       "      <td>5335.353628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mou_cvce_Mean</td>\n",
       "      <td>4842.949657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>avg3mou</td>\n",
       "      <td>4386.863595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mou_peav_Mean</td>\n",
       "      <td>3420.231671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mou_rvce_Mean</td>\n",
       "      <td>3368.459113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>attempt_Mean</td>\n",
       "      <td>2729.697499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plcd_vce_Mean</td>\n",
       "      <td>2699.289705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>complete_Mean</td>\n",
       "      <td>2658.987843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comp_vce_Mean</td>\n",
       "      <td>2629.334608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>opk_vce_Mean</td>\n",
       "      <td>2576.425838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>hnd_price</td>\n",
       "      <td>2451.726234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>peak_vce_Mean</td>\n",
       "      <td>2248.332580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mouiwylisv_Mean</td>\n",
       "      <td>2081.216639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>avg3qty</td>\n",
       "      <td>1957.651393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>recv_vce_Mean</td>\n",
       "      <td>1869.502773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crclscod</td>\n",
       "      <td>1803.718570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mouowylisv_Mean</td>\n",
       "      <td>1494.647753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>avg6mou</td>\n",
       "      <td>1491.073168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>iwylis_vce_Mean</td>\n",
       "      <td>1377.360158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>owylis_vce_Mean</td>\n",
       "      <td>1301.186624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ccrndmou_Mean</td>\n",
       "      <td>1298.614824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inonemin_Mean</td>\n",
       "      <td>1213.117677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cc_mou_Mean</td>\n",
       "      <td>1047.154769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>unan_vce_Mean</td>\n",
       "      <td>1005.990315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ovrmou_Mean</td>\n",
       "      <td>930.123025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custcare_Mean</td>\n",
       "      <td>789.274859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vceovr_Mean</td>\n",
       "      <td>637.557031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ovrrev_Mean</td>\n",
       "      <td>593.328565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>totmou</td>\n",
       "      <td>37.616938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>adults</td>\n",
       "      <td>29.885154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>income</td>\n",
       "      <td>23.423592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blck_dat_Mean</td>\n",
       "      <td>19.036527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>infobase</td>\n",
       "      <td>16.703458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>datovr_Mean</td>\n",
       "      <td>14.177501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>dwllsize</td>\n",
       "      <td>12.391973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>marital</td>\n",
       "      <td>11.766214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ownrent</td>\n",
       "      <td>11.640799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>refurb_new</td>\n",
       "      <td>10.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drop_dat_Mean</td>\n",
       "      <td>9.177046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>prizm_social_one</td>\n",
       "      <td>7.534467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>avgrev</td>\n",
       "      <td>7.395918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>avg6rev</td>\n",
       "      <td>5.241889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unan_dat_Mean</td>\n",
       "      <td>4.866322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>creditcd</td>\n",
       "      <td>2.698027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>adjqty</td>\n",
       "      <td>1.999588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>numbcars</td>\n",
       "      <td>1.949395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>callfwdv_Mean</td>\n",
       "      <td>1.139871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kid0_2</td>\n",
       "      <td>0.610982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>new_cell</td>\n",
       "      <td>0.517556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>kid16_17</td>\n",
       "      <td>0.333963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recv_sms_Mean</td>\n",
       "      <td>0.295490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kid3_5</td>\n",
       "      <td>0.248540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>totcalls</td>\n",
       "      <td>0.018273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kid11_15</td>\n",
       "      <td>0.010772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kid6_10</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>truck</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>forgntvl</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature       Scores\n",
       "74           eqpdays  7327.590098\n",
       "1           mou_Mean  6249.960050\n",
       "39     mou_opkv_Mean  5335.353628\n",
       "26     mou_cvce_Mean  4842.949657\n",
       "58           avg3mou  4386.863595\n",
       "35     mou_peav_Mean  3420.231671\n",
       "28     mou_rvce_Mean  3368.459113\n",
       "42      attempt_Mean  2729.697499\n",
       "15     plcd_vce_Mean  2699.289705\n",
       "43     complete_Mean  2658.987843\n",
       "19     comp_vce_Mean  2629.334608\n",
       "37      opk_vce_Mean  2576.425838\n",
       "64         hnd_price  2451.726234\n",
       "33     peak_vce_Mean  2248.332580\n",
       "32   mouiwylisv_Mean  2081.216639\n",
       "59           avg3qty  1957.651393\n",
       "17     recv_vce_Mean  1869.502773\n",
       "76          crclscod  1803.718570\n",
       "30   mouowylisv_Mean  1494.647753\n",
       "61           avg6mou  1491.073168\n",
       "31   iwylis_vce_Mean  1377.360158\n",
       "29   owylis_vce_Mean  1301.186624\n",
       "22     ccrndmou_Mean  1298.614824\n",
       "24     inonemin_Mean  1213.117677\n",
       "23       cc_mou_Mean  1047.154769\n",
       "13     unan_vce_Mean  1005.990315\n",
       "4        ovrmou_Mean   930.123025\n",
       "21     custcare_Mean   789.274859\n",
       "6        vceovr_Mean   637.557031\n",
       "5        ovrrev_Mean   593.328565\n",
       "..               ...          ...\n",
       "50            totmou    37.616938\n",
       "70            adults    29.885154\n",
       "71            income    23.423592\n",
       "12     blck_dat_Mean    19.036527\n",
       "86          infobase    16.703458\n",
       "7        datovr_Mean    14.177501\n",
       "88          dwllsize    12.391973\n",
       "85           marital    11.766214\n",
       "83           ownrent    11.640799\n",
       "81        refurb_new    10.910400\n",
       "10     drop_dat_Mean     9.177046\n",
       "78  prizm_social_one     7.534467\n",
       "55            avgrev     7.395918\n",
       "63           avg6rev     5.241889\n",
       "14     unan_dat_Mean     4.866322\n",
       "95          creditcd     2.698027\n",
       "54            adjqty     1.999588\n",
       "72          numbcars     1.949395\n",
       "44     callfwdv_Mean     1.139871\n",
       "90            kid0_2     0.610982\n",
       "75          new_cell     0.517556\n",
       "94          kid16_17     0.333963\n",
       "18     recv_sms_Mean     0.295490\n",
       "91            kid3_5     0.248540\n",
       "49          totcalls     0.018273\n",
       "93          kid11_15     0.010772\n",
       "92           kid6_10     0.000403\n",
       "67             truck          NaN\n",
       "68                rv          NaN\n",
       "73          forgntvl          NaN\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X\n",
    "y_train=y\n",
    "#Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_feature = SelectKBest(chi2, k=80).fit(X_train, y_train)\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                     'Scores':select_feature.scores_})\n",
    "selected_features_df.sort_values(by='Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y\n",
    "x_train_chi = select_feature.transform(X)\n",
    "x_train_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14044 36394 10956 38606\n",
      "pod:  0.778943545458214\n",
      "pof:  0.7215591419168088\n",
      "AUC:  0.5286922017707026\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var_smoothing': 0.0004328761281083057}\n",
      "GaussianNB(priors=None, var_smoothing=0.0004328761281083057)\n",
      "0.5290366666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "classifier=GaussianNB();\n",
    "#params = {\n",
    "#          \"priors\" : \"None\",\n",
    "#          \"var_smoothing\" : 1e-9\n",
    "#}\n",
    "#create an list of var_smoothing to cross validate\n",
    "#steps = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "grid = GridSearchCV(estimator=classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17680 32758 14403 35159\n",
      "pod:  0.7093942940155764\n",
      "pof:  0.6494706372179706\n",
      "AUC:  0.5299618283988029\n",
      "accuracy:  0.52839\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB(priors=None, var_smoothing=0.0004328761281083057)\n",
    "\n",
    "classifier.fit(x_train_chi,y_train)\n",
    "\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=10, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 5)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    " #accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "accuracy=(26836+27976)/(27976+22462+22726+26836)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 555.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 3133.4min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 3152.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.57708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},    \n",
    "    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}\n",
    "]\n",
    " \n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29517 20921 20916 28646\n",
      "pod:  0.5779831322384085\n",
      "pof:  0.41478647051826006\n",
      "AUC:  0.5815983308600742\n",
      "accuracy:  0.58163\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=100,\n",
    "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=100,\n",
    "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 88.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "param_grid ={\n",
    "   'n_neighbors': [3,5,11,19],\n",
    "   'weights': ['uniform', 'distance'],\n",
    "   'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 42 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 410.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 1993.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], \n",
    "                     'gamma': [0.001, 0.01, 0.1, 1, 1e-3, 1e-4], \n",
    "                     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28641 21797 22933 26629\n",
      "pod:  0.5372866308865664\n",
      "pof:  0.43215432808596693\n",
      "AUC:  0.5525661514002997\n",
      "accuracy:  0.5527\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(metric='manhattan', weights='uniform', n_neighbors=19 )  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27765 22673 23718 25844\n",
      "pod:  0.5214478834591018\n",
      "pof:  0.4495221856536738\n",
      "AUC:  0.535962848902714\n",
      "accuracy:  0.53609\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 48.3min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 115.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 212.7min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 931.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "#parameters = {'n_estimators': [4, 6, 9], \n",
    "#              'max_features': ['log2', 'sqrt','auto'], \n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [2, 3, 5, 10], \n",
    "#              'min_samples_split': [2, 3, 5],\n",
    "#              'min_samples_leaf': [1,5,8]\n",
    "#             }\n",
    "\n",
    "#model_params = {\n",
    "#    'n_estimators': [50, 150, 250],\n",
    "#    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "#    'min_samples_split': [2, 4, 6]\n",
    "#}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=110, max_features=3, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=3, min_samples_split=8,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.58342\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29114 21324 19885 29677\n",
      "pod:  0.5987853597514224\n",
      "pof:  0.4227764780522622\n",
      "AUC:  0.58800444084958\n",
      "accuracy:  0.58791\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(bootstrap= True, max_depth= 110, max_features= 3, min_samples_leaf= 3, min_samples_split= 8, n_estimators= 1000)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26379 24059 20763 28799\n",
      "pod:  0.5810701747306404\n",
      "pof:  0.4770014671477854\n",
      "AUC:  0.5520343537914275\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26284 24154 23714 25848\n",
      "pod:  0.5215285904523627\n",
      "pof:  0.4788849676830961\n",
      "AUC:  0.5213218113846332\n",
      "accuracy:  0.52132\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26186 24252 23534 26028\n",
      "pod:  0.5251604051491062\n",
      "pof:  0.48082794718267974\n",
      "AUC:  0.5221662289832133\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27938 22500 18230 31332\n",
      "pod:  0.6321778782131472\n",
      "pof:  0.44609223204726595\n",
      "AUC:  0.5930428230829405\n",
      "accuracy:  0.5927\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28151 22287 18414 31148\n",
      "pod:  0.6284653565231427\n",
      "pof:  0.44186922558388514\n",
      "AUC:  0.5932980654696288\n",
      "accuracy:  0.59299\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 114us/step - loss: 0.6903 - accuracy: 0.5503 - val_loss: 0.6751 - val_accuracy: 0.5923\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 8s 106us/step - loss: 0.6756 - accuracy: 0.5717 - val_loss: 0.6919 - val_accuracy: 0.5431\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 109us/step - loss: 0.6726 - accuracy: 0.5814 - val_loss: 0.6983 - val_accuracy: 0.5236\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.6700 - accuracy: 0.5847 - val_loss: 0.6925 - val_accuracy: 0.5643\n",
      "y2_pred:  [[0.3194872 ]\n",
      " [0.4869363 ]\n",
      " [0.93171793]\n",
      " ...\n",
      " [0.5597078 ]\n",
      " [0.66251004]\n",
      " [0.6078704 ]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7177728464797256\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 114us/step - loss: 0.6902 - accuracy: 0.5560 - val_loss: 0.6731 - val_accuracy: 0.6055\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.6749 - accuracy: 0.5776 - val_loss: 0.6765 - val_accuracy: 0.5969\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 9s 125us/step - loss: 0.6726 - accuracy: 0.5807 - val_loss: 0.6701 - val_accuracy: 0.6075\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 104us/step - loss: 0.6710 - accuracy: 0.5851 - val_loss: 0.7159 - val_accuracy: 0.5228\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 109us/step - loss: 0.6693 - accuracy: 0.5877 - val_loss: 0.6806 - val_accuracy: 0.5737\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 8s 106us/step - loss: 0.6688 - accuracy: 0.5891 - val_loss: 0.6817 - val_accuracy: 0.5775\n",
      "y2_pred:  [[0.47514525]\n",
      " [0.5188712 ]\n",
      " [0.5828011 ]\n",
      " ...\n",
      " [0.55272454]\n",
      " [0.6831906 ]\n",
      " [0.5581218 ]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6998184385717168\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6892 - accuracy: 0.5464 - val_loss: 0.6861 - val_accuracy: 0.5154\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6785 - accuracy: 0.5681 - val_loss: 0.6846 - val_accuracy: 0.4930\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6722 - accuracy: 0.5808 - val_loss: 0.6720 - val_accuracy: 0.5941\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6693 - accuracy: 0.5870 - val_loss: 0.6906 - val_accuracy: 0.5178\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6672 - accuracy: 0.5909 - val_loss: 0.6839 - val_accuracy: 0.5509\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6665 - accuracy: 0.5905 - val_loss: 0.6647 - val_accuracy: 0.5971\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6651 - accuracy: 0.5937 - val_loss: 0.6726 - val_accuracy: 0.5542\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 118us/step - loss: 0.6649 - accuracy: 0.5948 - val_loss: 0.6853 - val_accuracy: 0.5349\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6641 - accuracy: 0.5951 - val_loss: 0.6638 - val_accuracy: 0.5812\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6627 - accuracy: 0.5976 - val_loss: 0.6691 - val_accuracy: 0.5751\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6623 - accuracy: 0.5963 - val_loss: 0.6751 - val_accuracy: 0.5473\n",
      "Epoch 12/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6617 - accuracy: 0.5998 - val_loss: 0.6554 - val_accuracy: 0.5874\n",
      "Epoch 13/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6609 - accuracy: 0.5982 - val_loss: 0.6722 - val_accuracy: 0.5808\n",
      "Epoch 14/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6603 - accuracy: 0.6008 - val_loss: 0.6669 - val_accuracy: 0.5990\n",
      "Epoch 15/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6603 - accuracy: 0.6004 - val_loss: 0.6708 - val_accuracy: 0.5831\n",
      "y2_pred:  [[0.5301683 ]\n",
      " [0.5046818 ]\n",
      " [0.5011097 ]\n",
      " ...\n",
      " [0.47824112]\n",
      " [0.29539594]\n",
      " [0.6068208 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.5667877320419693\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.7006 - accuracy: 0.5581 - val_loss: 0.6709 - val_accuracy: 0.5893\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6708 - accuracy: 0.5842 - val_loss: 0.6586 - val_accuracy: 0.6109\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6675 - accuracy: 0.5918 - val_loss: 0.6439 - val_accuracy: 0.6386\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 9s 121us/step - loss: 0.6650 - accuracy: 0.5952 - val_loss: 0.6498 - val_accuracy: 0.6358\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6629 - accuracy: 0.5985 - val_loss: 0.6616 - val_accuracy: 0.6016\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6618 - accuracy: 0.6005 - val_loss: 0.6581 - val_accuracy: 0.6170\n",
      "y2_pred:  [[0.35930264]\n",
      " [0.49067792]\n",
      " [0.74690616]\n",
      " ...\n",
      " [0.29953453]\n",
      " [0.48752594]\n",
      " [0.5544789 ]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.6575867635189669\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6902 - accuracy: 0.5388 - val_loss: 0.6770 - val_accuracy: 0.6030\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6762 - accuracy: 0.5745 - val_loss: 0.6888 - val_accuracy: 0.5925\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6715 - accuracy: 0.5838 - val_loss: 0.6807 - val_accuracy: 0.5799\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6704 - accuracy: 0.5841 - val_loss: 0.6834 - val_accuracy: 0.6058\n",
      "y2_pred:  [[0.56132364]\n",
      " [0.4777665 ]\n",
      " [0.48920453]\n",
      " ...\n",
      " [0.3587848 ]\n",
      " [0.48607752]\n",
      " [0.4400084 ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.5744552058111381\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6883 - accuracy: 0.5588 - val_loss: 0.6734 - val_accuracy: 0.6002\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6707 - accuracy: 0.5850 - val_loss: 0.6789 - val_accuracy: 0.5530\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6683 - accuracy: 0.5880 - val_loss: 0.6646 - val_accuracy: 0.6138\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6660 - accuracy: 0.5932 - val_loss: 0.6584 - val_accuracy: 0.6107\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6653 - accuracy: 0.5937 - val_loss: 0.6905 - val_accuracy: 0.5436\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6640 - accuracy: 0.5956 - val_loss: 0.6530 - val_accuracy: 0.6178\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6622 - accuracy: 0.6005 - val_loss: 0.6703 - val_accuracy: 0.5937\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6609 - accuracy: 0.6017 - val_loss: 0.6784 - val_accuracy: 0.5563\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 9s 131us/step - loss: 0.6598 - accuracy: 0.6050 - val_loss: 0.6754 - val_accuracy: 0.5713\n",
      "y2_pred:  [[0.50170135]\n",
      " [0.623675  ]\n",
      " [0.48980105]\n",
      " ...\n",
      " [0.61257935]\n",
      " [0.44020137]\n",
      " [0.5675329 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7368845843422115\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6893 - accuracy: 0.5594 - val_loss: 0.6766 - val_accuracy: 0.6031\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.6685 - accuracy: 0.5912 - val_loss: 0.6743 - val_accuracy: 0.5646\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.6652 - accuracy: 0.5933 - val_loss: 0.6766 - val_accuracy: 0.5729\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6631 - accuracy: 0.5951 - val_loss: 0.6874 - val_accuracy: 0.5563\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6623 - accuracy: 0.5970 - val_loss: 0.6662 - val_accuracy: 0.6057\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6610 - accuracy: 0.6010 - val_loss: 0.6669 - val_accuracy: 0.5747\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6603 - accuracy: 0.6003 - val_loss: 0.6736 - val_accuracy: 0.5877\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 7s 98us/step - loss: 0.6596 - accuracy: 0.6023 - val_loss: 0.6655 - val_accuracy: 0.5799\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6594 - accuracy: 0.6010 - val_loss: 0.6921 - val_accuracy: 0.5799\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 7s 99us/step - loss: 0.6596 - accuracy: 0.6018 - val_loss: 0.6829 - val_accuracy: 0.5802\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.6586 - accuracy: 0.6024 - val_loss: 0.6827 - val_accuracy: 0.5637\n",
      "y2_pred:  [[0.4335248 ]\n",
      " [0.523316  ]\n",
      " [0.60294074]\n",
      " ...\n",
      " [0.72654706]\n",
      " [0.6039192 ]\n",
      " [0.51633936]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7213478611783697\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6887 - accuracy: 0.5713 - val_loss: 0.6981 - val_accuracy: 0.4883\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.6696 - accuracy: 0.5865 - val_loss: 0.6915 - val_accuracy: 0.5381\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 97us/step - loss: 0.6680 - accuracy: 0.5925 - val_loss: 0.6981 - val_accuracy: 0.5252\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.6655 - accuracy: 0.5924 - val_loss: 0.6991 - val_accuracy: 0.5166\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6637 - accuracy: 0.5956 - val_loss: 0.7155 - val_accuracy: 0.5011\n",
      "y2_pred:  [[0.5693573 ]\n",
      " [0.32609928]\n",
      " [0.60892415]\n",
      " ...\n",
      " [0.5754552 ]\n",
      " [0.60892415]\n",
      " [0.4877403 ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.6184422921711057\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6972 - accuracy: 0.5514 - val_loss: 0.6842 - val_accuracy: 0.5427\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6729 - accuracy: 0.5805 - val_loss: 0.7206 - val_accuracy: 0.4731\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6696 - accuracy: 0.5868 - val_loss: 0.7000 - val_accuracy: 0.5004\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.6688 - accuracy: 0.5886 - val_loss: 0.6972 - val_accuracy: 0.5239\n",
      "y2_pred:  [[0.6346507 ]\n",
      " [0.5266065 ]\n",
      " [0.59696245]\n",
      " ...\n",
      " [0.47826993]\n",
      " [0.49232036]\n",
      " [0.49437562]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7437449556093624\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6833 - accuracy: 0.5641 - val_loss: 0.6921 - val_accuracy: 0.4918\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6712 - accuracy: 0.5824 - val_loss: 0.6752 - val_accuracy: 0.5825\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6688 - accuracy: 0.5856 - val_loss: 0.6822 - val_accuracy: 0.5745\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6671 - accuracy: 0.5906 - val_loss: 0.6833 - val_accuracy: 0.5696\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 116us/step - loss: 0.6657 - accuracy: 0.5924 - val_loss: 0.6682 - val_accuracy: 0.5923\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6652 - accuracy: 0.5945 - val_loss: 0.6730 - val_accuracy: 0.5860\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.6645 - accuracy: 0.5960 - val_loss: 0.6884 - val_accuracy: 0.5536\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6637 - accuracy: 0.5959 - val_loss: 0.6696 - val_accuracy: 0.5925\n",
      "y2_pred:  [[0.6181746 ]\n",
      " [0.43544137]\n",
      " [0.76263106]\n",
      " ...\n",
      " [0.8631578 ]\n",
      " [0.7486564 ]\n",
      " [0.395567  ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.5076674737691687\n",
      "pod:  [0.7177728464797256, 0.6998184385717168, 0.5667877320419693, 0.6575867635189669, 0.5744552058111381, 0.7368845843422115, 0.7213478611783697, 0.6184422921711057, 0.7437449556093624, 0.5076674737691687]\n",
      "pof:  [0.48493259318001586, 0.4986122125297383, 0.380253766851705, 0.4674861221252974, 0.41019032513877873, 0.6042823156225218, 0.6911181601903251, 0.7279936558287073, 0.6613127106880825, 0.35554233591116396]\n",
      "auc:  [0.6164201266498548, 0.6006031130209892, 0.5932669825951322, 0.5950503206968347, 0.5821324403361796, 0.5663011343598447, 0.5151148504940223, 0.4452243181711992, 0.5412161224606399, 0.5760625689290023]\n",
      "tn_list:  [2598, 2529, 3126, 2686, 2975, 1996, 1558, 1372, 1708, 3250]\n",
      "fp_list:  [2446, 2515, 1918, 2358, 2069, 3048, 3486, 3672, 3335, 1793]\n",
      "fn_list:  [1399, 1488, 2147, 1697, 2109, 1304, 1381, 1891, 1270, 2440]\n",
      "tp_list:  [3558, 3469, 2809, 3259, 2847, 3652, 3575, 3065, 3686, 2516]\n",
      "23798 26640 17126 32436\n"
     ]
    }
   ],
   "source": [
    "df_x_train_chi = pd.DataFrame(x_train_chi)\n",
    "df_x_train_chi.head()\n",
    "\n",
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print('tn_list: ',tn_list)\n",
    "print ('fp_list: ',fp_list)\n",
    "print ('fn_list: ',fn_list)\n",
    "print ('tp_list: ',tp_list)\n",
    "\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 42s 585us/step - loss: 0.6856 - acc: 0.5517 - val_loss: 0.6589 - val_acc: 0.6233\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 40s 558us/step - loss: 0.6799 - acc: 0.5666 - val_loss: 0.6571 - val_acc: 0.6269\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 40s 551us/step - loss: 0.6780 - acc: 0.5701 - val_loss: 0.6613 - val_acc: 0.6286\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 40s 555us/step - loss: 0.6768 - acc: 0.5737 - val_loss: 0.6644 - val_acc: 0.6029\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 40s 559us/step - loss: 0.6755 - acc: 0.5763 - val_loss: 0.6619 - val_acc: 0.6101\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.6806536211418196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 41s 576us/step - loss: 0.6862 - acc: 0.5474 - val_loss: 0.6579 - val_acc: 0.6171\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 41s 564us/step - loss: 0.6798 - acc: 0.5672 - val_loss: 0.6656 - val_acc: 0.6218\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 41s 566us/step - loss: 0.6776 - acc: 0.5742 - val_loss: 0.6617 - val_acc: 0.6121\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 40s 549us/step - loss: 0.6755 - acc: 0.5776 - val_loss: 0.6519 - val_acc: 0.6244\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 41s 568us/step - loss: 0.6732 - acc: 0.5807 - val_loss: 0.6564 - val_acc: 0.6153\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7413758321565463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 580us/step - loss: 0.6845 - acc: 0.5534 - val_loss: 0.6567 - val_acc: 0.6246\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 40s 556us/step - loss: 0.6784 - acc: 0.5706 - val_loss: 0.6538 - val_acc: 0.6276\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 40s 558us/step - loss: 0.6761 - acc: 0.5755 - val_loss: 0.6661 - val_acc: 0.6059\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 40s 560us/step - loss: 0.6748 - acc: 0.5785 - val_loss: 0.6734 - val_acc: 0.5883\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 41s 574us/step - loss: 0.6730 - acc: 0.5820 - val_loss: 0.6683 - val_acc: 0.5984\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.735270379338176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 41s 563us/step - loss: 0.6870 - acc: 0.5476 - val_loss: 0.6688 - val_acc: 0.6064\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 40s 561us/step - loss: 0.6801 - acc: 0.5670 - val_loss: 0.6624 - val_acc: 0.6231\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 597us/step - loss: 0.6779 - acc: 0.5706 - val_loss: 0.6634 - val_acc: 0.6135\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 572us/step - loss: 0.6757 - acc: 0.5768 - val_loss: 0.6574 - val_acc: 0.6237\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 612us/step - loss: 0.6740 - acc: 0.5790 - val_loss: 0.6485 - val_acc: 0.6300\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.3878127522195319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 596us/step - loss: 0.6857 - acc: 0.5503 - val_loss: 0.6880 - val_acc: 0.5386\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 604us/step - loss: 0.6791 - acc: 0.5707 - val_loss: 0.6655 - val_acc: 0.6106\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 596us/step - loss: 0.6766 - acc: 0.5754 - val_loss: 0.6561 - val_acc: 0.6234\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 613us/step - loss: 0.6747 - acc: 0.5794 - val_loss: 0.6514 - val_acc: 0.6282\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 603us/step - loss: 0.6727 - acc: 0.5836 - val_loss: 0.6534 - val_acc: 0.6264\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.4963680387409201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 628us/step - loss: 0.6839 - acc: 0.5558 - val_loss: 0.6641 - val_acc: 0.6249\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.6770 - acc: 0.5745 - val_loss: 0.6596 - val_acc: 0.6264\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 604us/step - loss: 0.6749 - acc: 0.5795 - val_loss: 0.6533 - val_acc: 0.6253\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 45s 632us/step - loss: 0.6729 - acc: 0.5831 - val_loss: 0.6591 - val_acc: 0.6231\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 615us/step - loss: 0.6717 - acc: 0.5850 - val_loss: 0.6640 - val_acc: 0.6061\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7199354317998385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 604us/step - loss: 0.6854 - acc: 0.5461 - val_loss: 0.6929 - val_acc: 0.5262\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 45s 622us/step - loss: 0.6780 - acc: 0.5678 - val_loss: 0.6813 - val_acc: 0.5742\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 0.6738 - acc: 0.5754 - val_loss: 0.6730 - val_acc: 0.5980\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 578us/step - loss: 0.6708 - acc: 0.5809 - val_loss: 0.6654 - val_acc: 0.6089\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6685 - acc: 0.5884 - val_loss: 0.6673 - val_acc: 0.6027\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7110573042776432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 619us/step - loss: 0.6872 - acc: 0.5428 - val_loss: 0.6717 - val_acc: 0.6023\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 0.6820 - acc: 0.5641 - val_loss: 0.6682 - val_acc: 0.6054\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 589us/step - loss: 0.6801 - acc: 0.5683 - val_loss: 0.6704 - val_acc: 0.5948\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.6786 - acc: 0.5723 - val_loss: 0.6662 - val_acc: 0.6018\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 594us/step - loss: 0.6770 - acc: 0.5750 - val_loss: 0.6714 - val_acc: 0.5891\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.5034301856335754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 588us/step - loss: 0.6866 - acc: 0.5482 - val_loss: 0.7013 - val_acc: 0.4853\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 589us/step - loss: 0.6812 - acc: 0.5652 - val_loss: 0.6709 - val_acc: 0.5951\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 580us/step - loss: 0.6781 - acc: 0.5726 - val_loss: 0.6612 - val_acc: 0.6154\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 586us/step - loss: 0.6759 - acc: 0.5746 - val_loss: 0.6700 - val_acc: 0.5910\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 608us/step - loss: 0.6745 - acc: 0.5779 - val_loss: 0.6675 - val_acc: 0.5919\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.5389426957223568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.6881 - acc: 0.5433 - val_loss: 0.6787 - val_acc: 0.5746\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 580us/step - loss: 0.6829 - acc: 0.5584 - val_loss: 0.6535 - val_acc: 0.6284\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 593us/step - loss: 0.6805 - acc: 0.5684 - val_loss: 0.6714 - val_acc: 0.5977\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 574us/step - loss: 0.6784 - acc: 0.5707 - val_loss: 0.6600 - val_acc: 0.6237\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 578us/step - loss: 0.6762 - acc: 0.5765 - val_loss: 0.6618 - val_acc: 0.6233\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.4271589991928975\n",
      "pod:  [0.6806536211418196, 0.7413758321565463, 0.735270379338176, 0.3878127522195319, 0.4963680387409201, 0.7199354317998385, 0.7110573042776432, 0.5034301856335754, 0.5389426957223568, 0.4271589991928975]\n",
      "pof:  [0.5380650277557494, 0.5965503568596352, 0.6332275971451229, 0.2726011102299762, 0.4034496431403648, 0.6829896907216495, 0.6984536082474226, 0.45955590800951623, 0.329962324013484, 0.2494546896688479]\n",
      "auc:  [0.5712942966930352, 0.5724127376484556, 0.5510213910965265, 0.5576058209947778, 0.5464591978002777, 0.5184728705390945, 0.5063018480151104, 0.5219371388120296, 0.6044901858544363, 0.5888521547620248]\n",
      "25903 24535 20112 29450\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6dd550951f6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_chi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear']} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), tuned_parameters, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
