{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df_op=result_df\n",
    "\n",
    "X=result_df_op\n",
    "y=churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>drop_vce_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>infobase</th>\n",
       "      <th>HHstatin</th>\n",
       "      <th>dwllsize</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556517</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.028551</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017520</td>\n",
       "      <td>-0.164801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>-0.107643</td>\n",
       "      <td>-0.112533</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.060797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.400201</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015816</td>\n",
       "      <td>0.751490</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.028216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.028551</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.081092</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.095326</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.270162</td>\n",
       "      <td>-0.339154</td>\n",
       "      <td>-0.136630</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.037101</td>\n",
       "      <td>0.066336</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_Mean  mou_Mean  totmrc_Mean   da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "0  0.000000  0.556517     0.017520 -0.022460    -0.015257    -0.014783   \n",
       "1  0.017520 -0.164801     0.000000 -0.022460     0.045300    -0.107643   \n",
       "2 -0.015816  0.751490     0.094113  0.047296    -0.015257    -0.014783   \n",
       "3 -1.081092  0.199842     0.017520  0.047296    -0.015257    -0.014783   \n",
       "4 -0.270162 -0.339154    -0.136630  0.047296    -0.015257    -0.014783   \n",
       "\n",
       "   vceovr_Mean  datovr_Mean  roam_Mean  drop_vce_Mean  ...  infobase  \\\n",
       "0    -0.033784     0.018887   0.014593      -0.011647  ... -0.025602   \n",
       "1    -0.112533     0.018887   0.014593      -0.060797  ... -0.025602   \n",
       "2    -0.033784     0.018887   0.014593      -0.028216  ... -0.025602   \n",
       "3    -0.033784     0.018887   0.014593       0.175511  ... -0.025602   \n",
       "4    -0.033784     0.018887   0.014593       0.063701  ... -0.025602   \n",
       "\n",
       "   HHstatin  dwllsize    ethnic    kid0_2    kid3_5   kid6_10  kid11_15  \\\n",
       "0 -0.036986 -0.045587 -0.028551 -0.002808 -0.000904  0.002464   0.00339   \n",
       "1 -0.036986 -0.045587 -0.400201 -0.002808 -0.000904  0.002464   0.00339   \n",
       "2 -0.036986 -0.045587 -0.028551 -0.002808  0.043076  0.002464   0.00339   \n",
       "3 -0.036986  0.052330  0.018434  0.095326 -0.000904  0.002464   0.00339   \n",
       "4 -0.036986 -0.037101  0.066336 -0.002808 -0.000904  0.002464   0.00339   \n",
       "\n",
       "   kid16_17  creditcd  \n",
       "0 -0.001641 -0.021631  \n",
       "1 -0.001641 -0.021631  \n",
       "2 -0.001641 -0.021631  \n",
       "3 -0.001641 -0.021631  \n",
       "4 -0.001641 -0.021631  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install mlencoders\n",
    "from mlencoders.weight_of_evidence_encoder import WeightOfEvidenceEncoder\n",
    "\n",
    "enc = WeightOfEvidenceEncoder(cols=X.columns)\n",
    "X_encoded = enc.fit_transform(X, y)\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>drop_vce_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>infobase</th>\n",
       "      <th>HHstatin</th>\n",
       "      <th>dwllsize</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.556517</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.751490</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.095326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_Mean  mou_Mean  totmrc_Mean   da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "0   0.00000  0.556517     0.017520  0.000000       0.0000          0.0   \n",
       "1   0.01752  0.000000     0.000000  0.000000       0.0453          0.0   \n",
       "2   0.00000  0.751490     0.094113  0.047296       0.0000          0.0   \n",
       "3   0.00000  0.199842     0.017520  0.047296       0.0000          0.0   \n",
       "4   0.00000  0.000000     0.000000  0.047296       0.0000          0.0   \n",
       "\n",
       "   vceovr_Mean  datovr_Mean  roam_Mean  drop_vce_Mean  ...  infobase  \\\n",
       "0          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "1          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "2          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "3          0.0     0.018887   0.014593       0.175511  ...       0.0   \n",
       "4          0.0     0.018887   0.014593       0.063701  ...       0.0   \n",
       "\n",
       "   HHstatin  dwllsize    ethnic    kid0_2    kid3_5   kid6_10  kid11_15  \\\n",
       "0       0.0   0.00000  0.000000  0.000000  0.000000  0.002464   0.00339   \n",
       "1       0.0   0.00000  0.000000  0.000000  0.000000  0.002464   0.00339   \n",
       "2       0.0   0.00000  0.000000  0.000000  0.043076  0.002464   0.00339   \n",
       "3       0.0   0.05233  0.018434  0.095326  0.000000  0.002464   0.00339   \n",
       "4       0.0   0.00000  0.066336  0.000000  0.000000  0.002464   0.00339   \n",
       "\n",
       "   kid16_17  creditcd  \n",
       "0       0.0       0.0  \n",
       "1       0.0       0.0  \n",
       "2       0.0       0.0  \n",
       "3       0.0       0.0  \n",
       "4       0.0       0.0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded[X_encoded < 0]=0\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>adjmou</td>\n",
       "      <td>4469.047321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>totmou</td>\n",
       "      <td>4321.946233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>avgqty</td>\n",
       "      <td>3940.782983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>avgrev</td>\n",
       "      <td>3117.288810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mouowylisv_Mean</td>\n",
       "      <td>2820.734757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rev_Mean</td>\n",
       "      <td>2749.843626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>adjqty</td>\n",
       "      <td>2368.620739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>totcalls</td>\n",
       "      <td>2181.358870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mou_Mean</td>\n",
       "      <td>2116.792847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>avgmou</td>\n",
       "      <td>1829.590695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mou_rvce_Mean</td>\n",
       "      <td>1502.044842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mouiwylisv_Mean</td>\n",
       "      <td>1492.836308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mou_opkv_Mean</td>\n",
       "      <td>1378.265845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>eqpdays</td>\n",
       "      <td>1197.404702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ovrrev_Mean</td>\n",
       "      <td>1006.440979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vceovr_Mean</td>\n",
       "      <td>963.531316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mou_peav_Mean</td>\n",
       "      <td>821.183143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>totmrc_Mean</td>\n",
       "      <td>810.422128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>avg3mou</td>\n",
       "      <td>658.711431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>adjrev</td>\n",
       "      <td>586.280936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mou_cvce_Mean</td>\n",
       "      <td>575.880369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cc_mou_Mean</td>\n",
       "      <td>574.177708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>months</td>\n",
       "      <td>556.896933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>totrev</td>\n",
       "      <td>515.241620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ovrmou_Mean</td>\n",
       "      <td>440.476838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>avg6mou</td>\n",
       "      <td>373.066811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>avg3qty</td>\n",
       "      <td>369.092859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>attempt_Mean</td>\n",
       "      <td>367.833902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plcd_vce_Mean</td>\n",
       "      <td>358.747538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roam_Mean</td>\n",
       "      <td>291.596940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>comp_dat_Mean</td>\n",
       "      <td>5.563177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>infobase</td>\n",
       "      <td>5.006212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>plcd_dat_Mean</td>\n",
       "      <td>4.487070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>asl_flag</td>\n",
       "      <td>3.761008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>adults</td>\n",
       "      <td>3.693284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>prizm_social_one</td>\n",
       "      <td>3.414378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>opk_dat_Mean</td>\n",
       "      <td>3.028874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>marital</td>\n",
       "      <td>2.918636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>income</td>\n",
       "      <td>2.664919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>peak_dat_Mean</td>\n",
       "      <td>2.471720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>dwllsize</td>\n",
       "      <td>2.376925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ownrent</td>\n",
       "      <td>2.068909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>threeway_Mean</td>\n",
       "      <td>2.037977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>dwlltype</td>\n",
       "      <td>1.967053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>HHstatin</td>\n",
       "      <td>1.949548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drop_dat_Mean</td>\n",
       "      <td>1.248045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>creditcd</td>\n",
       "      <td>1.007975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kid0_2</td>\n",
       "      <td>0.868184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>callfwdv_Mean</td>\n",
       "      <td>0.270322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>numbcars</td>\n",
       "      <td>0.227427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kid3_5</td>\n",
       "      <td>0.093833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blck_dat_Mean</td>\n",
       "      <td>0.058090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>kid16_17</td>\n",
       "      <td>0.045588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unan_dat_Mean</td>\n",
       "      <td>0.040383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>new_cell</td>\n",
       "      <td>0.002731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kid11_15</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kid6_10</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>forgntvl</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rv</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature       Scores\n",
       "53            adjmou  4469.047321\n",
       "50            totmou  4321.946233\n",
       "57            avgqty  3940.782983\n",
       "55            avgrev  3117.288810\n",
       "30   mouowylisv_Mean  2820.734757\n",
       "0           rev_Mean  2749.843626\n",
       "54            adjqty  2368.620739\n",
       "49          totcalls  2181.358870\n",
       "1           mou_Mean  2116.792847\n",
       "56            avgmou  1829.590695\n",
       "28     mou_rvce_Mean  1502.044842\n",
       "32   mouiwylisv_Mean  1492.836308\n",
       "39     mou_opkv_Mean  1378.265845\n",
       "74           eqpdays  1197.404702\n",
       "5        ovrrev_Mean  1006.440979\n",
       "6        vceovr_Mean   963.531316\n",
       "35     mou_peav_Mean   821.183143\n",
       "2        totmrc_Mean   810.422128\n",
       "58           avg3mou   658.711431\n",
       "52            adjrev   586.280936\n",
       "26     mou_cvce_Mean   575.880369\n",
       "23       cc_mou_Mean   574.177708\n",
       "46            months   556.896933\n",
       "51            totrev   515.241620\n",
       "4        ovrmou_Mean   440.476838\n",
       "61           avg6mou   373.066811\n",
       "59           avg3qty   369.092859\n",
       "42      attempt_Mean   367.833902\n",
       "15     plcd_vce_Mean   358.747538\n",
       "8          roam_Mean   291.596940\n",
       "..               ...          ...\n",
       "20     comp_dat_Mean     5.563177\n",
       "86          infobase     5.006212\n",
       "16     plcd_dat_Mean     4.487070\n",
       "77          asl_flag     3.761008\n",
       "70            adults     3.693284\n",
       "78  prizm_social_one     3.414378\n",
       "38      opk_dat_Mean     3.028874\n",
       "85           marital     2.918636\n",
       "71            income     2.664919\n",
       "34     peak_dat_Mean     2.471720\n",
       "88          dwllsize     2.376925\n",
       "83           ownrent     2.068909\n",
       "25     threeway_Mean     2.037977\n",
       "84          dwlltype     1.967053\n",
       "87          HHstatin     1.949548\n",
       "10     drop_dat_Mean     1.248045\n",
       "95          creditcd     1.007975\n",
       "90            kid0_2     0.868184\n",
       "44     callfwdv_Mean     0.270322\n",
       "72          numbcars     0.227427\n",
       "91            kid3_5     0.093833\n",
       "12     blck_dat_Mean     0.058090\n",
       "94          kid16_17     0.045588\n",
       "14     unan_dat_Mean     0.040383\n",
       "75          new_cell     0.002731\n",
       "67             truck     0.000947\n",
       "93          kid11_15     0.000871\n",
       "92           kid6_10     0.000337\n",
       "73          forgntvl     0.000308\n",
       "68                rv     0.000180\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X_encoded\n",
    "y_train=y\n",
    "#Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_feature = SelectKBest(chi2, k=90).fit(X_train, y_train)\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                     'Scores':select_feature.scores_})\n",
    "selected_features_df.sort_values(by='Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 90)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y\n",
    "x_train_chi = select_feature.transform(X_train)\n",
    "x_train_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43529 6909 22937 26625\n",
      "pod:  0.5372059238933053\n",
      "pof:  0.13698005472064714\n",
      "AUC:  0.7001129345863291\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 100 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 14.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var_smoothing': 1e-09}\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "0.5169666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "classifier=GaussianNB();\n",
    "#params = {\n",
    "#          \"priors\" : \"None\",\n",
    "#          \"var_smoothing\" : 1e-9\n",
    "#}\n",
    "#create an list of var_smoothing to cross validate\n",
    "#steps = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "grid = GridSearchCV(estimator=classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "{'var_smoothing': 1e-09}\n",
    "GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "0.51704\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43513 6925 22451 27111\n",
      "pod:  0.5470118235745127\n",
      "pof:  0.1372972758634363\n",
      "AUC:  0.7048572738555382\n",
      "accuracy:  0.70624\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "\n",
    "classifier.fit(x_train_chi,y_train)\n",
    "\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=10, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 5)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    " #accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "accuracy=(26836+27976)/(27976+22462+22726+26836)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 16.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5, 'multi_class': 'ovr', 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "0.79606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},    \n",
    "    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}\n",
    "]\n",
    " \n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41123 9315 11080 38482\n",
      "pod:  0.776441628667124\n",
      "pof:  0.18468218406756812\n",
      "AUC:  0.7958797222997779\n",
      "accuracy:  0.79605\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 207.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
      "                     weights='distance')\n",
      "0.72368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "param_grid ={\n",
    "   'n_neighbors': [3,5,11,19],\n",
    "   'weights': ['uniform', 'distance'],\n",
    "   'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 42 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 410.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 1993.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], \n",
    "                     'gamma': [0.001, 0.01, 0.1, 1, 1e-3, 1e-4], \n",
    "                     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45856 4582 22368 27194\n",
      "pod:  0.5486864936846778\n",
      "pof:  0.09084420476624767\n",
      "AUC:  0.7289211444592151\n",
      "accuracy:  0.7305\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=11, p=2,\n",
    "                     weights='distance' )  \n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46533 3905 23033 26529\n",
      "pod:  0.5352689560550422\n",
      "pof:  0.07742178516198105\n",
      "AUC:  0.7289235854465306\n",
      "accuracy:  0.73062\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=19)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "for 19 \n",
    "46533 3905 23033 26529\n",
    "pod:  0.5352689560550422\n",
    "pof:  0.07742178516198105\n",
    "AUC:  0.7289235854465306\n",
    "accuracy:  0.73062\n",
    "    \n",
    "    \n",
    "    45856 4582 22369 27193\n",
    "pod:  0.5486663169363626\n",
    "pof:  0.09084420476624767\n",
    "AUC:  0.7289110560850575\n",
    "accuracy:  0.73049\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 44.0min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 82.0min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 113.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=80, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=12,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.78775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "#parameters = {'n_estimators': [4, 6, 9], \n",
    "#              'max_features': ['log2', 'sqrt','auto'], \n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [2, 3, 5, 10], \n",
    "#              'min_samples_split': [2, 3, 5],\n",
    "#              'min_samples_leaf': [1,5,8]\n",
    "#             }\n",
    "\n",
    "#model_params = {\n",
    "#    'n_estimators': [50, 150, 250],\n",
    "#    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "#    'min_samples_split': [2, 4, 6]\n",
    "#}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 80, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=80, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=12,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.78775\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39476 10962 10113 39449\n",
      "pod:  0.7959525442879626\n",
      "pof:  0.21733613545342798\n",
      "AUC:  0.7893082044172672\n",
      "accuracy:  0.78925\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(bootstrap= True, max_depth= 80, max_features= 3, min_samples_leaf= 3, min_samples_split= 12, n_estimators= 1000)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'bootstrap': True, 'max_depth': 100, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 1000}\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=100, max_features=3, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "========================\n",
    "39671 10767 10370 39192\n",
    "pod:  0.7907671199709455\n",
    "pof:  0.213470002775685\n",
    "AUC:  0.7886485585976302\n",
    "accuracy:  0.78863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40436 10002 16269 33293\n",
      "pod:  0.6717444816593358\n",
      "pof:  0.19830286688607796\n",
      "AUC:  0.7367208073866289\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier =RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   16.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'splitter': 'best'}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "0.67634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "\n",
    "\n",
    "#param_grid={\n",
    "#    'criterion': ['gini', 'entropy'],\n",
    "#    'splitter': ['best', 'random'],\n",
    "#    'min_samples_split' : range(10,500,20),\n",
    "#    'max_depth': [1, 2, 3, 4, 5, 20],\n",
    "#    'max_features': [1, 2, 3, 4],\n",
    "#    'max_leaf_nodes': list(range(2, 100)), \n",
    "#    'min_samples_split': [2, 3, 4]\n",
    "#} \n",
    "\n",
    "param_grid={\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random']\n",
    "} \n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34866 15572 16514 33048\n",
      "pod:  0.6668011783221016\n",
      "pof:  0.3087354772195567\n",
      "AUC:  0.6790328505512724\n",
      "accuracy:  0.67914\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34767 15671 16767 32795\n",
      "pod:  0.6616964609983456\n",
      "pof:  0.3106982830405646\n",
      "AUC:  0.6754990889788904\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "0.79167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=10, n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40877 9561 11272 38290\n",
      "pod:  0.7725676929905977\n",
      "pof:  0.18955945913795155\n",
      "AUC:  0.7915041169263232\n",
      "accuracy:  0.79167\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40877 9561 11272 38290\n",
      "pod:  0.7725676929905977\n",
      "pof:  0.18955945913795155\n",
      "AUC:  0.7915041169263232\n",
      "accuracy:  0.79167\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 111us/step - loss: 0.4494 - accuracy: 0.7878 - val_loss: 0.4044 - val_accuracy: 0.8203\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 7s 100us/step - loss: 0.4284 - accuracy: 0.8005 - val_loss: 0.3944 - val_accuracy: 0.8284\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.4239 - accuracy: 0.8029 - val_loss: 0.3880 - val_accuracy: 0.8322\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 102us/step - loss: 0.4204 - accuracy: 0.8061 - val_loss: 0.3971 - val_accuracy: 0.8291\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 106us/step - loss: 0.4188 - accuracy: 0.8060 - val_loss: 0.3865 - val_accuracy: 0.8317\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 8s 114us/step - loss: 0.4176 - accuracy: 0.8062 - val_loss: 0.3919 - val_accuracy: 0.8267\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 7s 103us/step - loss: 0.4167 - accuracy: 0.8066 - val_loss: 0.3882 - val_accuracy: 0.8284\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 8s 104us/step - loss: 0.4149 - accuracy: 0.8082 - val_loss: 0.3853 - val_accuracy: 0.8313\n",
      "Epoch 9/30\n",
      "71999/71999 [==============================] - 7s 101us/step - loss: 0.4144 - accuracy: 0.8077 - val_loss: 0.3860 - val_accuracy: 0.8314\n",
      "Epoch 10/30\n",
      "71999/71999 [==============================] - 7s 104us/step - loss: 0.4135 - accuracy: 0.8077 - val_loss: 0.3880 - val_accuracy: 0.8321\n",
      "Epoch 11/30\n",
      "71999/71999 [==============================] - 7s 100us/step - loss: 0.4129 - accuracy: 0.8086 - val_loss: 0.3942 - val_accuracy: 0.8293\n",
      "y2_pred:  [[0.86421454]\n",
      " [0.06518376]\n",
      " [0.19098222]\n",
      " ...\n",
      " [0.15542573]\n",
      " [0.9250479 ]\n",
      " [0.34960198]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.76982045592092\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 117us/step - loss: 0.4515 - accuracy: 0.7901 - val_loss: 0.3973 - val_accuracy: 0.8256\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 8s 112us/step - loss: 0.4287 - accuracy: 0.8022 - val_loss: 0.3910 - val_accuracy: 0.8279\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 104us/step - loss: 0.4235 - accuracy: 0.8052 - val_loss: 0.3876 - val_accuracy: 0.8303\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 7s 101us/step - loss: 0.4217 - accuracy: 0.8058 - val_loss: 0.3876 - val_accuracy: 0.8302\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.4190 - accuracy: 0.8067 - val_loss: 0.3883 - val_accuracy: 0.8289\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 7s 101us/step - loss: 0.4180 - accuracy: 0.8063 - val_loss: 0.3826 - val_accuracy: 0.8321\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.4169 - accuracy: 0.8079 - val_loss: 0.3878 - val_accuracy: 0.8306\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 7s 101us/step - loss: 0.4168 - accuracy: 0.8071 - val_loss: 0.3886 - val_accuracy: 0.8256\n",
      "Epoch 9/30\n",
      "71999/71999 [==============================] - 8s 110us/step - loss: 0.4154 - accuracy: 0.8085 - val_loss: 0.3871 - val_accuracy: 0.8294\n",
      "y2_pred:  [[0.03241193]\n",
      " [0.6180359 ]\n",
      " [0.6712352 ]\n",
      " ...\n",
      " [0.29579428]\n",
      " [0.2927305 ]\n",
      " [0.14033954]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8259027637684083\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.4525 - accuracy: 0.7861 - val_loss: 0.4023 - val_accuracy: 0.8191\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.4309 - accuracy: 0.8009 - val_loss: 0.3944 - val_accuracy: 0.8277\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4248 - accuracy: 0.8033 - val_loss: 0.3871 - val_accuracy: 0.8279\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.4216 - accuracy: 0.8055 - val_loss: 0.3864 - val_accuracy: 0.8292\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.4194 - accuracy: 0.8061 - val_loss: 0.3845 - val_accuracy: 0.8319\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4184 - accuracy: 0.8063 - val_loss: 0.3912 - val_accuracy: 0.8271\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4170 - accuracy: 0.8067 - val_loss: 0.3836 - val_accuracy: 0.8306\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 9s 126us/step - loss: 0.4155 - accuracy: 0.8084 - val_loss: 0.3864 - val_accuracy: 0.8308\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4146 - accuracy: 0.8087 - val_loss: 0.3939 - val_accuracy: 0.8246\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.4142 - accuracy: 0.8098 - val_loss: 0.3862 - val_accuracy: 0.8293\n",
      "y2_pred:  [[0.7665529 ]\n",
      " [0.9846945 ]\n",
      " [0.94929194]\n",
      " ...\n",
      " [0.04085267]\n",
      " [0.53423333]\n",
      " [0.2617182 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.801452784503632\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.4524 - accuracy: 0.7863 - val_loss: 0.4027 - val_accuracy: 0.8220\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.4297 - accuracy: 0.8014 - val_loss: 0.3973 - val_accuracy: 0.8218\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4247 - accuracy: 0.8041 - val_loss: 0.3877 - val_accuracy: 0.8276\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.4227 - accuracy: 0.8046 - val_loss: 0.3879 - val_accuracy: 0.8265\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4208 - accuracy: 0.8060 - val_loss: 0.3878 - val_accuracy: 0.8266\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.4189 - accuracy: 0.8064 - val_loss: 0.3951 - val_accuracy: 0.8267\n",
      "y2_pred:  [[0.912729  ]\n",
      " [0.9827179 ]\n",
      " [0.96704733]\n",
      " ...\n",
      " [0.12177095]\n",
      " [0.03974023]\n",
      " [0.9057908 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.8097255851493139\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4536 - accuracy: 0.7861 - val_loss: 0.4083 - val_accuracy: 0.8221\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4300 - accuracy: 0.8002 - val_loss: 0.3971 - val_accuracy: 0.8264\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.4252 - accuracy: 0.8043 - val_loss: 0.3888 - val_accuracy: 0.8279\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4232 - accuracy: 0.8050 - val_loss: 0.3898 - val_accuracy: 0.8265\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.4209 - accuracy: 0.8056 - val_loss: 0.3848 - val_accuracy: 0.8295\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.4194 - accuracy: 0.8061 - val_loss: 0.3929 - val_accuracy: 0.8272\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.4188 - accuracy: 0.8065 - val_loss: 0.3887 - val_accuracy: 0.8292\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4172 - accuracy: 0.8075 - val_loss: 0.3842 - val_accuracy: 0.8322\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 7s 101us/step - loss: 0.4157 - accuracy: 0.8081 - val_loss: 0.3850 - val_accuracy: 0.8313\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.4154 - accuracy: 0.8085 - val_loss: 0.3856 - val_accuracy: 0.8306\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.4147 - accuracy: 0.8095 - val_loss: 0.3899 - val_accuracy: 0.8290\n",
      "y2_pred:  [[0.6940745 ]\n",
      " [0.4851477 ]\n",
      " [0.1980331 ]\n",
      " ...\n",
      " [0.04030678]\n",
      " [0.02035874]\n",
      " [0.06303322]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8044794188861986\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.4527 - accuracy: 0.7874 - val_loss: 0.3969 - val_accuracy: 0.8232\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4316 - accuracy: 0.8009 - val_loss: 0.3905 - val_accuracy: 0.8269\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4269 - accuracy: 0.8040 - val_loss: 0.3963 - val_accuracy: 0.8224\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.4237 - accuracy: 0.8046 - val_loss: 0.3924 - val_accuracy: 0.8261\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.4212 - accuracy: 0.8056 - val_loss: 0.3913 - val_accuracy: 0.8259\n",
      "y2_pred:  [[0.9867902 ]\n",
      " [0.9132851 ]\n",
      " [0.90444493]\n",
      " ...\n",
      " [0.04237205]\n",
      " [0.14310274]\n",
      " [0.05800176]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8464487489911219\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4471 - accuracy: 0.7900 - val_loss: 0.4044 - val_accuracy: 0.8194\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4268 - accuracy: 0.8037 - val_loss: 0.3984 - val_accuracy: 0.8244\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.4217 - accuracy: 0.8055 - val_loss: 0.3958 - val_accuracy: 0.8257\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4195 - accuracy: 0.8067 - val_loss: 0.3887 - val_accuracy: 0.8286\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.4167 - accuracy: 0.8078 - val_loss: 0.3904 - val_accuracy: 0.8290\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 9s 128us/step - loss: 0.4158 - accuracy: 0.8084 - val_loss: 0.3887 - val_accuracy: 0.8306\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.4146 - accuracy: 0.8087 - val_loss: 0.3915 - val_accuracy: 0.8281\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4137 - accuracy: 0.8093 - val_loss: 0.3875 - val_accuracy: 0.8299\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.4126 - accuracy: 0.8100 - val_loss: 0.3950 - val_accuracy: 0.8269\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4125 - accuracy: 0.8103 - val_loss: 0.3952 - val_accuracy: 0.8276\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 7s 100us/step - loss: 0.4112 - accuracy: 0.8106 - val_loss: 0.3858 - val_accuracy: 0.8313\n",
      "Epoch 12/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4104 - accuracy: 0.8105 - val_loss: 0.3923 - val_accuracy: 0.8276\n",
      "Epoch 13/30\n",
      "72000/72000 [==============================] - 7s 102us/step - loss: 0.4102 - accuracy: 0.8122 - val_loss: 0.3897 - val_accuracy: 0.8287\n",
      "Epoch 14/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.4094 - accuracy: 0.8121 - val_loss: 0.3904 - val_accuracy: 0.8282\n",
      "y2_pred:  [[0.9717686 ]\n",
      " [0.38990545]\n",
      " [0.7386635 ]\n",
      " ...\n",
      " [0.26052105]\n",
      " [0.01380774]\n",
      " [0.01951942]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8391848264729621\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.4541 - accuracy: 0.7877 - val_loss: 0.4080 - val_accuracy: 0.8178\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4306 - accuracy: 0.8014 - val_loss: 0.3922 - val_accuracy: 0.8237\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.4250 - accuracy: 0.8039 - val_loss: 0.3882 - val_accuracy: 0.8284\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4222 - accuracy: 0.8055 - val_loss: 0.3978 - val_accuracy: 0.8256\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4206 - accuracy: 0.8067 - val_loss: 0.3919 - val_accuracy: 0.8284\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.4191 - accuracy: 0.8071 - val_loss: 0.3881 - val_accuracy: 0.8281\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.4179 - accuracy: 0.8072 - val_loss: 0.3913 - val_accuracy: 0.8283\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.4172 - accuracy: 0.8079 - val_loss: 0.3887 - val_accuracy: 0.8271\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4160 - accuracy: 0.8088 - val_loss: 0.3899 - val_accuracy: 0.8272\n",
      "y2_pred:  [[0.9724474 ]\n",
      " [0.2926361 ]\n",
      " [0.9999839 ]\n",
      " ...\n",
      " [0.07018253]\n",
      " [0.8815408 ]\n",
      " [0.74872255]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7495964487489911\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 128us/step - loss: 0.4523 - accuracy: 0.7862 - val_loss: 0.4381 - val_accuracy: 0.8003\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 116us/step - loss: 0.4320 - accuracy: 0.7994 - val_loss: 0.4045 - val_accuracy: 0.8172\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 118us/step - loss: 0.4274 - accuracy: 0.8019 - val_loss: 0.4036 - val_accuracy: 0.8165\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.4257 - accuracy: 0.8027 - val_loss: 0.4060 - val_accuracy: 0.8185\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.4241 - accuracy: 0.8034 - val_loss: 0.3997 - val_accuracy: 0.8213\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.4228 - accuracy: 0.8047 - val_loss: 0.4005 - val_accuracy: 0.8216\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 9s 129us/step - loss: 0.4214 - accuracy: 0.8045 - val_loss: 0.4159 - val_accuracy: 0.8154\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.4207 - accuracy: 0.8048 - val_loss: 0.4008 - val_accuracy: 0.8211\n",
      "y2_pred:  [[0.9849664 ]\n",
      " [0.23313561]\n",
      " [0.96815753]\n",
      " ...\n",
      " [0.14359888]\n",
      " [0.23747621]\n",
      " [0.05557331]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8030669895076675\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.4558 - accuracy: 0.7837 - val_loss: 0.3976 - val_accuracy: 0.8238\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.4354 - accuracy: 0.7978 - val_loss: 0.3963 - val_accuracy: 0.8238\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.4302 - accuracy: 0.8005 - val_loss: 0.3908 - val_accuracy: 0.8292\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.4275 - accuracy: 0.8020 - val_loss: 0.3918 - val_accuracy: 0.8253\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 9s 129us/step - loss: 0.4255 - accuracy: 0.8028 - val_loss: 0.3896 - val_accuracy: 0.8272\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 104us/step - loss: 0.4239 - accuracy: 0.8044 - val_loss: 0.3845 - val_accuracy: 0.8280\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.4225 - accuracy: 0.8040 - val_loss: 0.3961 - val_accuracy: 0.8228\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.4208 - accuracy: 0.8049 - val_loss: 0.4126 - val_accuracy: 0.8152\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.4204 - accuracy: 0.8048 - val_loss: 0.3899 - val_accuracy: 0.8278\n",
      "y2_pred:  [[0.43989807]\n",
      " [0.3748438 ]\n",
      " [0.9365127 ]\n",
      " ...\n",
      " [0.7568976 ]\n",
      " [0.51388043]\n",
      " [0.9608445 ]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7671509281678773\n",
      "pod:  [0.76982045592092, 0.8259027637684083, 0.801452784503632, 0.8097255851493139, 0.8044794188861986, 0.8464487489911219, 0.8391848264729621, 0.7495964487489911, 0.8030669895076675, 0.7671509281678773]\n",
      "pof:  [0.16514670896114195, 0.23750991276764472, 0.19310071371927043, 0.20479777954004758, 0.17981760507533703, 0.24801744647105473, 0.31225218080888184, 0.16732751784298175, 0.13305572080111044, 0.13642673012095974]\n",
      "auc:  [0.8023368734798889, 0.7941964255003817, 0.8041760353921809, 0.8024639028046332, 0.8123309069054307, 0.7992156512600336, 0.7634663228320401, 0.7911344654530047, 0.8350056343532786, 0.8153620990234587]\n",
      "tn_list:  [4211, 3846, 4070, 4011, 4137, 3793, 3469, 4200, 4372, 4355]\n",
      "fp_list:  [833, 1198, 974, 1033, 907, 1251, 1575, 844, 671, 688]\n",
      "fn_list:  [1141, 863, 984, 943, 969, 761, 797, 1241, 976, 1154]\n",
      "tp_list:  [3816, 4094, 3972, 4013, 3987, 4195, 4159, 3715, 3980, 3802]\n",
      "40464 9974 9829 39733\n"
     ]
    }
   ],
   "source": [
    "df_x_train_chi = pd.DataFrame(x_train_chi)\n",
    "df_x_train_chi.head()\n",
    "\n",
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print('tn_list: ',tn_list)\n",
    "print ('fp_list: ',fp_list)\n",
    "print ('fn_list: ',fn_list)\n",
    "print ('tp_list: ',tp_list)\n",
    "\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 51s 704us/step - loss: 0.6187 - acc: 0.6564 - val_loss: 0.5793 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 46s 636us/step - loss: 0.5814 - acc: 0.7124 - val_loss: 0.5842 - val_acc: 0.7198\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 46s 632us/step - loss: 0.5789 - acc: 0.7134 - val_loss: 0.5654 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 46s 633us/step - loss: 0.5810 - acc: 0.7135 - val_loss: 0.8698 - val_acc: 0.4088\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 46s 643us/step - loss: 0.5796 - acc: 0.7129 - val_loss: 0.5845 - val_acc: 0.7198\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7236231591688521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 51s 709us/step - loss: 0.6155 - acc: 0.6605 - val_loss: 0.5771 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 48s 662us/step - loss: 0.5816 - acc: 0.7133 - val_loss: 0.5778 - val_acc: 0.7171\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 47s 651us/step - loss: 0.5747 - acc: 0.7145 - val_loss: 0.5627 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 49s 674us/step - loss: 0.5741 - acc: 0.7136 - val_loss: 0.5693 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 48s 661us/step - loss: 0.5818 - acc: 0.7098 - val_loss: 0.6086 - val_acc: 0.7198\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7252370385313698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 49s 679us/step - loss: 0.6158 - acc: 0.6615 - val_loss: 0.6088 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 48s 664us/step - loss: 0.5777 - acc: 0.7139 - val_loss: 0.5650 - val_acc: 0.7198\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 49s 674us/step - loss: 0.5812 - acc: 0.7143 - val_loss: 0.5752 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 54s 753us/step - loss: 0.5814 - acc: 0.7122 - val_loss: 0.5756 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 49s 677us/step - loss: 0.5793 - acc: 0.7162 - val_loss: 0.5740 - val_acc: 0.7198\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7058111380145279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 49s 675us/step - loss: 0.6144 - acc: 0.6645 - val_loss: 0.5867 - val_acc: 0.7189\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 49s 680us/step - loss: 0.5790 - acc: 0.7115 - val_loss: 0.5777 - val_acc: 0.7198\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 49s 686us/step - loss: 0.5765 - acc: 0.7130 - val_loss: 0.5713 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 48s 672us/step - loss: 0.5783 - acc: 0.7122 - val_loss: 0.5653 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 47s 656us/step - loss: 0.5750 - acc: 0.7143 - val_loss: 0.5607 - val_acc: 0.7198\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7249798224374495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 51s 707us/step - loss: 0.6181 - acc: 0.6563 - val_loss: 0.5782 - val_acc: 0.7188\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 48s 670us/step - loss: 0.5785 - acc: 0.7109 - val_loss: 0.5692 - val_acc: 0.7228\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 48s 661us/step - loss: 0.5755 - acc: 0.7128 - val_loss: 0.6139 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 49s 675us/step - loss: 0.5746 - acc: 0.7128 - val_loss: 0.5719 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 49s 684us/step - loss: 0.5729 - acc: 0.7132 - val_loss: 0.5729 - val_acc: 0.7198\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7215496368038741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 48s 668us/step - loss: 0.6169 - acc: 0.6573 - val_loss: 0.5842 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 49s 678us/step - loss: 0.5792 - acc: 0.7125 - val_loss: 0.5720 - val_acc: 0.7198\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 50s 692us/step - loss: 0.5756 - acc: 0.7135 - val_loss: 0.5603 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 49s 681us/step - loss: 0.5750 - acc: 0.7132 - val_loss: 0.5731 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 48s 673us/step - loss: 0.5744 - acc: 0.7140 - val_loss: 0.5579 - val_acc: 0.7202\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7193301049233253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 48s 665us/step - loss: 0.6143 - acc: 0.6646 - val_loss: 0.5782 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 49s 679us/step - loss: 0.5809 - acc: 0.7095 - val_loss: 0.5854 - val_acc: 0.7198\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 51s 704us/step - loss: 0.5766 - acc: 0.7113 - val_loss: 0.6043 - val_acc: 0.7198\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 50s 692us/step - loss: 0.5767 - acc: 0.7108 - val_loss: 0.5654 - val_acc: 0.7198\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 48s 671us/step - loss: 0.5770 - acc: 0.7095 - val_loss: 0.5570 - val_acc: 0.7217\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6190476190476191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 48s 661us/step - loss: 0.6924 - acc: 0.5131 - val_loss: 0.6058 - val_acc: 0.7069\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 46s 645us/step - loss: 0.5830 - acc: 0.7096 - val_loss: 0.5740 - val_acc: 0.7224\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 47s 649us/step - loss: 0.5856 - acc: 0.7133 - val_loss: 0.5761 - val_acc: 0.7224\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 47s 659us/step - loss: 0.5856 - acc: 0.7153 - val_loss: 0.5806 - val_acc: 0.7224\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 47s 649us/step - loss: 0.5844 - acc: 0.7159 - val_loss: 0.5754 - val_acc: 0.7224\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.6834140435835351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 47s 654us/step - loss: 0.6205 - acc: 0.6557 - val_loss: 0.6555 - val_acc: 0.7177\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 46s 646us/step - loss: 0.5816 - acc: 0.7074 - val_loss: 0.6013 - val_acc: 0.7177\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 49s 683us/step - loss: 0.5781 - acc: 0.7109 - val_loss: 0.5726 - val_acc: 0.7177\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 47s 650us/step - loss: 0.5765 - acc: 0.7127 - val_loss: 0.5686 - val_acc: 0.7177\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 47s 653us/step - loss: 0.5768 - acc: 0.7106 - val_loss: 0.5650 - val_acc: 0.7177\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7374899112187248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 49s 679us/step - loss: 0.6145 - acc: 0.6615 - val_loss: 0.5607 - val_acc: 0.7198\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 46s 637us/step - loss: 0.5817 - acc: 0.7098 - val_loss: 0.5727 - val_acc: 0.7239\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 48s 665us/step - loss: 0.5788 - acc: 0.7117 - val_loss: 0.5622 - val_acc: 0.7239\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 47s 654us/step - loss: 0.5765 - acc: 0.7122 - val_loss: 0.5766 - val_acc: 0.7239\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 47s 657us/step - loss: 0.5767 - acc: 0.7122 - val_loss: 0.5707 - val_acc: 0.7239\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.733454398708636\n",
      "pod:  [0.7236231591688521, 0.7252370385313698, 0.7058111380145279, 0.7249798224374495, 0.7215496368038741, 0.7193301049233253, 0.6190476190476191, 0.6834140435835351, 0.7374899112187248, 0.733454398708636]\n",
      "pof:  [0.28786677240285485, 0.2971847739888977, 0.3021411578112609, 0.2908406026962728, 0.29401268834258526, 0.2914353687549564, 0.20697858842188738, 0.2789452815226011, 0.2774142375570097, 0.29545905215149715]\n",
      "auc:  [0.7178781933829987, 0.714026132271236, 0.7018349901016335, 0.7170696098705883, 0.7137684742306444, 0.7139473680841845, 0.7060345153128659, 0.702234381030467, 0.7300378368308575, 0.7189976732785694]\n",
      "36203 14235 14403 35159\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
