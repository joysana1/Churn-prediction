{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "#BOX-COX\n",
    "#lemda=0.5\n",
    "#num_df=(num_df**lemda)\n",
    "#num_df=num_df-1\n",
    "#num_df[num_df < 0]=0\n",
    "#num_df=num_df.div(lemda)\n",
    "#End BOX-COX\n",
    "#Z-score\n",
    "#num_df=(num_df-num_df.min())/(num_df.std(ddof=0)) ##Z-score\n",
    "\n",
    "#Log\n",
    "num_df=round(np.log(num_df.add(1)),2)\n",
    "num_df=num_df.replace([np.inf, -np.inf], np.nan)\n",
    "#End Log\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df[result_df < 0]=0\n",
    "\n",
    "result_df_op=result_df\n",
    "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
    "#result_df_op=np.nan_to_num(result_df_op)\n",
    "\n",
    "X=result_df_op\n",
    "#y=result_df['churn'] \n",
    "y=churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crclscod</td>\n",
       "      <td>1803.718570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mouiwylisv_Mean</td>\n",
       "      <td>543.543293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mou_opkv_Mean</td>\n",
       "      <td>490.794990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ccrndmou_Mean</td>\n",
       "      <td>474.621785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>iwylis_vce_Mean</td>\n",
       "      <td>451.771701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cc_mou_Mean</td>\n",
       "      <td>422.894192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>asl_flag</td>\n",
       "      <td>415.391673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mou_cvce_Mean</td>\n",
       "      <td>387.201183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custcare_Mean</td>\n",
       "      <td>376.980953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mou_rvce_Mean</td>\n",
       "      <td>341.931592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>opk_vce_Mean</td>\n",
       "      <td>340.848723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mou_peav_Mean</td>\n",
       "      <td>335.151915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mou_Mean</td>\n",
       "      <td>306.393340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mouowylisv_Mean</td>\n",
       "      <td>300.777692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>complete_Mean</td>\n",
       "      <td>293.585312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comp_vce_Mean</td>\n",
       "      <td>292.289639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>peak_vce_Mean</td>\n",
       "      <td>278.953029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>attempt_Mean</td>\n",
       "      <td>271.942043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plcd_vce_Mean</td>\n",
       "      <td>270.508733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>owylis_vce_Mean</td>\n",
       "      <td>268.834785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>recv_vce_Mean</td>\n",
       "      <td>240.697329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>hnd_webcap</td>\n",
       "      <td>226.846793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>unan_vce_Mean</td>\n",
       "      <td>205.231082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inonemin_Mean</td>\n",
       "      <td>201.831172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>avg3mou</td>\n",
       "      <td>201.305786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>eqpdays</td>\n",
       "      <td>196.419048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>callwait_Mean</td>\n",
       "      <td>187.236927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>lor</td>\n",
       "      <td>151.749136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>avg3qty</td>\n",
       "      <td>149.348801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vceovr_Mean</td>\n",
       "      <td>130.510981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>avg6qty</td>\n",
       "      <td>11.391180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unan_dat_Mean</td>\n",
       "      <td>11.011101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>refurb_new</td>\n",
       "      <td>10.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>adjrev</td>\n",
       "      <td>7.998187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>prizm_social_one</td>\n",
       "      <td>7.534467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>numbcars</td>\n",
       "      <td>6.715042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>totcalls</td>\n",
       "      <td>5.489226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>totrev</td>\n",
       "      <td>5.416160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>avg3rev</td>\n",
       "      <td>5.318513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>adjqty</td>\n",
       "      <td>4.862023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drop_dat_Mean</td>\n",
       "      <td>4.595589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>actvsubs</td>\n",
       "      <td>3.281815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>avgmou</td>\n",
       "      <td>3.108271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>creditcd</td>\n",
       "      <td>2.698027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>callfwdv_Mean</td>\n",
       "      <td>2.650712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>totmou</td>\n",
       "      <td>2.111632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>avg6rev</td>\n",
       "      <td>1.894306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>adjmou</td>\n",
       "      <td>1.704058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recv_sms_Mean</td>\n",
       "      <td>1.550934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>forgntvl</td>\n",
       "      <td>1.494926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>avgqty</td>\n",
       "      <td>0.915564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.797717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kid0_2</td>\n",
       "      <td>0.610982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>new_cell</td>\n",
       "      <td>0.517556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>kid16_17</td>\n",
       "      <td>0.333963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kid3_5</td>\n",
       "      <td>0.248540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>avgrev</td>\n",
       "      <td>0.185670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rv</td>\n",
       "      <td>0.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kid11_15</td>\n",
       "      <td>0.010772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kid6_10</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature       Scores\n",
       "76          crclscod  1803.718570\n",
       "32   mouiwylisv_Mean   543.543293\n",
       "39     mou_opkv_Mean   490.794990\n",
       "22     ccrndmou_Mean   474.621785\n",
       "31   iwylis_vce_Mean   451.771701\n",
       "23       cc_mou_Mean   422.894192\n",
       "77          asl_flag   415.391673\n",
       "26     mou_cvce_Mean   387.201183\n",
       "21     custcare_Mean   376.980953\n",
       "28     mou_rvce_Mean   341.931592\n",
       "37      opk_vce_Mean   340.848723\n",
       "35     mou_peav_Mean   335.151915\n",
       "1           mou_Mean   306.393340\n",
       "30   mouowylisv_Mean   300.777692\n",
       "43     complete_Mean   293.585312\n",
       "19     comp_vce_Mean   292.289639\n",
       "33     peak_vce_Mean   278.953029\n",
       "42      attempt_Mean   271.942043\n",
       "15     plcd_vce_Mean   270.508733\n",
       "29   owylis_vce_Mean   268.834785\n",
       "17     recv_vce_Mean   240.697329\n",
       "82        hnd_webcap   226.846793\n",
       "13     unan_vce_Mean   205.231082\n",
       "24     inonemin_Mean   201.831172\n",
       "58           avg3mou   201.305786\n",
       "74           eqpdays   196.419048\n",
       "45     callwait_Mean   187.236927\n",
       "69               lor   151.749136\n",
       "59           avg3qty   149.348801\n",
       "6        vceovr_Mean   130.510981\n",
       "..               ...          ...\n",
       "62           avg6qty    11.391180\n",
       "14     unan_dat_Mean    11.011101\n",
       "81        refurb_new    10.910400\n",
       "52            adjrev     7.998187\n",
       "78  prizm_social_one     7.534467\n",
       "72          numbcars     6.715042\n",
       "49          totcalls     5.489226\n",
       "51            totrev     5.416160\n",
       "60           avg3rev     5.318513\n",
       "54            adjqty     4.862023\n",
       "10     drop_dat_Mean     4.595589\n",
       "48          actvsubs     3.281815\n",
       "56            avgmou     3.108271\n",
       "95          creditcd     2.698027\n",
       "44     callfwdv_Mean     2.650712\n",
       "50            totmou     2.111632\n",
       "63           avg6rev     1.894306\n",
       "53            adjmou     1.704058\n",
       "18     recv_sms_Mean     1.550934\n",
       "73          forgntvl     1.494926\n",
       "57            avgqty     0.915564\n",
       "67             truck     0.797717\n",
       "90            kid0_2     0.610982\n",
       "75          new_cell     0.517556\n",
       "94          kid16_17     0.333963\n",
       "91            kid3_5     0.248540\n",
       "55            avgrev     0.185670\n",
       "68                rv     0.027819\n",
       "93          kid11_15     0.010772\n",
       "92           kid6_10     0.000403\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X\n",
    "y_train=y\n",
    "#Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_feature = SelectKBest(chi2, k=80).fit(X_train, y_train)\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                     'Scores':select_feature.scores_})\n",
    "selected_features_df.sort_values(by='Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y\n",
    "x_train_chi = select_feature.transform(X)\n",
    "x_train_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var_smoothing': 1.873817422860383e-07}\n",
      "GaussianNB(priors=None, var_smoothing=1.873817422860383e-07)\n",
      "0.5508966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "classifier=GaussianNB();\n",
    "#params = {\n",
    "#          \"priors\" : \"None\",\n",
    "#          \"var_smoothing\" : 1e-9\n",
    "#}\n",
    "#create an list of var_smoothing to cross validate\n",
    "#steps = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "grid = GridSearchCV(estimator=classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27976 22462 22726 26836\n",
      "pod:  0.5414632177878214\n",
      "pof:  0.44533883183314166\n",
      "AUC:  0.5480621929773398\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB(priors=None, var_smoothing=1.873817422860383e-07)\n",
    "\n",
    "classifier.fit(x_train_chi,y_train)\n",
    "\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=10, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 5)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    " #accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "accuracy=(26836+27976)/(27976+22462+22726+26836)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 555.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 3133.4min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 3152.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.57708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},    \n",
    "    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}\n",
    "]\n",
    " \n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30042 20396 21896 27666\n",
      "pod:  0.5582099188894718\n",
      "pof:  0.4043776517704905\n",
      "AUC:  0.5769161335594907\n",
      "accuracy:  0.57708\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 88.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "param_grid ={\n",
    "   'n_neighbors': [3,5,11,19],\n",
    "   'weights': ['uniform', 'distance'],\n",
    "   'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 432.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "kernel = ['rbf']\n",
    "param_grid = {'C': Cs,'kernel' : kernel}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], \n",
    "                     'gamma': [0.001, 0.01, 0.1, 1, 1e-3, 1e-4], \n",
    "                     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29454 20984 21181 28381\n",
      "pod:  0.5726362939348695\n",
      "pof:  0.4160355287679924\n",
      "AUC:  0.5783003825834385\n"
     ]
    }
   ],
   "source": [
    "#SVM -1\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, C=1.0, gamma=0.01)\n",
    "classifier.fit(x_train_chi, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32254 18184 26304 23258\n",
      "pod:  0.4692708123158872\n",
      "pof:  0.3605218287798882\n",
      "AUC:  0.5543744917679996\n"
     ]
    }
   ],
   "source": [
    "#SVM -2\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, C=0.01, gamma=0.01)\n",
    "classifier.fit(x_train_chi, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32252 18186 27216 22346\n",
      "pod:  0.4508696178523869\n",
      "pof:  0.3605614814227368\n",
      "AUC:  0.545154068214825\n"
     ]
    }
   ],
   "source": [
    "#SVM -3\n",
    "from sklearn.svm import SVC\n",
    "#classifier = SVC(kernel = 'rbf', random_state = 0, C=1.0, gamma=0.01)\n",
    "#classifier = SVC(kernel = 'rbf', random_state = 0, C=0.01, gamma=0.01)\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, C=1.0, gamma=0.1)\n",
    "classifier.fit(x_train_chi, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28689 21749 24095 25467\n",
      "pod:  0.5138412493442557\n",
      "pof:  0.43120266465759943\n",
      "AUC:  0.5413192923433281\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(metric='manhattan', weights='uniform', n_neighbors=19 )  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27029 23409 24215 25347\n",
      "pod:  0.5114200395464267\n",
      "pof:  0.4641143582219755\n",
      "AUC:  0.5236528406622256\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 48.3min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 115.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 212.7min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 931.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "#parameters = {'n_estimators': [4, 6, 9], \n",
    "#              'max_features': ['log2', 'sqrt','auto'], \n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [2, 3, 5, 10], \n",
    "#              'min_samples_split': [2, 3, 5],\n",
    "#              'min_samples_leaf': [1,5,8]\n",
    "#             }\n",
    "\n",
    "#model_params = {\n",
    "#    'n_estimators': [50, 150, 250],\n",
    "#    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "#    'min_samples_split': [2, 4, 6]\n",
    "#}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=110, max_features=3, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=3, min_samples_split=8,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.58342\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29194 21244 20013 29549\n",
      "pod:  0.5962027359670715\n",
      "pof:  0.42119037233831635\n",
      "AUC:  0.5875061818143776\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(bootstrap= True, max_depth= 110, max_features= 3, min_samples_leaf= 3, min_samples_split= 8, n_estimators= 1000)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26466 23972 20690 28872\n",
      "pod:  0.5825430773576531\n",
      "pof:  0.4752765771838693\n",
      "AUC:  0.5536332500868919\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26437 24001 23720 25842\n",
      "pod:  0.5214075299624712\n",
      "pof:  0.4758515405051747\n",
      "AUC:  0.5227779947286483\n",
      "accuracy:  0.52279\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26256 24182 23898 25664\n",
      "pod:  0.5178160687623583\n",
      "pof:  0.4794401046829771\n",
      "AUC:  0.5191879820396906\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27836 22602 18291 31271\n",
      "pod:  0.6309470965659174\n",
      "pof:  0.4481145168325469\n",
      "AUC:  0.5914162898666853\n",
      "accuracy:  0.59107\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27860 22578 18243 31319\n",
      "pod:  0.631915580485049\n",
      "pof:  0.44763868511836313\n",
      "AUC:  0.592138447683343\n",
      "accuracy:  0.59179\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 113us/step - loss: 0.6882 - accuracy: 0.5409 - val_loss: 0.6767 - val_accuracy: 0.5964\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 8s 110us/step - loss: 0.6795 - accuracy: 0.5661 - val_loss: 0.6666 - val_accuracy: 0.6153\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 108us/step - loss: 0.6750 - accuracy: 0.5757 - val_loss: 0.6633 - val_accuracy: 0.6019\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.6730 - accuracy: 0.5789 - val_loss: 0.6674 - val_accuracy: 0.6107\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.6717 - accuracy: 0.5811 - val_loss: 0.6722 - val_accuracy: 0.5892\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 9s 119us/step - loss: 0.6705 - accuracy: 0.5846 - val_loss: 0.6677 - val_accuracy: 0.5987\n",
      "y2_pred:  [[0.35615712]\n",
      " [0.44954413]\n",
      " [0.78932065]\n",
      " ...\n",
      " [0.553946  ]\n",
      " [0.5614486 ]\n",
      " [0.57458556]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7242283639297963\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 8s 117us/step - loss: 0.6855 - accuracy: 0.5489 - val_loss: 0.6663 - val_accuracy: 0.6115\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 8s 109us/step - loss: 0.6770 - accuracy: 0.5712 - val_loss: 0.6714 - val_accuracy: 0.5879\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 8s 112us/step - loss: 0.6744 - accuracy: 0.5774 - val_loss: 0.6645 - val_accuracy: 0.6078\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.6724 - accuracy: 0.5814 - val_loss: 0.6859 - val_accuracy: 0.5575\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 9s 122us/step - loss: 0.6717 - accuracy: 0.5839 - val_loss: 0.6763 - val_accuracy: 0.5902\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 9s 119us/step - loss: 0.6705 - accuracy: 0.5840 - val_loss: 0.6643 - val_accuracy: 0.5996\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 8s 113us/step - loss: 0.6691 - accuracy: 0.5862 - val_loss: 0.6781 - val_accuracy: 0.5851\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.6686 - accuracy: 0.5871 - val_loss: 0.7313 - val_accuracy: 0.4508\n",
      "Epoch 9/30\n",
      "71999/71999 [==============================] - 8s 113us/step - loss: 0.6688 - accuracy: 0.5876 - val_loss: 0.6708 - val_accuracy: 0.5810\n",
      "y2_pred:  [[0.42107636]\n",
      " [0.5050704 ]\n",
      " [0.5642133 ]\n",
      " ...\n",
      " [0.54231817]\n",
      " [0.60699826]\n",
      " [0.5496419 ]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7433931813596933\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6899 - accuracy: 0.5167 - val_loss: 0.6930 - val_accuracy: 0.5873\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6826 - accuracy: 0.5495 - val_loss: 0.6758 - val_accuracy: 0.6106\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6763 - accuracy: 0.5723 - val_loss: 0.6711 - val_accuracy: 0.6096\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6728 - accuracy: 0.5796 - val_loss: 0.6844 - val_accuracy: 0.5831\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 9s 126us/step - loss: 0.6722 - accuracy: 0.5807 - val_loss: 0.6837 - val_accuracy: 0.5778\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6709 - accuracy: 0.5825 - val_loss: 0.6713 - val_accuracy: 0.5997\n",
      "y2_pred:  [[0.5692464 ]\n",
      " [0.5259713 ]\n",
      " [0.58827317]\n",
      " ...\n",
      " [0.5437654 ]\n",
      " [0.3779259 ]\n",
      " [0.6020322 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7427360774818402\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 121us/step - loss: 0.6860 - accuracy: 0.5439 - val_loss: 0.6751 - val_accuracy: 0.5964\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6762 - accuracy: 0.5718 - val_loss: 0.6867 - val_accuracy: 0.5626\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 9s 121us/step - loss: 0.6722 - accuracy: 0.5785 - val_loss: 0.6635 - val_accuracy: 0.6047\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 9s 123us/step - loss: 0.6700 - accuracy: 0.5865 - val_loss: 0.6601 - val_accuracy: 0.6141\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 10s 135us/step - loss: 0.6688 - accuracy: 0.5860 - val_loss: 0.6658 - val_accuracy: 0.5961\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6681 - accuracy: 0.5867 - val_loss: 0.6713 - val_accuracy: 0.5951\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6670 - accuracy: 0.5898 - val_loss: 0.6558 - val_accuracy: 0.6174\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6671 - accuracy: 0.5893 - val_loss: 0.6599 - val_accuracy: 0.6141\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6660 - accuracy: 0.5911 - val_loss: 0.6725 - val_accuracy: 0.5857\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6659 - accuracy: 0.5902 - val_loss: 0.6736 - val_accuracy: 0.5819\n",
      "y2_pred:  [[0.36331144]\n",
      " [0.524365  ]\n",
      " [0.7625683 ]\n",
      " ...\n",
      " [0.3465233 ]\n",
      " [0.58316535]\n",
      " [0.59557754]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7431396287328491\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 123us/step - loss: 0.6848 - accuracy: 0.5506 - val_loss: 0.6900 - val_accuracy: 0.5736\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 116us/step - loss: 0.6759 - accuracy: 0.5751 - val_loss: 0.6707 - val_accuracy: 0.5859\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 9s 123us/step - loss: 0.6720 - accuracy: 0.5820 - val_loss: 0.6800 - val_accuracy: 0.5412\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6706 - accuracy: 0.5845 - val_loss: 0.6740 - val_accuracy: 0.5807\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6691 - accuracy: 0.5866 - val_loss: 0.6775 - val_accuracy: 0.5781\n",
      "y2_pred:  [[0.5891589 ]\n",
      " [0.59527177]\n",
      " [0.4983024 ]\n",
      " ...\n",
      " [0.43323493]\n",
      " [0.5206725 ]\n",
      " [0.3772679 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7324455205811138\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 118us/step - loss: 0.6862 - accuracy: 0.5461 - val_loss: 0.6927 - val_accuracy: 0.5206\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6777 - accuracy: 0.5679 - val_loss: 0.6693 - val_accuracy: 0.5967\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6729 - accuracy: 0.5786 - val_loss: 0.6642 - val_accuracy: 0.6069\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6711 - accuracy: 0.5829 - val_loss: 0.6928 - val_accuracy: 0.5354\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6701 - accuracy: 0.5855 - val_loss: 0.6763 - val_accuracy: 0.5811\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6685 - accuracy: 0.5882 - val_loss: 0.6604 - val_accuracy: 0.6163\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6680 - accuracy: 0.5874 - val_loss: 0.6860 - val_accuracy: 0.5798\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6661 - accuracy: 0.5928 - val_loss: 0.6679 - val_accuracy: 0.6033\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6659 - accuracy: 0.5907 - val_loss: 0.6923 - val_accuracy: 0.5374\n",
      "y2_pred:  [[0.5042854 ]\n",
      " [0.5783215 ]\n",
      " [0.5733013 ]\n",
      " ...\n",
      " [0.5985937 ]\n",
      " [0.5509425 ]\n",
      " [0.58942294]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7887409200968523\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6801 - accuracy: 0.5590 - val_loss: 0.6762 - val_accuracy: 0.5742\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6716 - accuracy: 0.5807 - val_loss: 0.6773 - val_accuracy: 0.5849\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6687 - accuracy: 0.5856 - val_loss: 0.6777 - val_accuracy: 0.5843\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6668 - accuracy: 0.5872 - val_loss: 0.6724 - val_accuracy: 0.5943\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 7s 103us/step - loss: 0.6653 - accuracy: 0.5918 - val_loss: 0.6624 - val_accuracy: 0.6046\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6642 - accuracy: 0.5946 - val_loss: 0.6727 - val_accuracy: 0.5911\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 7s 104us/step - loss: 0.6632 - accuracy: 0.5953 - val_loss: 0.6810 - val_accuracy: 0.5634\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6630 - accuracy: 0.5953 - val_loss: 0.6764 - val_accuracy: 0.5731\n",
      "y2_pred:  [[0.40369666]\n",
      " [0.4754604 ]\n",
      " [0.5865899 ]\n",
      " ...\n",
      " [0.7572775 ]\n",
      " [0.5443733 ]\n",
      " [0.42891687]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7189265536723164\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 121us/step - loss: 0.6840 - accuracy: 0.5506 - val_loss: 0.6773 - val_accuracy: 0.5842\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 116us/step - loss: 0.6726 - accuracy: 0.5791 - val_loss: 0.6686 - val_accuracy: 0.6001\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6688 - accuracy: 0.5902 - val_loss: 0.6724 - val_accuracy: 0.5807\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6669 - accuracy: 0.5917 - val_loss: 0.6685 - val_accuracy: 0.5956\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6655 - accuracy: 0.5971 - val_loss: 0.6772 - val_accuracy: 0.5762\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6646 - accuracy: 0.5955 - val_loss: 0.6688 - val_accuracy: 0.5854\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6633 - accuracy: 0.5985 - val_loss: 0.6746 - val_accuracy: 0.5872\n",
      "y2_pred:  [[0.56420517]\n",
      " [0.36265144]\n",
      " [0.6784726 ]\n",
      " ...\n",
      " [0.53283155]\n",
      " [0.5690936 ]\n",
      " [0.42479202]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.4515738498789346\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 118us/step - loss: 0.6882 - accuracy: 0.5366 - val_loss: 0.6835 - val_accuracy: 0.5641\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6782 - accuracy: 0.5683 - val_loss: 0.6834 - val_accuracy: 0.5732\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6735 - accuracy: 0.5802 - val_loss: 0.6979 - val_accuracy: 0.5280\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6708 - accuracy: 0.5855 - val_loss: 0.6708 - val_accuracy: 0.6014\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6688 - accuracy: 0.5872 - val_loss: 0.6791 - val_accuracy: 0.5776\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6676 - accuracy: 0.5883 - val_loss: 0.6896 - val_accuracy: 0.5507\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6669 - accuracy: 0.5914 - val_loss: 0.6906 - val_accuracy: 0.5300\n",
      "y2_pred:  [[0.6635611 ]\n",
      " [0.51768804]\n",
      " [0.51259184]\n",
      " ...\n",
      " [0.5001758 ]\n",
      " [0.5196963 ]\n",
      " [0.43449757]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7580710250201775\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 121us/step - loss: 0.6834 - accuracy: 0.5565 - val_loss: 0.6898 - val_accuracy: 0.5418\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 118us/step - loss: 0.6750 - accuracy: 0.5753 - val_loss: 0.6798 - val_accuracy: 0.5565\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 9s 130us/step - loss: 0.6714 - accuracy: 0.5834 - val_loss: 0.6781 - val_accuracy: 0.5750\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6694 - accuracy: 0.5871 - val_loss: 0.6670 - val_accuracy: 0.5884\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 9s 118us/step - loss: 0.6681 - accuracy: 0.5881 - val_loss: 0.6656 - val_accuracy: 0.5969\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 118us/step - loss: 0.6677 - accuracy: 0.5902 - val_loss: 0.6956 - val_accuracy: 0.5329\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 9s 118us/step - loss: 0.6662 - accuracy: 0.5915 - val_loss: 0.6741 - val_accuracy: 0.5690\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 117us/step - loss: 0.6656 - accuracy: 0.5926 - val_loss: 0.6636 - val_accuracy: 0.6090\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 10s 134us/step - loss: 0.6661 - accuracy: 0.5915 - val_loss: 0.6713 - val_accuracy: 0.5901\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6649 - accuracy: 0.5939 - val_loss: 0.6787 - val_accuracy: 0.5616\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6646 - accuracy: 0.5949 - val_loss: 0.6926 - val_accuracy: 0.5481\n",
      "y2_pred:  [[0.6549574 ]\n",
      " [0.428434  ]\n",
      " [0.77629477]\n",
      " ...\n",
      " [0.9722286 ]\n",
      " [0.79084766]\n",
      " [0.65127605]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6896690879741727\n",
      "pod:  [0.7242283639297963, 0.7433931813596933, 0.7427360774818402, 0.7431396287328491, 0.7324455205811138, 0.7887409200968523, 0.7189265536723164, 0.4515738498789346, 0.7580710250201775, 0.6896690879741727]\n",
      "pof:  [0.5162569389373514, 0.5555114988104679, 0.5955590800951626, 0.5683980967486122, 0.5578905630452022, 0.6486915146708961, 0.6980570975416336, 0.440325138778747, 0.5740630577037478, 0.5399563751735078]\n",
      "auc:  [0.6039857124962225, 0.5939408412746127, 0.5735884986933388, 0.5873707659921185, 0.5872774787679558, 0.5700247027129781, 0.5104347280653414, 0.5056243555500938, 0.5920039836582149, 0.5748563564003324]\n",
      "tn_list:  [2440, 2242, 2040, 2177, 2230, 1772, 1523, 2823, 2148, 2320]\n",
      "fp_list:  [2604, 2802, 3004, 2867, 2814, 3272, 3521, 2221, 2895, 2723]\n",
      "fn_list:  [1367, 1272, 1275, 1273, 1326, 1047, 1393, 2718, 1199, 1538]\n",
      "tp_list:  [3590, 3685, 3681, 3683, 3630, 3909, 3563, 2238, 3757, 3418]\n",
      "tn mean:  2171.5\n",
      "fp mean:  2872.3\n",
      "fn mean:  1440.8\n",
      "tp mean:  3515.4\n"
     ]
    }
   ],
   "source": [
    "df_x_train_chi = pd.DataFrame(x_train_chi)\n",
    "df_x_train_chi.head()\n",
    "\n",
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print('tn_list: ',tn_list)\n",
    "print ('fp_list: ',fp_list)\n",
    "print ('fn_list: ',fn_list)\n",
    "print ('tp_list: ',tp_list)\n",
    "\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21715 28723 14408 35154\n"
     ]
    }
   ],
   "source": [
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 43s 594us/step - loss: 0.6889 - acc: 0.5370 - val_loss: 0.6883 - val_acc: 0.5413\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 43s 601us/step - loss: 0.6860 - acc: 0.5494 - val_loss: 0.6904 - val_acc: 0.5226\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 43s 596us/step - loss: 0.6839 - acc: 0.5539 - val_loss: 0.6907 - val_acc: 0.5357\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 41s 568us/step - loss: 0.6819 - acc: 0.5597 - val_loss: 0.6883 - val_acc: 0.5408\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 42s 585us/step - loss: 0.6811 - acc: 0.5635 - val_loss: 0.6851 - val_acc: 0.5467\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8194472463183377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 42s 589us/step - loss: 0.6882 - acc: 0.5385 - val_loss: 0.7361 - val_acc: 0.4331\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 42s 586us/step - loss: 0.6855 - acc: 0.5500 - val_loss: 0.6898 - val_acc: 0.5371\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 41s 563us/step - loss: 0.6845 - acc: 0.5539 - val_loss: 0.6959 - val_acc: 0.5076\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 43s 600us/step - loss: 0.6832 - acc: 0.5574 - val_loss: 0.6927 - val_acc: 0.5273\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 42s 581us/step - loss: 0.6819 - acc: 0.5602 - val_loss: 0.6779 - val_acc: 0.5710\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7256405083719992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 581us/step - loss: 0.6878 - acc: 0.5423 - val_loss: 0.6913 - val_acc: 0.5259\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.6836 - acc: 0.5548 - val_loss: 0.6815 - val_acc: 0.5584\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 582us/step - loss: 0.6816 - acc: 0.5617 - val_loss: 0.6851 - val_acc: 0.5573\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 581us/step - loss: 0.6801 - acc: 0.5659 - val_loss: 0.6849 - val_acc: 0.5563 \n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 41s 563us/step - loss: 0.6787 - acc: 0.5690 - val_loss: 0.6944 - val_acc: 0.5400\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.48748991121872476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 617us/step - loss: 0.6884 - acc: 0.5392 - val_loss: 0.6806 - val_acc: 0.5723\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 588us/step - loss: 0.6857 - acc: 0.5502 - val_loss: 0.6877 - val_acc: 0.5405\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 591us/step - loss: 0.6840 - acc: 0.5547 - val_loss: 0.6951 - val_acc: 0.5144\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 590us/step - loss: 0.6822 - acc: 0.5594 - val_loss: 0.6726 - val_acc: 0.5851\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 603us/step - loss: 0.6800 - acc: 0.5653 - val_loss: 0.6764 - val_acc: 0.5761\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.5708232445520581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 590us/step - loss: 0.6884 - acc: 0.5391 - val_loss: 0.6772 - val_acc: 0.5759\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.6853 - acc: 0.5514 - val_loss: 0.6939 - val_acc: 0.5156\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 588us/step - loss: 0.6836 - acc: 0.5576 - val_loss: 0.6897 - val_acc: 0.5379\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 594us/step - loss: 0.6825 - acc: 0.5617 - val_loss: 0.7040 - val_acc: 0.4867\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 582us/step - loss: 0.6818 - acc: 0.5646 - val_loss: 0.7059 - val_acc: 0.4954\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.8484665052461663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 622us/step - loss: 0.6882 - acc: 0.5436 - val_loss: 0.6970 - val_acc: 0.4988\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 588us/step - loss: 0.6846 - acc: 0.5541 - val_loss: 0.7083 - val_acc: 0.4773\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 600us/step - loss: 0.6833 - acc: 0.5578 - val_loss: 0.7069 - val_acc: 0.5042\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 611us/step - loss: 0.6819 - acc: 0.5616 - val_loss: 0.6814 - val_acc: 0.5598\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 604us/step - loss: 0.6812 - acc: 0.5646 - val_loss: 0.6852 - val_acc: 0.5487\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.6087570621468926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 618us/step - loss: 0.6864 - acc: 0.5395 - val_loss: 0.6921 - val_acc: 0.5221\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 44s 607us/step - loss: 0.6821 - acc: 0.5557 - val_loss: 0.6882 - val_acc: 0.5333\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 599us/step - loss: 0.6804 - acc: 0.5587 - val_loss: 0.6904 - val_acc: 0.5536\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 605us/step - loss: 0.6797 - acc: 0.5631 - val_loss: 0.6886 - val_acc: 0.5342\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 594us/step - loss: 0.6788 - acc: 0.5623 - val_loss: 0.6785 - val_acc: 0.5848\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.35512510088781274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 602us/step - loss: 0.6879 - acc: 0.5413 - val_loss: 0.6948 - val_acc: 0.5256\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 586us/step - loss: 0.6845 - acc: 0.5541 - val_loss: 0.6970 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 602us/step - loss: 0.6819 - acc: 0.5618 - val_loss: 0.6884 - val_acc: 0.5431\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 601us/step - loss: 0.6791 - acc: 0.5665 - val_loss: 0.6929 - val_acc: 0.5413\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 611us/step - loss: 0.6779 - acc: 0.5714 - val_loss: 0.6953 - val_acc: 0.5242\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.5411622276029056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 606us/step - loss: 0.6884 - acc: 0.5386 - val_loss: 0.6935 - val_acc: 0.5248\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6850 - acc: 0.5523 - val_loss: 0.6902 - val_acc: 0.5394\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 608us/step - loss: 0.6840 - acc: 0.5556 - val_loss: 0.6871 - val_acc: 0.5546\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 595us/step - loss: 0.6834 - acc: 0.5578 - val_loss: 0.6896 - val_acc: 0.5371\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 600us/step - loss: 0.6824 - acc: 0.5595 - val_loss: 0.6965 - val_acc: 0.5147\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.6668684422921711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 595us/step - loss: 0.6885 - acc: 0.5378 - val_loss: 0.6818 - val_acc: 0.5605\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 44s 607us/step - loss: 0.6852 - acc: 0.5529 - val_loss: 0.6795 - val_acc: 0.5664\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 580us/step - loss: 0.6841 - acc: 0.5539 - val_loss: 0.6802 - val_acc: 0.5659\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6829 - acc: 0.5583 - val_loss: 0.6923 - val_acc: 0.5295\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6816 - acc: 0.5624 - val_loss: 0.6934 - val_acc: 0.5335\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6963276836158192\n",
      "pod:  [0.8194472463183377, 0.7256405083719992, 0.48748991121872476, 0.5708232445520581, 0.8484665052461663, 0.6087570621468926, 0.35512510088781274, 0.5411622276029056, 0.6668684422921711, 0.6963276836158192]\n",
      "pof:  [0.7018239492466296, 0.6038858049167327, 0.3959159397303727, 0.4234734337827121, 0.7817208564631245, 0.503370340999207, 0.4888977002379064, 0.5465900079302141, 0.5540352964505255, 0.6157049375371803]\n",
      "auc:  [0.558811648535854, 0.5608773517276333, 0.5457869857441761, 0.5736749053846729, 0.533372824391521, 0.5526933605738429, 0.4331137003249531, 0.49728610983634575, 0.5564165729208228, 0.5403113730393194]\n",
      "22115 28323 18238 31324\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
