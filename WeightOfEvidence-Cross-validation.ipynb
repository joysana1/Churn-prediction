{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"D:\\Thesis\\BIG DATA\\Database Churn\\joy work\\Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=0\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "\n",
    "num_df=num_df.fillna(conData)\n",
    "\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "result_df_op=result_df\n",
    "\n",
    "X=result_df_op\n",
    "y=churn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>drop_vce_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>infobase</th>\n",
       "      <th>HHstatin</th>\n",
       "      <th>dwllsize</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556517</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.028551</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017520</td>\n",
       "      <td>-0.164801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022460</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>-0.107643</td>\n",
       "      <td>-0.112533</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.060797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.400201</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015816</td>\n",
       "      <td>0.751490</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>-0.028216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.045587</td>\n",
       "      <td>-0.028551</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.081092</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.095326</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.270162</td>\n",
       "      <td>-0.339154</td>\n",
       "      <td>-0.136630</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.014783</td>\n",
       "      <td>-0.033784</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.036986</td>\n",
       "      <td>-0.037101</td>\n",
       "      <td>0.066336</td>\n",
       "      <td>-0.002808</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.021631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_Mean  mou_Mean  totmrc_Mean   da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "0  0.000000  0.556517     0.017520 -0.022460    -0.015257    -0.014783   \n",
       "1  0.017520 -0.164801     0.000000 -0.022460     0.045300    -0.107643   \n",
       "2 -0.015816  0.751490     0.094113  0.047296    -0.015257    -0.014783   \n",
       "3 -1.081092  0.199842     0.017520  0.047296    -0.015257    -0.014783   \n",
       "4 -0.270162 -0.339154    -0.136630  0.047296    -0.015257    -0.014783   \n",
       "\n",
       "   vceovr_Mean  datovr_Mean  roam_Mean  drop_vce_Mean  ...  infobase  \\\n",
       "0    -0.033784     0.018887   0.014593      -0.011647  ... -0.025602   \n",
       "1    -0.112533     0.018887   0.014593      -0.060797  ... -0.025602   \n",
       "2    -0.033784     0.018887   0.014593      -0.028216  ... -0.025602   \n",
       "3    -0.033784     0.018887   0.014593       0.175511  ... -0.025602   \n",
       "4    -0.033784     0.018887   0.014593       0.063701  ... -0.025602   \n",
       "\n",
       "   HHstatin  dwllsize    ethnic    kid0_2    kid3_5   kid6_10  kid11_15  \\\n",
       "0 -0.036986 -0.045587 -0.028551 -0.002808 -0.000904  0.002464   0.00339   \n",
       "1 -0.036986 -0.045587 -0.400201 -0.002808 -0.000904  0.002464   0.00339   \n",
       "2 -0.036986 -0.045587 -0.028551 -0.002808  0.043076  0.002464   0.00339   \n",
       "3 -0.036986  0.052330  0.018434  0.095326 -0.000904  0.002464   0.00339   \n",
       "4 -0.036986 -0.037101  0.066336 -0.002808 -0.000904  0.002464   0.00339   \n",
       "\n",
       "   kid16_17  creditcd  \n",
       "0 -0.001641 -0.021631  \n",
       "1 -0.001641 -0.021631  \n",
       "2 -0.001641 -0.021631  \n",
       "3 -0.001641 -0.021631  \n",
       "4 -0.001641 -0.021631  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlencoders.weight_of_evidence_encoder import WeightOfEvidenceEncoder\n",
    "\n",
    "enc = WeightOfEvidenceEncoder(cols=X.columns)\n",
    "X_encoded = enc.fit_transform(X, y)\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>drop_vce_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>infobase</th>\n",
       "      <th>HHstatin</th>\n",
       "      <th>dwllsize</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.556517</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.751490</td>\n",
       "      <td>0.094113</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>0.018434</td>\n",
       "      <td>0.095326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_Mean  mou_Mean  totmrc_Mean   da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "0   0.00000  0.556517     0.017520  0.000000       0.0000          0.0   \n",
       "1   0.01752  0.000000     0.000000  0.000000       0.0453          0.0   \n",
       "2   0.00000  0.751490     0.094113  0.047296       0.0000          0.0   \n",
       "3   0.00000  0.199842     0.017520  0.047296       0.0000          0.0   \n",
       "4   0.00000  0.000000     0.000000  0.047296       0.0000          0.0   \n",
       "\n",
       "   vceovr_Mean  datovr_Mean  roam_Mean  drop_vce_Mean  ...  infobase  \\\n",
       "0          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "1          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "2          0.0     0.018887   0.014593       0.000000  ...       0.0   \n",
       "3          0.0     0.018887   0.014593       0.175511  ...       0.0   \n",
       "4          0.0     0.018887   0.014593       0.063701  ...       0.0   \n",
       "\n",
       "   HHstatin  dwllsize    ethnic    kid0_2    kid3_5   kid6_10  kid11_15  \\\n",
       "0       0.0   0.00000  0.000000  0.000000  0.000000  0.002464   0.00339   \n",
       "1       0.0   0.00000  0.000000  0.000000  0.000000  0.002464   0.00339   \n",
       "2       0.0   0.00000  0.000000  0.000000  0.043076  0.002464   0.00339   \n",
       "3       0.0   0.05233  0.018434  0.095326  0.000000  0.002464   0.00339   \n",
       "4       0.0   0.00000  0.066336  0.000000  0.000000  0.002464   0.00339   \n",
       "\n",
       "   kid16_17  creditcd  \n",
       "0       0.0       0.0  \n",
       "1       0.0       0.0  \n",
       "2       0.0       0.0  \n",
       "3       0.0       0.0  \n",
       "4       0.0       0.0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded[X_encoded < 0]=0\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.0, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35559 4858 18834 20749\n",
      "pod:  0.5241896773867569\n",
      "pof:  0.1201969468293045\n",
      "AUC:  0.7019963652787262\n",
      "roc_auc:  0.8132135095420031\n",
      "auc:  0.7019963652787262\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32906 7511 8300 31283\n",
      "pod:  0.7903140236970416\n",
      "pof:  0.1858376425761437\n",
      "AUC:  0.802238190560449\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39425 11013 15148 34414\n",
      "pod:  0.6943626165207215\n",
      "pof:  0.21834727784606844\n",
      "AUC:  0.7380076693373265\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:  0.7380076693373265\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "print('auc: ',auc)\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('WOE DT- ROC Curve for RF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33325 7092 8895 30688\n",
      "pod:  0.7752823181668899\n",
      "pof:  0.17547071776727616\n",
      "AUC:  0.799905800199807\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 4s 58us/step - loss: 0.4464 - acc: 0.7896 - val_loss: 0.4299 - val_acc: 0.8036\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 4s 54us/step - loss: 0.4244 - acc: 0.8041 - val_loss: 0.4196 - val_acc: 0.8062\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 4s 53us/step - loss: 0.4186 - acc: 0.8065 - val_loss: 0.4232 - val_acc: 0.8008\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 4s 50us/step - loss: 0.4166 - acc: 0.8084 - val_loss: 0.4157 - val_acc: 0.8097\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 4s 50us/step - loss: 0.4141 - acc: 0.8096 - val_loss: 0.4162 - val_acc: 0.8076\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 4s 49us/step - loss: 0.4128 - acc: 0.8105 - val_loss: 0.4186 - val_acc: 0.8066\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 4s 51us/step - loss: 0.4124 - acc: 0.8104 - val_loss: 0.4195 - val_acc: 0.8082\n",
      "y2_pred:  [[0.57288957]\n",
      " [0.09420136]\n",
      " [0.10913637]\n",
      " ...\n",
      " [0.314513  ]\n",
      " [0.20129684]\n",
      " [0.09110048]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.8083518257010288\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 4s 55us/step - loss: 0.4463 - acc: 0.7917 - val_loss: 0.4273 - val_acc: 0.8020\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 4s 52us/step - loss: 0.4237 - acc: 0.8055 - val_loss: 0.4251 - val_acc: 0.8044\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 4s 51us/step - loss: 0.4198 - acc: 0.8072 - val_loss: 0.4318 - val_acc: 0.7992\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 4s 53us/step - loss: 0.4175 - acc: 0.8086 - val_loss: 0.4208 - val_acc: 0.8070\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 4s 52us/step - loss: 0.4162 - acc: 0.8099 - val_loss: 0.4238 - val_acc: 0.8023\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 4s 51us/step - loss: 0.4139 - acc: 0.8113 - val_loss: 0.4172 - val_acc: 0.8095\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 4s 54us/step - loss: 0.4131 - acc: 0.8102 - val_loss: 0.4169 - val_acc: 0.8075\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 4s 54us/step - loss: 0.4112 - acc: 0.8127 - val_loss: 0.4207 - val_acc: 0.8042\n",
      "Epoch 9/30\n",
      "71999/71999 [==============================] - 4s 52us/step - loss: 0.4109 - acc: 0.8111 - val_loss: 0.4208 - val_acc: 0.8082\n",
      "Epoch 10/30\n",
      "71999/71999 [==============================] - 4s 51us/step - loss: 0.4100 - acc: 0.8118 - val_loss: 0.4200 - val_acc: 0.8065\n",
      "y2_pred:  [[0.96121615]\n",
      " [0.71185726]\n",
      " [0.83429885]\n",
      " ...\n",
      " [0.68891716]\n",
      " [0.3130775 ]\n",
      " [0.35258794]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7577163607020375\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.4486 - acc: 0.7894 - val_loss: 0.4301 - val_acc: 0.7996\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4247 - acc: 0.8049 - val_loss: 0.4222 - val_acc: 0.8049\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4204 - acc: 0.8073 - val_loss: 0.4194 - val_acc: 0.8042\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4171 - acc: 0.8092 - val_loss: 0.4206 - val_acc: 0.8060\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4156 - acc: 0.8091 - val_loss: 0.4163 - val_acc: 0.8085\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4138 - acc: 0.8109 - val_loss: 0.4159 - val_acc: 0.8101\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4124 - acc: 0.8107 - val_loss: 0.4167 - val_acc: 0.8067\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4111 - acc: 0.8115 - val_loss: 0.4210 - val_acc: 0.8061\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4103 - acc: 0.8121 - val_loss: 0.4201 - val_acc: 0.8053\n",
      "y2_pred:  [[0.75923395]\n",
      " [0.757073  ]\n",
      " [0.9292098 ]\n",
      " ...\n",
      " [0.05507132]\n",
      " [0.04598513]\n",
      " [0.09603405]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7459644874899112\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4462 - acc: 0.7913 - val_loss: 0.4336 - val_acc: 0.8009\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4247 - acc: 0.8054 - val_loss: 0.4238 - val_acc: 0.8030\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4183 - acc: 0.8078 - val_loss: 0.4281 - val_acc: 0.8012\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4155 - acc: 0.8099 - val_loss: 0.4171 - val_acc: 0.8087\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.4134 - acc: 0.8114 - val_loss: 0.4242 - val_acc: 0.8045\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4117 - acc: 0.8127 - val_loss: 0.4192 - val_acc: 0.8034\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4101 - acc: 0.8124 - val_loss: 0.4168 - val_acc: 0.8064\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4094 - acc: 0.8137 - val_loss: 0.4161 - val_acc: 0.8051\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4083 - acc: 0.8130 - val_loss: 0.4174 - val_acc: 0.8078\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4079 - acc: 0.8143 - val_loss: 0.4174 - val_acc: 0.8081\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4066 - acc: 0.8148 - val_loss: 0.4179 - val_acc: 0.8067\n",
      "y2_pred:  [[0.7326926 ]\n",
      " [0.61625624]\n",
      " [0.92978907]\n",
      " ...\n",
      " [0.28157252]\n",
      " [0.7305066 ]\n",
      " [0.01260594]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7520177562550444\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4440 - acc: 0.7930 - val_loss: 0.4235 - val_acc: 0.8029\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.4221 - acc: 0.8063 - val_loss: 0.4201 - val_acc: 0.8059\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4169 - acc: 0.8090 - val_loss: 0.4195 - val_acc: 0.8075\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4142 - acc: 0.8104 - val_loss: 0.4195 - val_acc: 0.8086\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4127 - acc: 0.8125 - val_loss: 0.4193 - val_acc: 0.8087\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4111 - acc: 0.8129 - val_loss: 0.4217 - val_acc: 0.8073\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4101 - acc: 0.8126 - val_loss: 0.4197 - val_acc: 0.8063\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4090 - acc: 0.8130 - val_loss: 0.4186 - val_acc: 0.8076\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4081 - acc: 0.8135 - val_loss: 0.4158 - val_acc: 0.8079\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 4s 59us/step - loss: 0.4074 - acc: 0.8135 - val_loss: 0.4288 - val_acc: 0.8056\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4074 - acc: 0.8126 - val_loss: 0.4182 - val_acc: 0.8067\n",
      "Epoch 12/30\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4062 - acc: 0.8148 - val_loss: 0.4208 - val_acc: 0.8055\n",
      "y2_pred:  [[0.9566022 ]\n",
      " [0.91665494]\n",
      " [0.9124772 ]\n",
      " ...\n",
      " [0.02246845]\n",
      " [0.19530961]\n",
      " [0.06912255]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7723970944309927\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4438 - acc: 0.7925 - val_loss: 0.4293 - val_acc: 0.8026\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4237 - acc: 0.8053 - val_loss: 0.4212 - val_acc: 0.8045\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4195 - acc: 0.8068 - val_loss: 0.4214 - val_acc: 0.8061\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4169 - acc: 0.8090 - val_loss: 0.4210 - val_acc: 0.8077\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4154 - acc: 0.8091 - val_loss: 0.4216 - val_acc: 0.8081\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4140 - acc: 0.8110 - val_loss: 0.4181 - val_acc: 0.8077\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 4s 53us/step - loss: 0.4123 - acc: 0.8113 - val_loss: 0.4167 - val_acc: 0.8084\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4109 - acc: 0.8119 - val_loss: 0.4192 - val_acc: 0.8088\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4098 - acc: 0.8124 - val_loss: 0.4154 - val_acc: 0.8093\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4094 - acc: 0.8127 - val_loss: 0.4187 - val_acc: 0.8053\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4082 - acc: 0.8131 - val_loss: 0.4209 - val_acc: 0.8063\n",
      "Epoch 12/30\n",
      "72000/72000 [==============================] - 4s 54us/step - loss: 0.4077 - acc: 0.8132 - val_loss: 0.4157 - val_acc: 0.8091\n",
      "y2_pred:  [[0.778091  ]\n",
      " [0.9639335 ]\n",
      " [0.5456676 ]\n",
      " ...\n",
      " [0.09102577]\n",
      " [0.41665298]\n",
      " [0.7134238 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7962066182405165\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.4458 - acc: 0.7899 - val_loss: 0.4229 - val_acc: 0.8061\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4226 - acc: 0.8056 - val_loss: 0.4186 - val_acc: 0.8074\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4184 - acc: 0.8078 - val_loss: 0.4168 - val_acc: 0.8092\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.4156 - acc: 0.8108 - val_loss: 0.4183 - val_acc: 0.8069\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 55us/step - loss: 0.4135 - acc: 0.8107 - val_loss: 0.4178 - val_acc: 0.8063\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.4123 - acc: 0.8119 - val_loss: 0.4169 - val_acc: 0.8089\n",
      "y2_pred:  [[0.8507004 ]\n",
      " [0.616691  ]\n",
      " [0.8387333 ]\n",
      " ...\n",
      " [0.17698619]\n",
      " [0.6345718 ]\n",
      " [0.1702233 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7889426957223568\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.4465 - acc: 0.7897 - val_loss: 0.4304 - val_acc: 0.8039\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.4246 - acc: 0.8054 - val_loss: 0.4177 - val_acc: 0.8081\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 57us/step - loss: 0.4190 - acc: 0.8072 - val_loss: 0.4236 - val_acc: 0.8047\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.4160 - acc: 0.8096 - val_loss: 0.4188 - val_acc: 0.8053\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 56us/step - loss: 0.4147 - acc: 0.8091 - val_loss: 0.4257 - val_acc: 0.8029\n",
      "y2_pred:  [[0.16088706]\n",
      " [0.7896972 ]\n",
      " [0.1647844 ]\n",
      " ...\n",
      " [0.9817333 ]\n",
      " [0.57773066]\n",
      " [0.95819956]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8494753833736884\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 5s 69us/step - loss: 0.4464 - acc: 0.7906 - val_loss: 0.4409 - val_acc: 0.7980\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4242 - acc: 0.8053 - val_loss: 0.4157 - val_acc: 0.8075\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.4190 - acc: 0.8079 - val_loss: 0.4179 - val_acc: 0.8037\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.4163 - acc: 0.8083 - val_loss: 0.4174 - val_acc: 0.8061\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 58us/step - loss: 0.4151 - acc: 0.8095 - val_loss: 0.4260 - val_acc: 0.8107\n",
      "y2_pred:  [[0.03582287]\n",
      " [0.18277696]\n",
      " [0.2437616 ]\n",
      " ...\n",
      " [0.21484296]\n",
      " [0.19130264]\n",
      " [0.06894672]]\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.817998385794996\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 6s 77us/step - loss: 0.4458 - acc: 0.7918 - val_loss: 0.4221 - val_acc: 0.8061\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4245 - acc: 0.8035 - val_loss: 0.4205 - val_acc: 0.8092\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4212 - acc: 0.8057 - val_loss: 0.4154 - val_acc: 0.8095\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.4186 - acc: 0.8077 - val_loss: 0.4165 - val_acc: 0.8074\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4170 - acc: 0.8088 - val_loss: 0.4161 - val_acc: 0.8078\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4154 - acc: 0.8097 - val_loss: 0.4153 - val_acc: 0.8098\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4143 - acc: 0.8092 - val_loss: 0.4147 - val_acc: 0.8105\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4126 - acc: 0.8109 - val_loss: 0.4190 - val_acc: 0.8064\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4116 - acc: 0.8116 - val_loss: 0.4139 - val_acc: 0.8103\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4105 - acc: 0.8121 - val_loss: 0.4134 - val_acc: 0.8095\n",
      "Epoch 11/30\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.4099 - acc: 0.8123 - val_loss: 0.4131 - val_acc: 0.8106\n",
      "Epoch 12/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4097 - acc: 0.8119 - val_loss: 0.4122 - val_acc: 0.8104\n",
      "Epoch 13/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4083 - acc: 0.8137 - val_loss: 0.4172 - val_acc: 0.8081\n",
      "Epoch 14/30\n",
      "72000/72000 [==============================] - 4s 60us/step - loss: 0.4081 - acc: 0.8123 - val_loss: 0.4132 - val_acc: 0.8102\n",
      "Epoch 15/30\n",
      "72000/72000 [==============================] - 5s 64us/step - loss: 0.4072 - acc: 0.8128 - val_loss: 0.4111 - val_acc: 0.8110\n",
      "Epoch 16/30\n",
      "72000/72000 [==============================] - 5s 66us/step - loss: 0.4065 - acc: 0.8137 - val_loss: 0.4120 - val_acc: 0.8125\n",
      "Epoch 17/30\n",
      "72000/72000 [==============================] - 4s 62us/step - loss: 0.4061 - acc: 0.8141 - val_loss: 0.4134 - val_acc: 0.8113\n",
      "Epoch 18/30\n",
      "72000/72000 [==============================] - 4s 61us/step - loss: 0.4054 - acc: 0.8140 - val_loss: 0.4154 - val_acc: 0.8120\n",
      "y2_pred:  [[0.38780746]\n",
      " [0.6928217 ]\n",
      " [0.7400174 ]\n",
      " ...\n",
      " [0.00802334]\n",
      " [0.6177392 ]\n",
      " [0.46347108]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.786723163841808\n",
      "pod:  [0.8083518257010288, 0.7577163607020375, 0.7459644874899112, 0.7520177562550444, 0.7723970944309927, 0.7962066182405165, 0.7889426957223568, 0.8494753833736884, 0.817998385794996, 0.786723163841808]\n",
      "pof:  [0.18219666931007136, 0.1308485329103886, 0.1352101506740682, 0.14432989690721648, 0.16990483743061063, 0.18675654242664552, 0.1591990483743061, 0.22898493259318, 0.19968272853460242, 0.1683521713265913]\n",
      "auc:  [0.8130775781954788, 0.8134339138958244, 0.8053771684079215, 0.8038439296739139, 0.801246128500191, 0.8047250379069355, 0.8148718236740253, 0.8102452253902541, 0.8091578286301968, 0.8091854962576084]\n",
      "tn mean:  4183.6\n",
      "fp mean:  860.2\n",
      "fn mean:  1052.8\n",
      "tp mean:  3903.4\n"
     ]
    }
   ],
   "source": [
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 27s 369us/step - loss: 0.6319 - acc: 0.6325 - val_loss: 0.5781 - val_acc: 0.7105\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 25s 345us/step - loss: 0.5808 - acc: 0.7120 - val_loss: 0.5840 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 25s 349us/step - loss: 0.5829 - acc: 0.7122 - val_loss: 0.5992 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 24s 338us/step - loss: 0.5839 - acc: 0.7156 - val_loss: 0.5839 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 25s 342us/step - loss: 0.5842 - acc: 0.7152 - val_loss: 0.5851 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7212023401250757\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 26s 357us/step - loss: 0.6232 - acc: 0.6474 - val_loss: 0.5802 - val_acc: 0.7123\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 24s 340us/step - loss: 0.5795 - acc: 0.7107 - val_loss: 0.5891 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 25s 344us/step - loss: 0.5757 - acc: 0.7118 - val_loss: 0.5744 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 24s 339us/step - loss: 0.5734 - acc: 0.7147 - val_loss: 0.5741 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 25s 343us/step - loss: 0.5734 - acc: 0.7139 - val_loss: 0.5722 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7276578575751462\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 27s 369us/step - loss: 0.6276 - acc: 0.6392 - val_loss: 0.5785 - val_acc: 0.7123\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5795 - acc: 0.7119 - val_loss: 0.5879 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 25s 352us/step - loss: 0.5776 - acc: 0.7146 - val_loss: 0.5866 - val_acc: 0.7083\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 25s 348us/step - loss: 0.5754 - acc: 0.7133 - val_loss: 0.5809 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.5757 - acc: 0.7146 - val_loss: 0.5950 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7177158999192897\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 27s 373us/step - loss: 0.6255 - acc: 0.6430 - val_loss: 0.6106 - val_acc: 0.6613\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 25s 348us/step - loss: 0.5786 - acc: 0.7126 - val_loss: 0.5785 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5788 - acc: 0.7141 - val_loss: 0.5782 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 25s 350us/step - loss: 0.5790 - acc: 0.7082 - val_loss: 0.5901 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 25s 351us/step - loss: 0.5745 - acc: 0.7145 - val_loss: 0.5776 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7217514124293786\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 28s 383us/step - loss: 0.6231 - acc: 0.6426 - val_loss: 0.6118 - val_acc: 0.7123\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.5809 - acc: 0.7123 - val_loss: 0.5767 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5844 - acc: 0.7081 - val_loss: 0.6020 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.5841 - acc: 0.7114 - val_loss: 0.6027 - val_acc: 0.7113\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 25s 354us/step - loss: 0.5993 - acc: 0.7116 - val_loss: 0.5947 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7177158999192897\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 28s 382us/step - loss: 0.6394 - acc: 0.6175 - val_loss: 0.5792 - val_acc: 0.7119\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 367us/step - loss: 0.5816 - acc: 0.7107 - val_loss: 0.5815 - val_acc: 0.6935\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.5765 - acc: 0.7115 - val_loss: 0.5739 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.5730 - acc: 0.7141 - val_loss: 0.5780 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 26s 362us/step - loss: 0.5711 - acc: 0.7152 - val_loss: 0.5736 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7237691686844229\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 28s 393us/step - loss: 0.6422 - acc: 0.6084 - val_loss: 0.5846 - val_acc: 0.6814\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.5831 - acc: 0.7119 - val_loss: 0.5841 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 26s 359us/step - loss: 0.5906 - acc: 0.7069 - val_loss: 0.6971 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 26s 360us/step - loss: 0.5894 - acc: 0.7042 - val_loss: 0.5849 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 26s 358us/step - loss: 0.5830 - acc: 0.7151 - val_loss: 0.5846 - val_acc: 0.7123\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7263922518159807\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 28s 383us/step - loss: 0.6440 - acc: 0.6186 - val_loss: 0.5806 - val_acc: 0.7119s - loss: 0.6451 - acc:\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 356us/step - loss: 0.5796 - acc: 0.7137 - val_loss: 0.5910 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 25s 353us/step - loss: 0.5813 - acc: 0.7164 - val_loss: 0.5937 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5726 - acc: 0.7175 - val_loss: 0.5750 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 26s 355us/step - loss: 0.5711 - acc: 0.7175 - val_loss: 0.5762 - val_acc: 0.7123\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7126715092816788\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 29s 399us/step - loss: 0.6288 - acc: 0.6404 - val_loss: 0.5858 - val_acc: 0.7111\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 27s 371us/step - loss: 0.5827 - acc: 0.7136 - val_loss: 0.6587 - val_acc: 0.4923\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 27s 370us/step - loss: 0.5836 - acc: 0.7146 - val_loss: 0.5857 - val_acc: 0.7111\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 27s 370us/step - loss: 0.5844 - acc: 0.7148 - val_loss: 0.5877 - val_acc: 0.7111\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 27s 374us/step - loss: 0.5854 - acc: 0.7161 - val_loss: 0.5848 - val_acc: 0.7111\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7156981436642453\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 29s 400us/step - loss: 0.6366 - acc: 0.6192 - val_loss: 0.5854 - val_acc: 0.6882\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 26s 365us/step - loss: 0.5779 - acc: 0.7133 - val_loss: 0.5786 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 26s 361us/step - loss: 0.5746 - acc: 0.7152 - val_loss: 0.5857 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.5722 - acc: 0.7151 - val_loss: 0.5747 - val_acc: 0.7123\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 26s 364us/step - loss: 0.5742 - acc: 0.7129 - val_loss: 0.5747 - val_acc: 0.7123\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7152945924132365\n",
      "pod:  [0.7212023401250757, 0.7276578575751462, 0.7177158999192897, 0.7217514124293786, 0.7177158999192897, 0.7237691686844229, 0.7263922518159807, 0.7126715092816788, 0.7156981436642453, 0.7152945924132365]\n",
      "pof:  [0.29163362410785093, 0.28291038858049167, 0.28350515463917525, 0.2942109436954798, 0.29262490087232357, 0.2914353687549564, 0.2753766851704996, 0.2922283901665345, 0.2875272655165576, 0.29129486416815387]\n",
      "auc:  [0.7147843580086123, 0.7223737344973273, 0.7171053726400574, 0.7137702343669493, 0.7125454995234831, 0.7161668999647332, 0.7255077833227406, 0.7102215595575722, 0.7140854390738438, 0.7119998641225412]\n",
      "tn mean:  3589.8\n",
      "fp mean:  1454.0\n",
      "fn mean:  1387.8\n",
      "tp mean:  3568.4\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 48s 667us/step - loss: 0.6389 - acc: 0.6819 - val_loss: 0.5840 - val_acc: 0.7048\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 46s 641us/step - loss: 0.5825 - acc: 0.7121 - val_loss: 0.5889 - val_acc: 0.7082\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 46s 636us/step - loss: 0.5812 - acc: 0.7141 - val_loss: 0.5824 - val_acc: 0.7079\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 44s 608us/step - loss: 0.5830 - acc: 0.7154 - val_loss: 0.5827 - val_acc: 0.7122\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 41s 576us/step - loss: 0.5840 - acc: 0.7162 - val_loss: 0.5836 - val_acc: 0.7121\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7212023401250757\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 45s 622us/step - loss: 3.6522 - acc: 0.6005 - val_loss: 0.5910 - val_acc: 0.6993\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 43s 593us/step - loss: 0.5864 - acc: 0.7071 - val_loss: 0.5839 - val_acc: 0.7046\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 43s 591us/step - loss: 0.5848 - acc: 0.7107 - val_loss: 0.5899 - val_acc: 0.7122\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 43s 600us/step - loss: 0.5851 - acc: 0.7146 - val_loss: 0.5866 - val_acc: 0.7121\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 44s 607us/step - loss: 0.5890 - acc: 0.7155 - val_loss: 0.5946 - val_acc: 0.7119\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7276578575751462\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 628us/step - loss: 0.6247 - acc: 0.6875 - val_loss: 0.5848 - val_acc: 0.7045\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 590us/step - loss: 0.5859 - acc: 0.7095 - val_loss: 0.5858 - val_acc: 0.7078\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 614us/step - loss: 0.5864 - acc: 0.7119 - val_loss: 0.5936 - val_acc: 0.7120\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 591us/step - loss: 0.5830 - acc: 0.7139 - val_loss: 0.5802 - val_acc: 0.7122\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.5876 - acc: 0.7154 - val_loss: 0.5923 - val_acc: 0.7118\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7177158999192897\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 615us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 590us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 599us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 43s 593us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 45s 631us/step - loss: 7.9867 - acc: 0.5045 - val_loss: 7.9955 - val_acc: 0.5039\n",
      "y22_pred:  [[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " ...\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "pod 1st:  0.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 47s 648us/step - loss: 0.6343 - acc: 0.6816 - val_loss: 0.5875 - val_acc: 0.7043\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 44s 615us/step - loss: 0.5850 - acc: 0.7086 - val_loss: 0.5832 - val_acc: 0.7051\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 586us/step - loss: 0.5807 - acc: 0.7139 - val_loss: 0.5815 - val_acc: 0.7123\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 588us/step - loss: 0.5823 - acc: 0.7161 - val_loss: 0.5840 - val_acc: 0.7122\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 0.5838 - acc: 0.7168 - val_loss: 0.5893 - val_acc: 0.7120\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7177158999192897\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 44s 613us/step - loss: 0.6313 - acc: 0.6847 - val_loss: 0.5860 - val_acc: 0.7044\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 45s 628us/step - loss: 0.5839 - acc: 0.7094 - val_loss: 0.5818 - val_acc: 0.7078\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.5823 - acc: 0.7128 - val_loss: 0.5963 - val_acc: 0.7122\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 605us/step - loss: 0.5830 - acc: 0.7133 - val_loss: 0.5824 - val_acc: 0.7078\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 44s 607us/step - loss: 0.5815 - acc: 0.7149 - val_loss: 0.5949 - val_acc: 0.7118\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7237691686844229\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 46s 640us/step - loss: 0.6352 - acc: 0.6808 - val_loss: 0.5881 - val_acc: 0.7044\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 571us/step - loss: 0.5849 - acc: 0.7072 - val_loss: 0.5831 - val_acc: 0.7049\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 599us/step - loss: 0.5851 - acc: 0.7088 - val_loss: 0.5855 - val_acc: 0.7082\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 44s 613us/step - loss: 0.5831 - acc: 0.7108 - val_loss: 0.5863 - val_acc: 0.7078\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.5866 - acc: 0.7115 - val_loss: 0.5862 - val_acc: 0.7077\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7033898305084746\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 624us/step - loss: 0.6432 - acc: 0.6799 - val_loss: 0.5833 - val_acc: 0.7048\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.5816 - acc: 0.7118 - val_loss: 0.5819 - val_acc: 0.7078\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 43s 593us/step - loss: 0.5833 - acc: 0.7137 - val_loss: 0.5813 - val_acc: 0.7079\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.5841 - acc: 0.7165 - val_loss: 0.5888 - val_acc: 0.7079\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 582us/step - loss: 0.5843 - acc: 0.7170 - val_loss: 0.5899 - val_acc: 0.7119\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7126715092816788\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 631us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 581us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 577us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 597us/step - loss: 8.0020 - acc: 0.5035 - val_loss: 7.9341 - val_acc: 0.5077\n",
      "y22_pred:  [[-0.]\n",
      " [-0.]\n",
      " [-0.]\n",
      " ...\n",
      " [-0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "pod 1st:  0.0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 96, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 90, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 45s 625us/step - loss: 0.6628 - acc: 0.6761 - val_loss: 0.5883 - val_acc: 0.7083\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 575us/step - loss: 0.5827 - acc: 0.7149 - val_loss: 0.5832 - val_acc: 0.7086\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 607us/step - loss: 0.5826 - acc: 0.7170 - val_loss: 0.5834 - val_acc: 0.7122\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.5857 - acc: 0.7171 - val_loss: 0.5855 - val_acc: 0.7122\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 0.5863 - acc: 0.7169 - val_loss: 0.5815 - val_acc: 0.7122\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7152945924132365\n",
      "pod:  [0.7212023401250757, 0.7276578575751462, 0.7177158999192897, 0.0, 0.7177158999192897, 0.7237691686844229, 0.7033898305084746, 0.7126715092816788, 0.0, 0.7152945924132365]\n",
      "pof:  [0.29163362410785093, 0.28291038858049167, 0.28350515463917525, 0.0, 0.29262490087232357, 0.2914353687549564, 0.2626883425852498, 0.2922283901665345, 0.0, 0.29129486416815387]\n",
      "auc:  [0.7147843580086123, 0.7223737344973273, 0.7171053726400574, 0.5, 0.7125454995234831, 0.7161668999647332, 0.7203507439616124, 0.7102215595575722, 0.5, 0.7119998641225412]\n",
      "tn mean:  3889.6\n",
      "fp mean:  1154.2\n",
      "fn mean:  2111.6\n",
      "tp mean:  2844.6\n"
     ]
    }
   ],
   "source": [
    "#CNN - 1D CNN\n",
    "def get_CNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    from keras.optimizers import RMSprop\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(max_features, 128, input_length=96))\n",
    "    model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(5))\n",
    "    model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(1))\n",
    "    model.summary()\n",
    "    model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    y22_pred[y22_pred ==2.0]=1\n",
    "   # y22_pred=y22_pred[y22_pred > 1.0]=1\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(X_train,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=X_train.iloc[train_index], X_train.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_CNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print ('tn mean: ',sum(tn_list) / len(tn_list))\n",
    "print ('fp mean: ',sum(fp_list) / len(fp_list))\n",
    "print ('fn mean: ',sum(fn_list) / len(fn_list))\n",
    "print ('tp mean: ',sum(tp_list) / len(tp_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42015 8423 11212 38350\n",
      "pod:  0.7737782978895121\n",
      "pof:  0.16699710535707205\n",
      "AUC:  0.8033905962662201\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-5a43b770cfbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    " tn, fp, fn  = confusion_matrix(y2_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3573, 1469,    2, 1382, 3567,    8,    0,    0,    0], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned =  confusion_matrix(y2_test, y_pred).ravel()\n",
    "returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred ==2.0]=1\n",
    "y22=y_pred[y_pred ==2.0]\n",
    "y22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-4b5d70fd5296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my22_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my22_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int64' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y22_pred[y22_pred > 1.0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
