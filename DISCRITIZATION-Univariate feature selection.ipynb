{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Telecom_customer churn (100000).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df.select_dtypes(include=['float64', 'int64']).copy()\n",
    "cat_df = df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = cat_df.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = cat_df.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "import numpy as np\n",
    "#conData=np.log(0.00001 + 1)\n",
    "conData=-1\n",
    "cat_df=cat_df.fillna(conData)\n",
    "num_df=num_df.fillna(conData)\n",
    "cat_df=cat_df.astype(str)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cat_df[categorical_cols] = cat_df[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "#cat_df[categorical_cols].head(10)\n",
    "\n",
    "change_mou=num_df['change_mou']\n",
    "change_rev=num_df['change_rev']\n",
    "num_df=num_df.drop(['change_mou'], axis=1)\n",
    "num_df=num_df.drop(['change_rev'], axis=1)\n",
    "\n",
    "num_df=num_df.drop(['Customer_ID'], axis=1)\n",
    "\n",
    "churn=num_df['churn']\n",
    "num_df=num_df.drop(['churn'], axis=1)\n",
    "\n",
    "num_df=num_df.replace([np.inf, -np.inf], np.nan)\n",
    "num_df=num_df.fillna(conData)\n",
    "num_df[num_df < 0] = 0\n",
    "#num_df.to_csv('D://num_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_df.columns:\n",
    "    Bins=20\n",
    "    Omega=num_df[col].max()-num_df[col].min()\n",
    "    #Omega=num_df[col].max()-0\n",
    "    Omega=Omega/Bins\n",
    "    #print(num_df[col].min())\n",
    "    #BINValue=[1.1, 1.1]\n",
    "    BINValue=[]\n",
    "    BINValue.insert(0,num_df[col].min())\n",
    "    #BINValue.insert(0,-1)\n",
    "    for i in range(19):\n",
    "        BINValue.insert(i+1,BINValue[i]+Omega)\n",
    "        #print(BINValue[i+1])\n",
    "    #print(BINValue)\n",
    "    #print(num_df[col])\n",
    "    labels =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "    cats=pd.cut(num_df[col], BINValue, labels=labels)\n",
    "    num_df[col]=cats\n",
    "    #print(cats)\n",
    "    #cats.to_csv('D://cats.csv')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df=num_df.fillna(1)\n",
    "result_df = pd.concat([num_df, cat_df], axis=1)\n",
    "np.nan_to_num(result_df)\n",
    "\n",
    "#result_df=result_df.replace([np.inf, -np.inf], np.nan)\n",
    "#result_df=result_df.fillna(-1)\n",
    "#result_df=result_df.abs()\n",
    "\n",
    "result_df_op=result_df\n",
    "#result_df_op=result_df_op.drop(['churn'], axis=1)\n",
    "#result_df_op=np.nan_to_num(result_df_op)\n",
    "\n",
    "X=result_df_op\n",
    "#y=result_df['churn'] \n",
    "y=churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>eqpdays</td>\n",
       "      <td>1.985624e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crclscod</td>\n",
       "      <td>1.803719e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>hnd_price</td>\n",
       "      <td>1.000705e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>asl_flag</td>\n",
       "      <td>4.153917e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>lor</td>\n",
       "      <td>3.423019e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>totmrc_Mean</td>\n",
       "      <td>2.733059e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>hnd_webcap</td>\n",
       "      <td>2.268468e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>avg3mou</td>\n",
       "      <td>1.859614e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>complete_Mean</td>\n",
       "      <td>1.771026e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>attempt_Mean</td>\n",
       "      <td>1.770181e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plcd_vce_Mean</td>\n",
       "      <td>1.728720e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>comp_vce_Mean</td>\n",
       "      <td>1.719046e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mou_cvce_Mean</td>\n",
       "      <td>1.358156e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>avg6mou</td>\n",
       "      <td>1.231114e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>opk_vce_Mean</td>\n",
       "      <td>1.163215e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phones</td>\n",
       "      <td>1.071461e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>adults</td>\n",
       "      <td>1.044172e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mou_opkv_Mean</td>\n",
       "      <td>1.036803e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mou_Mean</td>\n",
       "      <td>1.006342e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>peak_vce_Mean</td>\n",
       "      <td>9.457746e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mou_rvce_Mean</td>\n",
       "      <td>9.061418e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>months</td>\n",
       "      <td>8.899427e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>avg3qty</td>\n",
       "      <td>8.445175e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mou_peav_Mean</td>\n",
       "      <td>8.408390e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>HHstatin</td>\n",
       "      <td>8.104567e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>avg6qty</td>\n",
       "      <td>7.874789e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>owylis_vce_Mean</td>\n",
       "      <td>7.680411e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>ethnic</td>\n",
       "      <td>6.968000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>models</td>\n",
       "      <td>6.886645e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>dualband</td>\n",
       "      <td>5.997127e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>drop_blk_Mean</td>\n",
       "      <td>1.074263e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>kid0_2</td>\n",
       "      <td>6.109816e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>new_cell</td>\n",
       "      <td>5.175562e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>comp_dat_Mean</td>\n",
       "      <td>3.441649e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>kid16_17</td>\n",
       "      <td>3.339629e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>kid3_5</td>\n",
       "      <td>2.485398e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>opk_dat_Mean</td>\n",
       "      <td>2.319470e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blck_vce_Mean</td>\n",
       "      <td>1.798482e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>peak_dat_Mean</td>\n",
       "      <td>1.437950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>plcd_dat_Mean</td>\n",
       "      <td>1.253901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>da_Mean</td>\n",
       "      <td>8.958491e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>custcare_Mean</td>\n",
       "      <td>7.905133e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mou_cdat_Mean</td>\n",
       "      <td>1.541882e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>kid11_15</td>\n",
       "      <td>1.077197e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>unan_dat_Mean</td>\n",
       "      <td>9.907605e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>mou_opkd_Mean</td>\n",
       "      <td>6.691312e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mou_pead_Mean</td>\n",
       "      <td>5.992645e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>drop_dat_Mean</td>\n",
       "      <td>4.186192e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>actvsubs</td>\n",
       "      <td>3.099948e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recv_sms_Mean</td>\n",
       "      <td>2.723796e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rev_Mean</td>\n",
       "      <td>2.656152e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>kid6_10</td>\n",
       "      <td>4.031239e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>uniqsubs</td>\n",
       "      <td>1.685128e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>datovr_Mean</td>\n",
       "      <td>1.647356e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>callfwdv_Mean</td>\n",
       "      <td>5.970589e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>roam_Mean</td>\n",
       "      <td>1.752780e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blck_dat_Mean</td>\n",
       "      <td>4.419364e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>forgntvl</td>\n",
       "      <td>1.049597e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>rv</td>\n",
       "      <td>1.049597e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>truck</td>\n",
       "      <td>1.049597e-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Feature        Scores\n",
       "74          eqpdays  1.985624e+03\n",
       "76         crclscod  1.803719e+03\n",
       "64        hnd_price  1.000705e+03\n",
       "77         asl_flag  4.153917e+02\n",
       "69              lor  3.423019e+02\n",
       "2       totmrc_Mean  2.733059e+02\n",
       "82       hnd_webcap  2.268468e+02\n",
       "58          avg3mou  1.859614e+02\n",
       "43    complete_Mean  1.771026e+02\n",
       "42     attempt_Mean  1.770181e+02\n",
       "15    plcd_vce_Mean  1.728720e+02\n",
       "19    comp_vce_Mean  1.719046e+02\n",
       "26    mou_cvce_Mean  1.358156e+02\n",
       "61          avg6mou  1.231114e+02\n",
       "37     opk_vce_Mean  1.163215e+02\n",
       "65           phones  1.071461e+02\n",
       "70           adults  1.044172e+02\n",
       "39    mou_opkv_Mean  1.036803e+02\n",
       "1          mou_Mean  1.006342e+02\n",
       "33    peak_vce_Mean  9.457746e+01\n",
       "28    mou_rvce_Mean  9.061418e+01\n",
       "46           months  8.899427e+01\n",
       "59          avg3qty  8.445175e+01\n",
       "35    mou_peav_Mean  8.408390e+01\n",
       "87         HHstatin  8.104567e+01\n",
       "62          avg6qty  7.874789e+01\n",
       "29  owylis_vce_Mean  7.680411e+01\n",
       "89           ethnic  6.968000e+01\n",
       "66           models  6.886645e+01\n",
       "80         dualband  5.997127e+01\n",
       "..              ...           ...\n",
       "41    drop_blk_Mean  1.074263e+00\n",
       "90           kid0_2  6.109816e-01\n",
       "75         new_cell  5.175562e-01\n",
       "20    comp_dat_Mean  3.441649e-01\n",
       "94         kid16_17  3.339629e-01\n",
       "91           kid3_5  2.485398e-01\n",
       "38     opk_dat_Mean  2.319470e-01\n",
       "11    blck_vce_Mean  1.798482e-01\n",
       "34    peak_dat_Mean  1.437950e-01\n",
       "16    plcd_dat_Mean  1.253901e-01\n",
       "3           da_Mean  8.958491e-02\n",
       "21    custcare_Mean  7.905133e-02\n",
       "27    mou_cdat_Mean  1.541882e-02\n",
       "93         kid11_15  1.077197e-02\n",
       "14    unan_dat_Mean  9.907605e-03\n",
       "40    mou_opkd_Mean  6.691312e-03\n",
       "36    mou_pead_Mean  5.992645e-03\n",
       "10    drop_dat_Mean  4.186192e-03\n",
       "48         actvsubs  3.099948e-03\n",
       "18    recv_sms_Mean  2.723796e-03\n",
       "0          rev_Mean  2.656152e-03\n",
       "92          kid6_10  4.031239e-04\n",
       "47         uniqsubs  1.685128e-04\n",
       "7       datovr_Mean  1.647356e-04\n",
       "44    callfwdv_Mean  5.970589e-05\n",
       "8         roam_Mean  1.752780e-05\n",
       "12    blck_dat_Mean  4.419364e-07\n",
       "73         forgntvl  1.049597e-27\n",
       "68               rv  1.049597e-27\n",
       "67            truck  1.049597e-27\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X\n",
    "y_train=y\n",
    "#Univariate Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_feature = SelectKBest(chi2, k=80).fit(X_train, y_train)\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                     'Scores':select_feature.scores_})\n",
    "selected_features_df.sort_values(by='Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y\n",
    "x_train_chi = select_feature.transform(X_train)\n",
    "x_train_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9124 41314 7386 42176\n",
      "pod:  0.8509745369436261\n",
      "pof:  0.8191046433244775\n",
      "AUC:  0.5159349468095743\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier.fit(X,y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var_smoothing': 1.0}\n",
      "GaussianNB(priors=None, var_smoothing=1.0)\n",
      "0.53826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "classifier=GaussianNB();\n",
    "#params = {\n",
    "#          \"priors\" : \"None\",\n",
    "#          \"var_smoothing\" : 1e-9\n",
    "#}\n",
    "#create an list of var_smoothing to cross validate\n",
    "#steps = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "\n",
    "grid = GridSearchCV(estimator=classifier, \n",
    "                     param_grid=params_NB, \n",
    "                     cv=cv_method,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    " \n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26710 23728 21590 27972\n",
      "pod:  0.5643840038739357\n",
      "pof:  0.4704389547563345\n",
      "AUC:  0.5469725245588006\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16647 33791 12889 36673\n",
      "pod:  0.7399418909648521\n",
      "pof:  0.6699512272492961\n",
      "AUC:  0.534995331857778\n",
      "accuracy:  0.5332\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create a Gaussian Classifier\n",
    "classifier = GaussianNB(priors=None, var_smoothing=1.0)\n",
    "\n",
    "classifier.fit(x_train_chi,y_train)\n",
    "\n",
    "\n",
    "cv_method = RepeatedKFold(n_splits=5, \n",
    "                          n_repeats=3, \n",
    "                          random_state=999)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 5)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.54812\n"
     ]
    }
   ],
   "source": [
    " #accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "accuracy=(26836+27976)/(27976+22462+22726+26836)\n",
    "print ('accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 555.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 3133.4min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 3152.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.57708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'C': c_values, 'penalty': ['l1'], 'solver' : ['liblinear'], 'multi_class' : ['ovr']},    \n",
    "    {'C': c_values, 'penalty': ['l2'], 'solver' : ['liblinear', 'newton-cg', 'lbfgs'], 'multi_class' : ['ovr']}\n",
    "]\n",
    " \n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29177 21261 21536 28026\n",
      "pod:  0.5654735482829587\n",
      "pof:  0.42152741980252983\n",
      "AUC:  0.5719730642402144\n",
      "accuracy:  0.57203\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=100,\n",
    "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 88.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
      "                     weights='uniform')\n",
      "0.54095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "# Specify parameters\n",
    "c_values = list(np.arange(1, 10))\n",
    "param_grid ={\n",
    "   'n_neighbors': [3,5,11,19],\n",
    "   'weights': ['uniform', 'distance'],\n",
    "   'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 42 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 410.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 1993.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# Specify parameters\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.01, 0.1, 1, 1e-3, 1e-4]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf', 'linear'], \n",
    "                     'gamma': [0.001, 0.01, 0.1, 1, 1e-3, 1e-4], \n",
    "                     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, verbose=1, cv=10, n_jobs=-1)\n",
    "grid.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28732 21706 23846 25716\n",
      "pod:  0.5188652596747508\n",
      "pof:  0.43035013283635354\n",
      "AUC:  0.5442575634191987\n",
      "accuracy:  0.54448\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(metric='manhattan', weights='uniform', n_neighbors=19 )  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27146 23292 24066 25496\n",
      "pod:  0.5144263750453977\n",
      "pof:  0.4617946786153297\n",
      "AUC:  0.526315848215034\n",
      "accuracy:  0.52642\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.7min\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 40.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 75.2min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 103.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 1000}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=110, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=3, min_samples_split=8,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.56918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "#parameters = {'n_estimators': [4, 6, 9], \n",
    "#              'max_features': ['log2', 'sqrt','auto'], \n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [2, 3, 5, 10], \n",
    "#              'min_samples_split': [2, 3, 5],\n",
    "#              'min_samples_leaf': [1,5,8]\n",
    "#             }\n",
    "\n",
    "#model_params = {\n",
    "#    'n_estimators': [50, 150, 250],\n",
    "#    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "#    'min_samples_split': [2, 4, 6]\n",
    "#}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train_chi, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 110, 'max_features': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=110, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=4, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.56987\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28019 22419 19450 30112\n",
      "pod:  0.6075622452685525\n",
      "pof:  0.4444863000118958\n",
      "AUC:  0.5815379726283283\n",
      "accuracy:  0.58131\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(bootstrap= True, max_depth= 110, max_features= 3, min_samples_leaf= 3, min_samples_split= 8, n_estimators= 1000)\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27908 22530 21132 28430\n",
      "pod:  0.5736249546023163\n",
      "pof:  0.44668702168999563\n",
      "AUC:  0.5634689664561604\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier =RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26691 23747 23675 25887\n",
      "pod:  0.5223154836366571\n",
      "pof:  0.47081565486339666\n",
      "AUC:  0.5257499143866303\n",
      "accuracy:  0.52578\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26949 23489 23729 25833\n",
      "pod:  0.521225939227634\n",
      "pof:  0.46570046393592135\n",
      "AUC:  0.5277627376458565\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25962 24476 17223 32339\n",
      "pod:  0.6524958637665954\n",
      "pof:  0.4852690431817281\n",
      "AUC:  0.5836134102924336\n",
      "accuracy:  0.58301\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#classifier = GradientBoostingClassifier(max_features=None, max_depth=3, criterion='friedman_mse')\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(x_train_chi, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = x_train_chi, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26026 24412 17324 32238\n",
      "pod:  0.650458012186756\n",
      "pof:  0.4840001586105714\n",
      "AUC:  0.5832289267880924\n",
      "accuracy:  0.58264\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train) \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred  = cross_val_predict(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_train, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "pod=tp/(tp+fn)\n",
    "\n",
    "print('pod: ',pod)\n",
    "pof=fp/(fp+tn)\n",
    "print ('pof: ',pof)\n",
    "auc_val=(1+pod-pof)/2\n",
    "print ('AUC: ',auc_val)\n",
    "\n",
    "accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "print ('accuracy: ',accuracy)\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#roc_auc = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,  scoring='roc_auc')\n",
    "#print('roc_auc: ',roc_auc.mean())\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
    "#auc = auc(fpr, tpr)\n",
    "#print('auc: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 9s 122us/step - loss: 0.6907 - accuracy: 0.5366 - val_loss: 0.6889 - val_accuracy: 0.5274\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 9s 120us/step - loss: 0.6847 - accuracy: 0.5530 - val_loss: 0.7126 - val_accuracy: 0.4685\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 9s 125us/step - loss: 0.6830 - accuracy: 0.5591 - val_loss: 0.6808 - val_accuracy: 0.5589\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 8s 108us/step - loss: 0.6813 - accuracy: 0.5634 - val_loss: 0.6997 - val_accuracy: 0.5002\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 111us/step - loss: 0.6802 - accuracy: 0.5672 - val_loss: 0.7003 - val_accuracy: 0.4874\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 9s 122us/step - loss: 0.6791 - accuracy: 0.5676 - val_loss: 0.6838 - val_accuracy: 0.5517\n",
      "y2_pred:  [[0.5236392 ]\n",
      " [0.35198325]\n",
      " [0.56284404]\n",
      " ...\n",
      " [0.4422416 ]\n",
      " [0.53151006]\n",
      " [0.64430374]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.47871696590679846\n",
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "71999/71999 [==============================] - 9s 125us/step - loss: 0.6878 - accuracy: 0.5459 - val_loss: 0.6909 - val_accuracy: 0.5133\n",
      "Epoch 2/30\n",
      "71999/71999 [==============================] - 9s 127us/step - loss: 0.6816 - accuracy: 0.5640 - val_loss: 0.6794 - val_accuracy: 0.5718\n",
      "Epoch 3/30\n",
      "71999/71999 [==============================] - 9s 123us/step - loss: 0.6798 - accuracy: 0.5688 - val_loss: 0.6739 - val_accuracy: 0.5912\n",
      "Epoch 4/30\n",
      "71999/71999 [==============================] - 8s 112us/step - loss: 0.6783 - accuracy: 0.5720 - val_loss: 0.6733 - val_accuracy: 0.5853\n",
      "Epoch 5/30\n",
      "71999/71999 [==============================] - 8s 106us/step - loss: 0.6771 - accuracy: 0.5747 - val_loss: 0.6847 - val_accuracy: 0.5634\n",
      "Epoch 6/30\n",
      "71999/71999 [==============================] - 8s 112us/step - loss: 0.6764 - accuracy: 0.5735 - val_loss: 0.6733 - val_accuracy: 0.5894\n",
      "Epoch 7/30\n",
      "71999/71999 [==============================] - 8s 105us/step - loss: 0.6757 - accuracy: 0.5778 - val_loss: 0.6683 - val_accuracy: 0.5927\n",
      "Epoch 8/30\n",
      "71999/71999 [==============================] - 8s 111us/step - loss: 0.6749 - accuracy: 0.5766 - val_loss: 0.6779 - val_accuracy: 0.5827\n",
      "Epoch 9/30\n",
      "71999/71999 [==============================] - 8s 107us/step - loss: 0.6745 - accuracy: 0.5781 - val_loss: 0.6615 - val_accuracy: 0.6072\n",
      "Epoch 10/30\n",
      "71999/71999 [==============================] - 8s 114us/step - loss: 0.6740 - accuracy: 0.5799 - val_loss: 0.6663 - val_accuracy: 0.5972\n",
      "Epoch 11/30\n",
      "71999/71999 [==============================] - 8s 114us/step - loss: 0.6736 - accuracy: 0.5804 - val_loss: 0.6718 - val_accuracy: 0.5916\n",
      "Epoch 12/30\n",
      "71999/71999 [==============================] - 8s 115us/step - loss: 0.6734 - accuracy: 0.5807 - val_loss: 0.6683 - val_accuracy: 0.5974\n",
      "y2_pred:  [[0.52233   ]\n",
      " [0.5845481 ]\n",
      " [0.5764821 ]\n",
      " ...\n",
      " [0.58010733]\n",
      " [0.70190877]\n",
      " [0.66018474]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7086947750655639\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6909 - accuracy: 0.5305 - val_loss: 0.6839 - val_accuracy: 0.5732\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6848 - accuracy: 0.5558 - val_loss: 0.6814 - val_accuracy: 0.5716\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 105us/step - loss: 0.6803 - accuracy: 0.5677 - val_loss: 0.6885 - val_accuracy: 0.5330\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6784 - accuracy: 0.5706 - val_loss: 0.7029 - val_accuracy: 0.4966\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6779 - accuracy: 0.5726 - val_loss: 0.6814 - val_accuracy: 0.5558\n",
      "y2_pred:  [[0.57934916]\n",
      " [0.5750533 ]\n",
      " [0.59370697]\n",
      " ...\n",
      " [0.55756396]\n",
      " [0.4318319 ]\n",
      " [0.56763244]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.8198143664245359\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 118us/step - loss: 0.6879 - accuracy: 0.5474 - val_loss: 0.6862 - val_accuracy: 0.5474\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6816 - accuracy: 0.5636 - val_loss: 0.6827 - val_accuracy: 0.5729\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6795 - accuracy: 0.5687 - val_loss: 0.6845 - val_accuracy: 0.5569\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6786 - accuracy: 0.5723 - val_loss: 0.6783 - val_accuracy: 0.5756\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6771 - accuracy: 0.5737 - val_loss: 0.6686 - val_accuracy: 0.5999\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6765 - accuracy: 0.5762 - val_loss: 0.6775 - val_accuracy: 0.5930\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6756 - accuracy: 0.5763 - val_loss: 0.6784 - val_accuracy: 0.5714\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6751 - accuracy: 0.5790 - val_loss: 0.6879 - val_accuracy: 0.5617\n",
      "y2_pred:  [[0.4280243 ]\n",
      " [0.57774764]\n",
      " [0.55435944]\n",
      " ...\n",
      " [0.42737737]\n",
      " [0.44754976]\n",
      " [0.6162472 ]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.8581517352703794\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 125us/step - loss: 0.6888 - accuracy: 0.5385 - val_loss: 0.6926 - val_accuracy: 0.4966\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6820 - accuracy: 0.5633 - val_loss: 0.6812 - val_accuracy: 0.5486\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 115us/step - loss: 0.6792 - accuracy: 0.5697 - val_loss: 0.6872 - val_accuracy: 0.5431\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6773 - accuracy: 0.5735 - val_loss: 0.6709 - val_accuracy: 0.5979\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6758 - accuracy: 0.5750 - val_loss: 0.6853 - val_accuracy: 0.5618\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6750 - accuracy: 0.5774 - val_loss: 0.6772 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 9s 123us/step - loss: 0.6743 - accuracy: 0.5788 - val_loss: 0.6675 - val_accuracy: 0.6027\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 9s 120us/step - loss: 0.6738 - accuracy: 0.5800 - val_loss: 0.6788 - val_accuracy: 0.5731\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6734 - accuracy: 0.5797 - val_loss: 0.6855 - val_accuracy: 0.5662\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6731 - accuracy: 0.5791 - val_loss: 0.6767 - val_accuracy: 0.5719\n",
      "y2_pred:  [[0.6186342 ]\n",
      " [0.47532684]\n",
      " [0.53633994]\n",
      " ...\n",
      " [0.5374683 ]\n",
      " [0.59214044]\n",
      " [0.5272732 ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.7951977401129944\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6930 - accuracy: 0.5231 - val_loss: 0.6964 - val_accuracy: 0.5161\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6832 - accuracy: 0.5609 - val_loss: 0.6983 - val_accuracy: 0.4992\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6782 - accuracy: 0.5742 - val_loss: 0.6791 - val_accuracy: 0.5756\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6768 - accuracy: 0.5749 - val_loss: 0.6840 - val_accuracy: 0.5479\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6754 - accuracy: 0.5783 - val_loss: 0.6794 - val_accuracy: 0.5714\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 116us/step - loss: 0.6745 - accuracy: 0.5793 - val_loss: 0.6978 - val_accuracy: 0.5196\n",
      "y2_pred:  [[0.5935907]\n",
      " [0.5611322]\n",
      " [0.5238334]\n",
      " ...\n",
      " [0.5563271]\n",
      " [0.5345089]\n",
      " [0.5473999]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8551251008878128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6882 - accuracy: 0.5457 - val_loss: 0.6956 - val_accuracy: 0.5080\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6817 - accuracy: 0.5646 - val_loss: 0.6724 - val_accuracy: 0.5915\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6789 - accuracy: 0.5714 - val_loss: 0.6715 - val_accuracy: 0.5838\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6769 - accuracy: 0.5732 - val_loss: 0.6778 - val_accuracy: 0.5671\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 112us/step - loss: 0.6758 - accuracy: 0.5748 - val_loss: 0.6944 - val_accuracy: 0.5378\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 9s 123us/step - loss: 0.6752 - accuracy: 0.5749 - val_loss: 0.6885 - val_accuracy: 0.5532\n",
      "y2_pred:  [[0.42367384]\n",
      " [0.54906607]\n",
      " [0.64165646]\n",
      " ...\n",
      " [0.7184779 ]\n",
      " [0.59699017]\n",
      " [0.43375948]]\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.7619047619047619\n",
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 109us/step - loss: 0.6876 - accuracy: 0.5455 - val_loss: 0.6794 - val_accuracy: 0.5724\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6807 - accuracy: 0.5634 - val_loss: 0.6877 - val_accuracy: 0.5266\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6788 - accuracy: 0.5675 - val_loss: 0.6777 - val_accuracy: 0.5769\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6769 - accuracy: 0.5708 - val_loss: 0.6752 - val_accuracy: 0.5846\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6758 - accuracy: 0.5754 - val_loss: 0.6899 - val_accuracy: 0.5521\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6751 - accuracy: 0.5764 - val_loss: 0.6832 - val_accuracy: 0.5658\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6742 - accuracy: 0.5785 - val_loss: 0.6798 - val_accuracy: 0.5727\n",
      "y2_pred:  [[0.5307613 ]\n",
      " [0.5530225 ]\n",
      " [0.7603835 ]\n",
      " ...\n",
      " [0.49734208]\n",
      " [0.49613687]\n",
      " [0.49223822]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6767554479418886\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 9s 124us/step - loss: 0.6902 - accuracy: 0.5324 - val_loss: 0.7132 - val_accuracy: 0.4229\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 108us/step - loss: 0.6809 - accuracy: 0.5648 - val_loss: 0.6895 - val_accuracy: 0.5274\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6775 - accuracy: 0.5719 - val_loss: 0.6977 - val_accuracy: 0.5199\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6762 - accuracy: 0.5759 - val_loss: 0.6818 - val_accuracy: 0.5557\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6751 - accuracy: 0.5774 - val_loss: 0.6869 - val_accuracy: 0.5568\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 8s 106us/step - loss: 0.6743 - accuracy: 0.5787 - val_loss: 0.7084 - val_accuracy: 0.4982\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 9s 119us/step - loss: 0.6734 - accuracy: 0.5806 - val_loss: 0.6812 - val_accuracy: 0.5649\n",
      "Epoch 8/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6730 - accuracy: 0.5820 - val_loss: 0.6920 - val_accuracy: 0.5335\n",
      "Epoch 9/30\n",
      "72000/72000 [==============================] - 8s 110us/step - loss: 0.6727 - accuracy: 0.5811 - val_loss: 0.6865 - val_accuracy: 0.5583\n",
      "Epoch 10/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6724 - accuracy: 0.5826 - val_loss: 0.6994 - val_accuracy: 0.5160\n",
      "y2_pred:  [[0.5734871 ]\n",
      " [0.5148169 ]\n",
      " [0.44920948]\n",
      " ...\n",
      " [0.45824885]\n",
      " [0.44801635]\n",
      " [0.5091783 ]]\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7483857949959645\n",
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/30\n",
      "72000/72000 [==============================] - 8s 114us/step - loss: 0.6899 - accuracy: 0.5340 - val_loss: 0.7052 - val_accuracy: 0.4599\n",
      "Epoch 2/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6816 - accuracy: 0.5655 - val_loss: 0.6941 - val_accuracy: 0.5046\n",
      "Epoch 3/30\n",
      "72000/72000 [==============================] - 8s 113us/step - loss: 0.6784 - accuracy: 0.5700 - val_loss: 0.6840 - val_accuracy: 0.5407\n",
      "Epoch 4/30\n",
      "72000/72000 [==============================] - 8s 107us/step - loss: 0.6773 - accuracy: 0.5723 - val_loss: 0.6737 - val_accuracy: 0.5824\n",
      "Epoch 5/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6764 - accuracy: 0.5747 - val_loss: 0.6860 - val_accuracy: 0.5435\n",
      "Epoch 6/30\n",
      "72000/72000 [==============================] - 9s 118us/step - loss: 0.6756 - accuracy: 0.5757 - val_loss: 0.6893 - val_accuracy: 0.5414\n",
      "Epoch 7/30\n",
      "72000/72000 [==============================] - 8s 111us/step - loss: 0.6750 - accuracy: 0.5777 - val_loss: 0.6984 - val_accuracy: 0.5116\n",
      "y2_pred:  [[0.58861303]\n",
      " [0.49560842]\n",
      " [0.5835916 ]\n",
      " ...\n",
      " [0.5349895 ]\n",
      " [0.4772184 ]\n",
      " [0.3721481 ]]\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.7308313155770783\n",
      "pod:  [0.47871696590679846, 0.7086947750655639, 0.8198143664245359, 0.8581517352703794, 0.7951977401129944, 0.8551251008878128, 0.7619047619047619, 0.6767554479418886, 0.7483857949959645, 0.7308313155770783]\n",
      "pof:  [0.2876685170499603, 0.5471847739888978, 0.7061855670103093, 0.7381046788263284, 0.6681205392545598, 0.7759714512291832, 0.643735130848533, 0.5360824742268041, 0.6230418401744993, 0.5901249256395003]\n",
      "auc:  [0.5955242244284191, 0.5807550005383331, 0.5568143997071133, 0.5600235282220255, 0.5635386004292173, 0.5395768248293149, 0.5590848155281145, 0.5703364868575422, 0.5626719774107326, 0.570353194968789]\n",
      "tn_list:  [3593, 2284, 1482, 1321, 1674, 1130, 1797, 2340, 1901, 2067]\n",
      "fp_list:  [1451, 2760, 3562, 3723, 3370, 3914, 3247, 2704, 3142, 2976]\n",
      "fn_list:  [2584, 1444, 893, 703, 1015, 718, 1180, 1602, 1247, 1334]\n",
      "tp_list:  [2373, 3513, 4063, 4253, 3941, 4238, 3776, 3354, 3709, 3622]\n",
      "19589 30849 12720 36842\n"
     ]
    }
   ],
   "source": [
    "df_x_train_chi = pd.DataFrame(x_train_chi)\n",
    "df_x_train_chi.head()\n",
    "\n",
    "#FNN\n",
    "def get_FNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #get number of columns in training data\n",
    "    n_cols = X2_train.shape[1]\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #train model\n",
    "    model.fit(X2_train, y2_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    print('y2_pred: ',y2_pred)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_FNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "print('tn_list: ',tn_list)\n",
    "print ('fp_list: ',fp_list)\n",
    "print ('fn_list: ',fn_list)\n",
    "print ('tp_list: ',tp_list)\n",
    "\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 45s 626us/step - loss: 0.6892 - acc: 0.5358 - val_loss: 0.6998 - val_acc: 0.5079\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 41s 567us/step - loss: 0.6867 - acc: 0.5480 - val_loss: 0.6870 - val_acc: 0.5420\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 42s 582us/step - loss: 0.6857 - acc: 0.5509 - val_loss: 0.6891 - val_acc: 0.5337\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 41s 568us/step - loss: 0.6850 - acc: 0.5532 - val_loss: 0.6812 - val_acc: 0.5627\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 41s 566us/step - loss: 0.6843 - acc: 0.5547 - val_loss: 0.6940 - val_acc: 0.5197\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.8785555779705467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71999 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "71999/71999 [==============================] - 42s 589us/step - loss: 0.6888 - acc: 0.5383 - val_loss: 0.6845 - val_acc: 0.5552\n",
      "Epoch 2/5\n",
      "71999/71999 [==============================] - 41s 569us/step - loss: 0.6859 - acc: 0.5505 - val_loss: 0.6842 - val_acc: 0.5533\n",
      "Epoch 3/5\n",
      "71999/71999 [==============================] - 41s 574us/step - loss: 0.6845 - acc: 0.5539 - val_loss: 0.6803 - val_acc: 0.5727\n",
      "Epoch 4/5\n",
      "71999/71999 [==============================] - 40s 557us/step - loss: 0.6840 - acc: 0.5538 - val_loss: 0.6797 - val_acc: 0.5676\n",
      "Epoch 5/5\n",
      "71999/71999 [==============================] - 41s 573us/step - loss: 0.6833 - acc: 0.5587 - val_loss: 0.6716 - val_acc: 0.5894\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.6669356465604196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 579us/step - loss: 0.6886 - acc: 0.5399 - val_loss: 0.6907 - val_acc: 0.5308\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 39s 543us/step - loss: 0.6856 - acc: 0.5501 - val_loss: 0.6889 - val_acc: 0.5375\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 40s 561us/step - loss: 0.6847 - acc: 0.5553 - val_loss: 0.6840 - val_acc: 0.5553\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 40s 554us/step - loss: 0.6840 - acc: 0.5561 - val_loss: 0.6761 - val_acc: 0.5856\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 40s 556us/step - loss: 0.6828 - acc: 0.5594 - val_loss: 0.6809 - val_acc: 0.5579\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.5544794188861986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 599us/step - loss: 0.6894 - acc: 0.5364 - val_loss: 0.6999 - val_acc: 0.4857\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 576us/step - loss: 0.6864 - acc: 0.5495 - val_loss: 0.6836 - val_acc: 0.5719\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 41s 568us/step - loss: 0.6855 - acc: 0.5536 - val_loss: 0.6950 - val_acc: 0.5111\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 576us/step - loss: 0.6846 - acc: 0.5540 - val_loss: 0.6972 - val_acc: 0.5029\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.6841 - acc: 0.5582 - val_loss: 0.6907 - val_acc: 0.5342\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.6154156577885391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 582us/step - loss: 0.6887 - acc: 0.5392 - val_loss: 0.6780 - val_acc: 0.5881\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 42s 581us/step - loss: 0.6857 - acc: 0.5509 - val_loss: 0.6815 - val_acc: 0.5665\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 41s 573us/step - loss: 0.6849 - acc: 0.5540 - val_loss: 0.6908 - val_acc: 0.5348\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 585us/step - loss: 0.6839 - acc: 0.5582 - val_loss: 0.6756 - val_acc: 0.5877\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 43s 593us/step - loss: 0.6825 - acc: 0.5614 - val_loss: 0.6817 - val_acc: 0.5627\n",
      "y22_pred:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "pod 1st:  0.572639225181598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 584us/step - loss: 0.6884 - acc: 0.5390 - val_loss: 0.6921 - val_acc: 0.5235\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 43s 591us/step - loss: 0.6858 - acc: 0.5525 - val_loss: 0.6873 - val_acc: 0.5456\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 582us/step - loss: 0.6848 - acc: 0.5548 - val_loss: 0.6880 - val_acc: 0.5414\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 566us/step - loss: 0.6838 - acc: 0.5576 - val_loss: 0.6980 - val_acc: 0.5072\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 583us/step - loss: 0.6828 - acc: 0.5606 - val_loss: 0.6941 - val_acc: 0.5316\n",
      "y22_pred:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.7282082324455206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 598us/step - loss: 0.6893 - acc: 0.5357 - val_loss: 0.6966 - val_acc: 0.4958\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 569us/step - loss: 0.6862 - acc: 0.5491 - val_loss: 0.7003 - val_acc: 0.4987\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 44s 612us/step - loss: 0.6850 - acc: 0.5545 - val_loss: 0.6861 - val_acc: 0.5527\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 42s 579us/step - loss: 0.6840 - acc: 0.5568 - val_loss: 0.6867 - val_acc: 0.5435\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 41s 576us/step - loss: 0.6830 - acc: 0.5614 - val_loss: 0.6772 - val_acc: 0.5822\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "pod 1st:  0.5209846650524617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 580us/step - loss: 0.6893 - acc: 0.5375 - val_loss: 0.6936 - val_acc: 0.5099\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 44s 612us/step - loss: 0.6865 - acc: 0.5473 - val_loss: 0.6933 - val_acc: 0.5131\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 41s 569us/step - loss: 0.6860 - acc: 0.5499 - val_loss: 0.6847 - val_acc: 0.5552\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 572us/step - loss: 0.6855 - acc: 0.5516 - val_loss: 0.6891 - val_acc: 0.5394\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 577us/step - loss: 0.6851 - acc: 0.5533 - val_loss: 0.6840 - val_acc: 0.5581\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "pod 1st:  0.5236077481840193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 42s 587us/step - loss: 0.6891 - acc: 0.5384 - val_loss: 0.6971 - val_acc: 0.5039\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 41s 567us/step - loss: 0.6861 - acc: 0.5500 - val_loss: 0.6970 - val_acc: 0.5046\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 586us/step - loss: 0.6853 - acc: 0.5519 - val_loss: 0.6871 - val_acc: 0.5491\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 41s 574us/step - loss: 0.6847 - acc: 0.5537 - val_loss: 0.6930 - val_acc: 0.5258\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 42s 581us/step - loss: 0.6838 - acc: 0.5592 - val_loss: 0.6794 - val_acc: 0.5790\n",
      "y22_pred:  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.37691686844229216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18001 samples\n",
      "Epoch 1/5\n",
      "72000/72000 [==============================] - 43s 592us/step - loss: 0.6889 - acc: 0.5382 - val_loss: 0.6824 - val_acc: 0.5619\n",
      "Epoch 2/5\n",
      "72000/72000 [==============================] - 40s 560us/step - loss: 0.6860 - acc: 0.5502 - val_loss: 0.6932 - val_acc: 0.5147\n",
      "Epoch 3/5\n",
      "72000/72000 [==============================] - 42s 578us/step - loss: 0.6853 - acc: 0.5521 - val_loss: 0.6809 - val_acc: 0.5700\n",
      "Epoch 4/5\n",
      "72000/72000 [==============================] - 40s 558us/step - loss: 0.6846 - acc: 0.5541 - val_loss: 0.6822 - val_acc: 0.5615\n",
      "Epoch 5/5\n",
      "72000/72000 [==============================] - 41s 570us/step - loss: 0.6838 - acc: 0.5563 - val_loss: 0.6693 - val_acc: 0.5940\n",
      "y22_pred:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "pod 1st:  0.30347054075867635\n",
      "pod:  [0.8785555779705467, 0.6669356465604196, 0.5544794188861986, 0.6154156577885391, 0.572639225181598, 0.7282082324455206, 0.5209846650524617, 0.5236077481840193, 0.37691686844229216, 0.30347054075867635]\n",
      "pof:  [0.799167327517843, 0.5455987311657414, 0.4734337827121332, 0.5160586835844568, 0.46213322759714515, 0.6383822363203806, 0.4544012688342585, 0.40384615384615385, 0.2587745389649018, 0.2082093991671624]\n",
      "auc:  [0.5396941252263519, 0.5606684576973391, 0.5405228180870327, 0.5496784871020413, 0.5552529987922263, 0.54491299806257, 0.5332916981091016, 0.5598807971689328, 0.5590711647386951, 0.5476305707957569]\n",
      "26429 24009 21107 28455\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "def get_RNN_Predict(X2_train, X2_test, y2_train, y2_test):\n",
    "    import pandas as pd\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense,Dropout, LSTM, GRU\n",
    "    from keras.layers import Embedding\n",
    "    max_features = 10000 # number of words to consider as features\n",
    "    import numpy as np\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 32))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit(X2_train, y2_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "    y2_pred = model.predict(X2_test)\n",
    "    y22_pred=y2_pred.round()\n",
    "    print('y22_pred: ',y22_pred)\n",
    "    return y22_pred\n",
    "\n",
    "pod_list = []\n",
    "pof_list = []\n",
    "auc_val_list = []\n",
    "tn_list= []\n",
    "fp_list= []\n",
    "fn_list= []\n",
    "tp_list= []\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in folds.split(df_x_train_chi,y_train):\n",
    "    X2_train, X2_test, y2_train, y2_test=df_x_train_chi.iloc[train_index], df_x_train_chi.iloc[test_index],y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    y_pred = get_RNN_Predict(X2_train, X2_test, y2_train, y2_test)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y2_test, y_pred).ravel()\n",
    "    tn_list.append(tn)\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    tp_list.append(tp)\n",
    "    \n",
    "    pod=tp/(tp+fn)\n",
    "    print('pod 1st: ',pod)\n",
    "    pof=fp/(fp+tn)\n",
    "    auc_val=(1+pod-pof)/2\n",
    "    #break\n",
    "    pod_list.append(pod)\n",
    "    pof_list.append(pof)\n",
    "    auc_val_list.append(auc_val)\n",
    "\n",
    "print('pod: ',pod_list)\n",
    "print ('pof: ',pof_list)\n",
    "print ('auc: ',auc_val_list)\n",
    "\n",
    "tn=sum(tn_list)\n",
    "fp=sum(fp_list) \n",
    "fn=sum(fn_list) \n",
    "tp=sum(tp_list)\n",
    "print(tn, fp, fn, tp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
